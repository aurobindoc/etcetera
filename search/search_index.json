{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"To provide a platform to enable stateful systems to handle reconciliations. Anywhere, some kind of consistency needs to be maintained across or within system states, it can be termed as reconciliation. Nightswatch, basically, ensures that flows across or within systems should be smooth and happen within some time frame. Nightswatch thus provides automated anomaly handling. When it comes to functionality , the use cases can be broadly classified into the following: Ensuring state transitions happens within pre specified TAT by invoking certain action if that TAT is breached. This is classic scheduler use case. However, Nightswatch provides the ability to retry a fixed number of times. Example: If an order remains in Initialized state and has not approved within a certain defined time, Nightswatch ensures such orders do not remain stuck in Initialized state. State propagation across systems. An entity might have a certain state in one system but hasn\u2019t reached the corresponding state in another system. Example: Recon between PG and checkout. Customer might have paid at the PG but the response did not reach checkout. Now this payment entity has inconsistent state between pg and checkout. Nightswatch ensures this anomaly is resolved within a fixed defined time. Raising alert/taking action, in case even after repeated attempts from Nightswatch the state of the system hasn't moved. These anomalies are detected based on logic provided by the systems who own the use cases. e.g. which state, what time, etc. These are configurable. Once an anomaly is identified it is handled as per the rules and policies provided by clients. e.g. which api to hit, what action to take, etc. Only action to be supported in the future is calling a varadhi api","title":"Introduction"},{"location":"alerts/","text":"Setup Alerts Clone / fetch the latest Nightswatch codebase from https://github.fkinternal.com/Flipkart/nightswatch Navigate to folder dev_support/alerts/ where alert scripts are placed. Common Alert options We have provided a script create_alertz.py to create alerts that takes in certain options based on which it creates the alerts. The options are described below: team: Team name where alerts will be configured tags: Tags for alerts. Consult http://fcp.fkinternal.com/#/docs/alertz/latest/concepts.md for more info on Team and Tag zone: Zone to configure alerts for. E.g. in-chennai-1, in-hyderabad-1 severity: Severity/Policy of alerts to be configured. Consult http://fcp.fkinternal.com/#/docs/alertz/latest/usage/manage-team.md for more info on policy overwrite: String flag to overwrite existing alert if exists. E.g. \"True\", \"False\" hint: Optional field String that will appended to the end of alert. Setup Kafka Lag Monitor Alerts python -m alertz.create_alertz kafka-lag-monitor --team \"Indradhanush\" --tags Indradhanush.oncall-email --zone \"in-chennai-1\" --severity \"SEV1\" --topology \"chore-refund\" --source \"client\" --absolute-lag 10000 --median-growth 1000 --mean-growth 1000 Formula Used for Alerting on Lag 3 factors are considered for alerting: absolute-lag-breach median-growth-breach mean-growth-breach absolute-lag: Median of 10 mins window median-growth: Difference between 1st 3 min Median and last 3 min Median of the 10 min window mean-growth: Difference between 1st 3 min Mean and last 3 min Mean of the 10 min window Kafka Lag Monitor Options topology: Topology Name for which alerts to be configured source: Alerts to be set on lag for which kafka? E.g. Client, Scheduler absolute-lag: Absolute Lag threshold. Default is 10K median-growth: Median growth threshold. Default is 1K mean-growth: Mean growth threshold. Default is 1K Setup Hystrix Metrics Latency Alert: Alert on 90 %ile latency of Command python3 -m alertz.create_alertz hystrix-latency --team \"Indradhanush\" --tags Indradhanush.oncall-email --zone \"in-chennai-1\" --severity \"SEV2\" --overwrite \"True\" --topology \"eur-RECON\" --component \"scheduler\" --absolute-lag 3000 --median-growth 500 --mean-growth 500 Error Alert: Alert on Error Percentage of Command python3 -m alertz.create_alertz hystrix-error --team \"Indradhanush\" --tags Indradhanush.oncall-email --zone \"in-chennai-1\" --severity \"SEV2\" --overwrite \"True\" --topology \"eur-RECON\" --component \"scheduler\" --absolute-median 5 --median-growth 1 --mean-growth 1 Formula Used for Alerting on Hystrix Metrics 3 factors are considered for alerting: absolute-median-breach || (median-growth-breach mean-growth-breach) absolute-median: Median of 10 mins window median-growth: Difference between 1st 3 min Median and last 3 min Median of the 10 min window mean-growth: Difference between 1st 3 min Mean and last 3 min Mean of the 10 min window Hystrix Alert Options topology: Topology Name for which alerts to be configured component: Component for which it is to be alerted? E.g. scheduler, yak, external-service-call, sideline absolute-median: Absolute Lag threshold. Default is 3K for latency, 5 % for Error median-growth: Median growth threshold. Default is 500 for latency, 1 % for Error mean-growth: Mean growth threshold. Default is 500 for latency, 1 % for Error Setup Sideline Metrics python3 -m alertz.create_alertz sideline-rate --team \"Indradhanush\" --tags Indradhanush.oncall-email --zone \"in-chennai-1\" --severity \"SEV2\" --topology \"eur-RECON\" --threshold 0.0005 Formula Used for Alerting on Hystrix Metrics This is plain threshold based alert. This is to be used by Clients threshold: Absolute Threshold of max value which when crossed should be alerted Sideline Rate Alert Options topology: Topology Name for which alerts to be configured threshold: Absolute Threshold of max value which when crossed should be alerted. Default is 0.0005","title":"Alerts"},{"location":"alerts/#setup-alerts","text":"Clone / fetch the latest Nightswatch codebase from https://github.fkinternal.com/Flipkart/nightswatch Navigate to folder dev_support/alerts/ where alert scripts are placed.","title":"Setup Alerts"},{"location":"alerts/#common-alert-options","text":"We have provided a script create_alertz.py to create alerts that takes in certain options based on which it creates the alerts. The options are described below: team: Team name where alerts will be configured tags: Tags for alerts. Consult http://fcp.fkinternal.com/#/docs/alertz/latest/concepts.md for more info on Team and Tag zone: Zone to configure alerts for. E.g. in-chennai-1, in-hyderabad-1 severity: Severity/Policy of alerts to be configured. Consult http://fcp.fkinternal.com/#/docs/alertz/latest/usage/manage-team.md for more info on policy overwrite: String flag to overwrite existing alert if exists. E.g. \"True\", \"False\" hint: Optional field String that will appended to the end of alert.","title":"Common Alert options"},{"location":"alerts/#setup-kafka-lag-monitor-alerts","text":"python -m alertz.create_alertz kafka-lag-monitor --team \"Indradhanush\" --tags Indradhanush.oncall-email --zone \"in-chennai-1\" --severity \"SEV1\" --topology \"chore-refund\" --source \"client\" --absolute-lag 10000 --median-growth 1000 --mean-growth 1000","title":"Setup Kafka Lag Monitor Alerts"},{"location":"alerts/#formula-used-for-alerting-on-lag","text":"3 factors are considered for alerting: absolute-lag-breach median-growth-breach mean-growth-breach absolute-lag: Median of 10 mins window median-growth: Difference between 1st 3 min Median and last 3 min Median of the 10 min window mean-growth: Difference between 1st 3 min Mean and last 3 min Mean of the 10 min window","title":"Formula Used for Alerting on Lag"},{"location":"alerts/#kafka-lag-monitor-options","text":"topology: Topology Name for which alerts to be configured source: Alerts to be set on lag for which kafka? E.g. Client, Scheduler absolute-lag: Absolute Lag threshold. Default is 10K median-growth: Median growth threshold. Default is 1K mean-growth: Mean growth threshold. Default is 1K","title":"Kafka Lag Monitor Options"},{"location":"alerts/#setup-hystrix-metrics","text":"Latency Alert: Alert on 90 %ile latency of Command python3 -m alertz.create_alertz hystrix-latency --team \"Indradhanush\" --tags Indradhanush.oncall-email --zone \"in-chennai-1\" --severity \"SEV2\" --overwrite \"True\" --topology \"eur-RECON\" --component \"scheduler\" --absolute-lag 3000 --median-growth 500 --mean-growth 500 Error Alert: Alert on Error Percentage of Command python3 -m alertz.create_alertz hystrix-error --team \"Indradhanush\" --tags Indradhanush.oncall-email --zone \"in-chennai-1\" --severity \"SEV2\" --overwrite \"True\" --topology \"eur-RECON\" --component \"scheduler\" --absolute-median 5 --median-growth 1 --mean-growth 1","title":"Setup Hystrix Metrics"},{"location":"alerts/#formula-used-for-alerting-on-hystrix-metrics","text":"3 factors are considered for alerting: absolute-median-breach || (median-growth-breach mean-growth-breach) absolute-median: Median of 10 mins window median-growth: Difference between 1st 3 min Median and last 3 min Median of the 10 min window mean-growth: Difference between 1st 3 min Mean and last 3 min Mean of the 10 min window","title":"Formula Used for Alerting on Hystrix Metrics"},{"location":"alerts/#hystrix-alert-options","text":"topology: Topology Name for which alerts to be configured component: Component for which it is to be alerted? E.g. scheduler, yak, external-service-call, sideline absolute-median: Absolute Lag threshold. Default is 3K for latency, 5 % for Error median-growth: Median growth threshold. Default is 500 for latency, 1 % for Error mean-growth: Mean growth threshold. Default is 500 for latency, 1 % for Error","title":"Hystrix Alert Options"},{"location":"alerts/#setup-sideline-metrics","text":"python3 -m alertz.create_alertz sideline-rate --team \"Indradhanush\" --tags Indradhanush.oncall-email --zone \"in-chennai-1\" --severity \"SEV2\" --topology \"eur-RECON\" --threshold 0.0005","title":"Setup Sideline Metrics"},{"location":"alerts/#formula-used-for-alerting-on-hystrix-metrics_1","text":"This is plain threshold based alert. This is to be used by Clients threshold: Absolute Threshold of max value which when crossed should be alerted","title":"Formula Used for Alerting on Hystrix Metrics"},{"location":"alerts/#sideline-rate-alert-options","text":"topology: Topology Name for which alerts to be configured threshold: Absolute Threshold of max value which when crossed should be alerted. Default is 0.0005","title":"Sideline Rate Alert Options"},{"location":"api/","text":"Jar Separated Clients Chennai Elb Endpoint: 10.47.102.118 Hyderabad Elb Endpoint: 10.24.7.64 Non Jar Separated Clients This is not available in Hyderabad. Please migrate to Jar separated version\" Rest Server Ip 10.34.229.64 Rest Server Port 10240 Ingestion API Name Method Path Payload Notes Add to Nightswatch POST /2.0/nw/{topo-name}/add {\"id\":\"PT12345678903\", \"topologyName\":\"nw-preprod\", \"sourceSpoutName\": \"source_preprod\", \"perfId\": \"PT12345678903\", \"domain\": \"FLIPKART\", \"sourceToFieldsMap\": {\"id\": \"PT12345678903\", \"val\": 1001, \"expression\": \"REVERSE\", \"testCase\": \"SCHEDULER_SYNC\"}} Add events to Nightswatch Debugging APIs Name Method Path Payload Notes Get Supervisors GET /2.0/nw/details/{topo-name}/supervisors Returns list of Supervisors for that topology Get Trace GET /2.0/nw/trace/{topo-tag}/{event-id} Returns tracer logs for given event Get DB Entry GET /2.0/nw/db/{topo-tag}/{event-id} Returns db entry for given event Sideline APIs Name Method Path Payload Notes List Sideline GET /2.0/nw/{topo-name}/sideline/list?page_size={page_size} Returns first {page_size} number of row keys of all sideline events List Sideline GET /2.0/nw/{topo-name}/sideline/list?from={timeStamp} to={timeStamp} page_size={page_size} Returns row keys of all sideline events in a time bound Unsideline POST /2.0/nw/{topo-name}/unsideline?id={rowkey} Unsidelines the event with the given row key Unsideline Bulk POST /2.0/nw/{topo-name}/unsideline [\"rowkey1\", \"rowkey2\"] Unsidelines the event with row key rowkey1, rowkey2 Delete from Sideline DELETE /2.0/nw/{topo-name}/sideline/remove?id={rowkey} Removes the event with row key rowkey from Sideline Table timeStamps are in milliseconds By default Page size is Set to 100 Max Number of Events to be unsidelined for Bulk Unsideline API at a time is limit to 100 Max Page Size allowed for List Sideline is limit to 2000","title":"API"},{"location":"api/#ingestion-api","text":"Name Method Path Payload Notes Add to Nightswatch POST /2.0/nw/{topo-name}/add {\"id\":\"PT12345678903\", \"topologyName\":\"nw-preprod\", \"sourceSpoutName\": \"source_preprod\", \"perfId\": \"PT12345678903\", \"domain\": \"FLIPKART\", \"sourceToFieldsMap\": {\"id\": \"PT12345678903\", \"val\": 1001, \"expression\": \"REVERSE\", \"testCase\": \"SCHEDULER_SYNC\"}} Add events to Nightswatch","title":"Ingestion API"},{"location":"api/#debugging-apis","text":"Name Method Path Payload Notes Get Supervisors GET /2.0/nw/details/{topo-name}/supervisors Returns list of Supervisors for that topology Get Trace GET /2.0/nw/trace/{topo-tag}/{event-id} Returns tracer logs for given event Get DB Entry GET /2.0/nw/db/{topo-tag}/{event-id} Returns db entry for given event","title":"Debugging APIs"},{"location":"api/#sideline-apis","text":"Name Method Path Payload Notes List Sideline GET /2.0/nw/{topo-name}/sideline/list?page_size={page_size} Returns first {page_size} number of row keys of all sideline events List Sideline GET /2.0/nw/{topo-name}/sideline/list?from={timeStamp} to={timeStamp} page_size={page_size} Returns row keys of all sideline events in a time bound Unsideline POST /2.0/nw/{topo-name}/unsideline?id={rowkey} Unsidelines the event with the given row key Unsideline Bulk POST /2.0/nw/{topo-name}/unsideline [\"rowkey1\", \"rowkey2\"] Unsidelines the event with row key rowkey1, rowkey2 Delete from Sideline DELETE /2.0/nw/{topo-name}/sideline/remove?id={rowkey} Removes the event with row key rowkey from Sideline Table timeStamps are in milliseconds By default Page size is Set to 100 Max Number of Events to be unsidelined for Bulk Unsideline API at a time is limit to 100 Max Page Size allowed for List Sideline is limit to 2000","title":"Sideline APIs"},{"location":"architecture/","text":"Basic Components of Nightswatch Nightswatch comprises of 6 stages: Source Spout: This is the entry point to Nightswatch. Objects received from respective client Kafka is read by the source spout. The source spout feed it to Nightswatch system where further actions are taken on it. Deser-Converter Bolt Deserialiser: The data received from Source Spout is of byte array type. To understand the client object it needs to be deserialised to the Client POJO. This action is taken care by the Deserialiser. Once deserialised, it also figures out if that object needs any action in Nightswatch based on configurations set by client. If required, it then forwards the client POJO to Converter, else it drops that event. Converter: We convert all the different client POJO received from Deserialiser to a common POJO which can be understood by Nightswatch. We call this as JobTupleInfo. It contains information like id, state etc. This JobTupleInfo is then sent to Builder Bolt. Builder Bolt: It receives the JobTupleInfo from Converter Bolt. Based on configuration provided by the client, it decide on the following: Save it to DataStore and Schedule it after ttl time units in Scheduler (if state is in forward expression). Remove it from Scheduler (if state is in reverse expression). Save it to DataStore but do not Schedule it (if max attempts done). Forward and reverse expression is mentioned by the client in configuration file. Forward expression comprises of the non terminal state and reverse expression comprises of the terminal states. Scheduler Bolt: It receives events from Builder bolt and schedules the events with Scheduler Service. Scheduler Spout: It listens to Scheduler Service Kafka and read events that pop out of Scheduler Service after their ttl expires. It hand overs the event to the Handler Bolt. SchedulerKeyToTupleConverter Bolt: We save only the JobTuple Id to the Scheduler. Once we get the id from Scheduler Kafka, we need to Query Yak to get the Tuple. Handler Bolt: It is responsible to act upon the event. It may interact with the client service to inform it about the anomaly. The client then takes care of the anomaly. Nightswatch Flow Diagram Example with Europa Recon Topology Source Spout reads europa-statebus Kafka topic Deserialiser Bolt deserialises to CheckoutExecutionState object and figures out the state. In case of Europa Recon every event id passed on to Converter Bolt. Converter converts CheckoutExecutionState Object to JobTupleInfo Builder check if state is not a terminal state, it persists it and schedules it for a ttl. (Assumption is by that time, that checkout would reach terminal state). If not, then Scheduler will pop that out to SchedulerKeyToTupleConverter bolt. SchedulerKeyToTupleConverter bolt query Yak to get the Tuple from the JobTuple Id that we get from Scheduler Spout. Handler Bolt then informs the Client about the issue and asks client to rectify it (i.e. force the checkout to an terminal state) Once it reaches a terminal state, it again comes to deser-converter and then to builder. Now, Builder finds out that the same events is already there in datastore in a non terminal state. So, it deletes the event from the datastore as it has reached a terminal state and continues. Handler bolt tries for a maxAttempts number of times to inform the client. It client don't react on that then, Handler bolt sidelines the event and continues.","title":"Architecture"},{"location":"architecture/#basic-components-of-nightswatch","text":"Nightswatch comprises of 6 stages: Source Spout: This is the entry point to Nightswatch. Objects received from respective client Kafka is read by the source spout. The source spout feed it to Nightswatch system where further actions are taken on it. Deser-Converter Bolt Deserialiser: The data received from Source Spout is of byte array type. To understand the client object it needs to be deserialised to the Client POJO. This action is taken care by the Deserialiser. Once deserialised, it also figures out if that object needs any action in Nightswatch based on configurations set by client. If required, it then forwards the client POJO to Converter, else it drops that event. Converter: We convert all the different client POJO received from Deserialiser to a common POJO which can be understood by Nightswatch. We call this as JobTupleInfo. It contains information like id, state etc. This JobTupleInfo is then sent to Builder Bolt. Builder Bolt: It receives the JobTupleInfo from Converter Bolt. Based on configuration provided by the client, it decide on the following: Save it to DataStore and Schedule it after ttl time units in Scheduler (if state is in forward expression). Remove it from Scheduler (if state is in reverse expression). Save it to DataStore but do not Schedule it (if max attempts done). Forward and reverse expression is mentioned by the client in configuration file. Forward expression comprises of the non terminal state and reverse expression comprises of the terminal states. Scheduler Bolt: It receives events from Builder bolt and schedules the events with Scheduler Service. Scheduler Spout: It listens to Scheduler Service Kafka and read events that pop out of Scheduler Service after their ttl expires. It hand overs the event to the Handler Bolt. SchedulerKeyToTupleConverter Bolt: We save only the JobTuple Id to the Scheduler. Once we get the id from Scheduler Kafka, we need to Query Yak to get the Tuple. Handler Bolt: It is responsible to act upon the event. It may interact with the client service to inform it about the anomaly. The client then takes care of the anomaly.","title":"Basic Components of Nightswatch"},{"location":"architecture/#nightswatch-flow-diagram","text":"","title":"Nightswatch Flow Diagram"},{"location":"architecture/#example-with-europa-recon-topology","text":"Source Spout reads europa-statebus Kafka topic Deserialiser Bolt deserialises to CheckoutExecutionState object and figures out the state. In case of Europa Recon every event id passed on to Converter Bolt. Converter converts CheckoutExecutionState Object to JobTupleInfo Builder check if state is not a terminal state, it persists it and schedules it for a ttl. (Assumption is by that time, that checkout would reach terminal state). If not, then Scheduler will pop that out to SchedulerKeyToTupleConverter bolt. SchedulerKeyToTupleConverter bolt query Yak to get the Tuple from the JobTuple Id that we get from Scheduler Spout. Handler Bolt then informs the Client about the issue and asks client to rectify it (i.e. force the checkout to an terminal state) Once it reaches a terminal state, it again comes to deser-converter and then to builder. Now, Builder finds out that the same events is already there in datastore in a non terminal state. So, it deletes the event from the datastore as it has reached a terminal state and continues. Handler bolt tries for a maxAttempts number of times to inform the client. It client don't react on that then, Handler bolt sidelines the event and continues.","title":"Example with Europa Recon Topology"},{"location":"configBucketsChennai/","text":"Nightswatch Bucket Bucket Name Link Description prod-nightswatch-config http://10.47.3.62/#/list-keys/prod-nightswatch-config Prod Nightswatch App Configurations prod-nightswatch-yak-config http://10.47.3.62/#/list-keys/prod-nightswatch-yak-config Prod Nightswatch Yak Configurations Nightswatch Yak Bucket Bucket Name Link Description prod-id-yak-archive http://10.47.3.62/#/list-keys/prod-id-yak-archive Yak RS group Configuration for Archive Cluster yak-prod_id_nightswatch http://10.47.3.62/#/list-keys/yak-prod_id_nightswatch Yak RS group Configuration for Nightswatch Storm Config Buckets Bucket Name Link Description prod-nightswatch-storm-config http://10.47.3.62/#/list-keys/prod-nightswatch-storm-config Storm Nimbus/Supervisor related Configurations prod-nightswatch-worker-config http://10.47.3.62/#/list-keys/prod-nightswatch-worker-config Worker Log related Configuration prod-nightswatch-cluster-config http://10.47.3.62/#/list-keys/prod-nightswatch-cluster-config Cluster Log related Configuration Zookeeper Bucket Bucket Name Link Description prod-nightswatch-zk http://10.47.3.62/#/list-keys/prod-nightswatch-zk Zookeeper Related Configuration for Nightswatch Hystrix Bucket Bucket Name Link Description nw-hystrix http://10.47.3.62/#/list-keys/nw-hystrix Hystrix Configurations for Nightswatch hystrix-monitor-id http://10.47.3.62/#/list-keys/hystrix-monitor-id Hystrix Configurations Master Bucket for ID Hystrix Kafka Lag Monitor Bucket Name Link Description prod-nightswatch-kafka-lag-monitor http://10.47.3.62/#/list-keys/prod-nightswatch-kafka-lag-monitor Hystrix Configurations for Nightswatch","title":"Chennai"},{"location":"configBucketsChennai/#nightswatch-bucket","text":"Bucket Name Link Description prod-nightswatch-config http://10.47.3.62/#/list-keys/prod-nightswatch-config Prod Nightswatch App Configurations prod-nightswatch-yak-config http://10.47.3.62/#/list-keys/prod-nightswatch-yak-config Prod Nightswatch Yak Configurations","title":"Nightswatch Bucket"},{"location":"configBucketsChennai/#nightswatch-yak-bucket","text":"Bucket Name Link Description prod-id-yak-archive http://10.47.3.62/#/list-keys/prod-id-yak-archive Yak RS group Configuration for Archive Cluster yak-prod_id_nightswatch http://10.47.3.62/#/list-keys/yak-prod_id_nightswatch Yak RS group Configuration for Nightswatch","title":"Nightswatch Yak Bucket"},{"location":"configBucketsChennai/#storm-config-buckets","text":"Bucket Name Link Description prod-nightswatch-storm-config http://10.47.3.62/#/list-keys/prod-nightswatch-storm-config Storm Nimbus/Supervisor related Configurations prod-nightswatch-worker-config http://10.47.3.62/#/list-keys/prod-nightswatch-worker-config Worker Log related Configuration prod-nightswatch-cluster-config http://10.47.3.62/#/list-keys/prod-nightswatch-cluster-config Cluster Log related Configuration","title":"Storm Config Buckets"},{"location":"configBucketsChennai/#zookeeper-bucket","text":"Bucket Name Link Description prod-nightswatch-zk http://10.47.3.62/#/list-keys/prod-nightswatch-zk Zookeeper Related Configuration for Nightswatch","title":"Zookeeper Bucket"},{"location":"configBucketsChennai/#hystrix-bucket","text":"Bucket Name Link Description nw-hystrix http://10.47.3.62/#/list-keys/nw-hystrix Hystrix Configurations for Nightswatch hystrix-monitor-id http://10.47.3.62/#/list-keys/hystrix-monitor-id Hystrix Configurations Master Bucket for ID Hystrix","title":"Hystrix Bucket"},{"location":"configBucketsChennai/#kafka-lag-monitor","text":"Bucket Name Link Description prod-nightswatch-kafka-lag-monitor http://10.47.3.62/#/list-keys/prod-nightswatch-kafka-lag-monitor Hystrix Configurations for Nightswatch","title":"Kafka Lag Monitor"},{"location":"configBucketsHyderabad/","text":"Nightswatch Bucket Bucket Name Link Description prod-nightswatch-config http://10.24.0.33/#/list-keys/prod-nightswatch-config Prod Nightswatch App Configurations prod-nightswatch-yak-config http://10.24.0.33/#/list-keys/prod-nightswatch-yak-config Prod Nightswatch Yak Configurations Nightswatch Yak Bucket Bucket Name Link Description prod-id-yak-archive http://10.24.0.33/#/list-keys/prod-id-yak-archive Yak RS group Configuration for Archive Cluster yak-prod_id_nightswatch http://10.24.0.33/#/list-keys/yak-prod_id_nightswatch Yak RS group Configuration for Nightswatch Storm Config Buckets Bucket Name Link Description prod-nightswatch-storm-config http://10.24.0.33/#/list-keys/prod-nightswatch-storm-config Storm Nimbus/Supervisor related Configurations prod-nightswatch-worker-config http://10.24.0.33/#/list-keys/prod-nightswatch-worker-config Worker Log related Configuration prod-nightswatch-cluster-config http://10.24.0.33/#/list-keys/prod-nightswatch-cluster-config Cluster Log related Configuration Zookeeper Bucket Bucket Name Link Description prod-nightswatch-zk http://10.24.0.33/#/list-keys/prod-nightswatch-zk Zookeeper Related Configuration for Nightswatch Hystrix Bucket Bucket Name Link Description tr-hystrix-nw http://10.24.0.33/#/list-keys/tr-hystrix-nw Hystrix Configurations for Nightswatch Kafka Lag Monitor Bucket Name Link Description prod-nightswatch-kafka-lag-monitor http://10.24.0.33/#/list-keys/prod-nightswatch-kafka-lag-monitor Hystrix Configurations for Nightswatch","title":"Hyderabad"},{"location":"configBucketsHyderabad/#nightswatch-bucket","text":"Bucket Name Link Description prod-nightswatch-config http://10.24.0.33/#/list-keys/prod-nightswatch-config Prod Nightswatch App Configurations prod-nightswatch-yak-config http://10.24.0.33/#/list-keys/prod-nightswatch-yak-config Prod Nightswatch Yak Configurations","title":"Nightswatch Bucket"},{"location":"configBucketsHyderabad/#nightswatch-yak-bucket","text":"Bucket Name Link Description prod-id-yak-archive http://10.24.0.33/#/list-keys/prod-id-yak-archive Yak RS group Configuration for Archive Cluster yak-prod_id_nightswatch http://10.24.0.33/#/list-keys/yak-prod_id_nightswatch Yak RS group Configuration for Nightswatch","title":"Nightswatch Yak Bucket"},{"location":"configBucketsHyderabad/#storm-config-buckets","text":"Bucket Name Link Description prod-nightswatch-storm-config http://10.24.0.33/#/list-keys/prod-nightswatch-storm-config Storm Nimbus/Supervisor related Configurations prod-nightswatch-worker-config http://10.24.0.33/#/list-keys/prod-nightswatch-worker-config Worker Log related Configuration prod-nightswatch-cluster-config http://10.24.0.33/#/list-keys/prod-nightswatch-cluster-config Cluster Log related Configuration","title":"Storm Config Buckets"},{"location":"configBucketsHyderabad/#zookeeper-bucket","text":"Bucket Name Link Description prod-nightswatch-zk http://10.24.0.33/#/list-keys/prod-nightswatch-zk Zookeeper Related Configuration for Nightswatch","title":"Zookeeper Bucket"},{"location":"configBucketsHyderabad/#hystrix-bucket","text":"Bucket Name Link Description tr-hystrix-nw http://10.24.0.33/#/list-keys/tr-hystrix-nw Hystrix Configurations for Nightswatch","title":"Hystrix Bucket"},{"location":"configBucketsHyderabad/#kafka-lag-monitor","text":"Bucket Name Link Description prod-nightswatch-kafka-lag-monitor http://10.24.0.33/#/list-keys/prod-nightswatch-kafka-lag-monitor Hystrix Configurations for Nightswatch","title":"Kafka Lag Monitor"},{"location":"controllingTopology/","text":"App Box Details Note Select the App Box from the instance group mentioned Chennai App Box Instance Group: storm-app-v3 Hyderabad App Box Instance Group: storm-app For non jar jeparated clients, App Box Instance Group: storm-app-v2 Please raise a bastion request for non-sudo permissions for prod-nightswatch app-id Stop Topology sudo /etc/init.d/fk-transact-nightswatch stop topology topology-name Reload Jar If there is any change in the Client Jar, the reload jar commands needs to be executed before starting the topology sudo /etc/init.d/fk-transact-nightswatch reload-jar topology-name All dependencies of the client, except those of nw-wall, should be shaded. If these dependencies are not shaded, then reload-jar will throw an error message containing a list of all the classes that were not shaded. Note that jar won't be reloaded unless all the client classes are shaded. Start Topology sudo /etc/init.d/fk-transact-nightswatch start topology topology-name In case of any issues or help please contact id-dev@flipkart.com","title":"Controlling the Topology"},{"location":"controllingTopology/#app-box-details","text":"Note Select the App Box from the instance group mentioned Chennai App Box Instance Group: storm-app-v3 Hyderabad App Box Instance Group: storm-app For non jar jeparated clients, App Box Instance Group: storm-app-v2 Please raise a bastion request for non-sudo permissions for prod-nightswatch app-id","title":"App Box Details"},{"location":"controllingTopology/#stop-topology","text":"sudo /etc/init.d/fk-transact-nightswatch stop topology topology-name","title":"Stop Topology"},{"location":"controllingTopology/#reload-jar","text":"If there is any change in the Client Jar, the reload jar commands needs to be executed before starting the topology sudo /etc/init.d/fk-transact-nightswatch reload-jar topology-name All dependencies of the client, except those of nw-wall, should be shaded. If these dependencies are not shaded, then reload-jar will throw an error message containing a list of all the classes that were not shaded. Note that jar won't be reloaded unless all the client classes are shaded.","title":"Reload Jar"},{"location":"controllingTopology/#start-topology","text":"sudo /etc/init.d/fk-transact-nightswatch start topology topology-name In case of any issues or help please contact id-dev@flipkart.com","title":"Start Topology"},{"location":"dashboards/","text":"Nightswatch Dashboards Nightswatch Client Dashboard V2.0 Nightswatch Client Dashboard Chennai: http://10.47.102.190/ Hyderabad: http://10.24.7.187/ Non jar Separated Clients (Soon to be deprecated) Not available in Hyderabad. Please migrate to Jar separated version for Helios support Topology Cosmos Dashboard: http://10.47.4.24/dashboard/script/nightswatch_dashboard_v2.js Nightswatch Client Dashboard is a Dashboard UI created for Nightswatch Clients to give them an accumulated view of what is happening in the Toplogy. The UI has several sections: Dashboard Event Trace Sideline Contact Us Dashboard Select a Topology from the dropdown on the Top Right Corner of the window and select the corresponding Dashboard from the Tabs in the Top left Corner. Hystrix : Give Hystrix metrics for each topology. Real time Thread metrics like latencies, throughput etc. for External Calls like Yak, Scheduler, Clients etc. Cosmos : Shows Cosmos Grafana Dashboard for that topology. This dashboards mainly consists of: Kafka Lag Monitor: Pre Scheduler Lag: Lag in reading from Client source Kafka Post Scheduler Lag: Lag in reading from Scheduler Sink Kafka Bolts Dashboard: Number of Bolt Processed per second: For every type of bolt, at what throughput it is executing for that topology? Number of Bolt Exceptions per second: For every type of bolt, at what rate it is throwing exceptions for that topology? Bolt Processing Latency: For every type of bolt, at what latency at which it is executing for that topology? Bolt Transfer Percentage: What percentage of events gets transferred from one bolt to other? Scheduler Dashboard: Scheduler API Error percentages for ADD, DELETE, UPDATE service calls Scheduler API Latencies (90 %ile and 99 %ile) for ADD, DELETE, UPDATE service calls Yak Dashboard: Yak Error percentages for GET, UPSERT service calls Yak Latencies (90 %ile and 99 %ile) for GET, UPSERT service calls External Service Call Dashboard: External Service Call Error percentages for DO_RECON service calls External Service Call Latencies (90 %ile and 99 %ile) for DO_RECON service calls Sideline Dashboard: Sideline Error percentages for ADD_TO_SIDELINE service calls Sideline Latencies (90 %ile and 99 %ile) for ADD_TO_SIDELINE service calls Sideline Rate: rate of event going into Sideline Scheduler : Scheduler Service Dashboard for Respective Topology Client Entries QPS and Latencies Number of Events evicted and Eviction Lag Storm : Storm Dashboard provides a view on the following metrics on a top level view: Number of Executors, Workers running for a Topology Events emitted, transferred, failed by the topology in last 10 mins, 3 hrs, 1 day, forever etc. Events emitted, transferred, failed by Spouts with the latencies Events emitted, transferred, failed by Bolts with the latencies Worker information like ip address of the supervisor box, id, uptime etc. Storm topology Configuration Bolts Dashboards are only available for Jar Separated Clients Event Console The Event Console provides with a console to check the Value of any event in the App Database. It has a Text Box where you need to provide the eventId__TopologyTag. To get the TopologyTag, check Topology Config Bucket. You can hover on the TextBox for a second to get more details. Trace Console The Trace Console provides with a console toget a view of what has happened to the event (Event journey in Nightswatch). It has a Text Box where you need to provide the eventId__TopologyTag. To get the TopologyTag, check Topology Config Bucket. You can hover on the TextBox for a second to get more details. Trace console provides what has happened to the event at what point of time and in which bolt. Shown a diagram below, where you can see logs separated for each bolt and sorted on Timestamp. Sideline Console The Sideline Console has 2 sub part in it: List Sideline : Select the Topology form the Dropdown and you can optionally select the to and from timestamps from the DateTimePicker and hit the List Sideline Button to get a list of all events sidelined in that time range. You can either unsideline or delete the event from sideline from this view. Unsideline : If you already have a Sideline Row Key, enter in the text box, select the topology and hit unsideline or Delete to perform the respective action on the event. Sideline Console is only available for Jar Separated Clients Yak Dashboard Yak dashboard for \"yak-nightswatch\" rs-group Displays Yak metrics like Read / Write TPT, Region Server Metrics, WAL metrics, Disk, Network, IPC details Nightswatch Yak Dashboard Chennai: http://10.47.4.24/dashboard/script/yak.js?prefix=prod-nightswatch-yak rsgroup=yak-prod_id_nightswatch refresh=30s orgId=1 from=now-15m to=now Hyderabad: http://10.24.0.243/dashboard/script/yak.js?prefix=prod-nightswatch-yak rsgroup=yak-prod_id_nightswatch refresh=30s orgId=1 from=now-15m to=now Nightswatch Zookeeper Dashboard Zookeeper related metrics like #connections, Data Size, #Alive Nodes, #Followers, #Outstanding Requests, #Nodes and Watchers Nightswatch Zookeeper Dashboard Chennai: http://10.47.4.24/dashboard/script/zk_v3.js?refresh=1m orgId=1 from=now-30m to=now app=prod-nightswatch-zookeeper Hyderabad: http://10.24.0.243/dashboard/script/zk_v3.js?refresh=1m orgId=1 from=now-30m to=now app=prod-nightswatch-zookeeper Nightswatch System Metrics Core system metrics like CPU usage, Load, Memory usage, Disk, and Network Metrics. Nightswatch System Metrics Dashboard Chennai: http://10.47.4.24/dashboard/script/system_v4.js?app=prod-nightswatch refresh=10s orgId=1 Hyderabad: http://10.24.0.243/dashboard/script/system_v4.js?app=prod-nightswatch refresh=10s orgId=1","title":"Dashboards"},{"location":"dashboards/#nightswatch-dashboards","text":"","title":"Nightswatch Dashboards"},{"location":"dashboards/#nightswatch-client-dashboard-v20","text":"Nightswatch Client Dashboard Chennai: http://10.47.102.190/ Hyderabad: http://10.24.7.187/ Non jar Separated Clients (Soon to be deprecated) Not available in Hyderabad. Please migrate to Jar separated version for Helios support Topology Cosmos Dashboard: http://10.47.4.24/dashboard/script/nightswatch_dashboard_v2.js Nightswatch Client Dashboard is a Dashboard UI created for Nightswatch Clients to give them an accumulated view of what is happening in the Toplogy. The UI has several sections: Dashboard Event Trace Sideline Contact Us","title":"Nightswatch Client Dashboard V2.0"},{"location":"dashboards/#dashboard","text":"Select a Topology from the dropdown on the Top Right Corner of the window and select the corresponding Dashboard from the Tabs in the Top left Corner. Hystrix : Give Hystrix metrics for each topology. Real time Thread metrics like latencies, throughput etc. for External Calls like Yak, Scheduler, Clients etc. Cosmos : Shows Cosmos Grafana Dashboard for that topology. This dashboards mainly consists of: Kafka Lag Monitor: Pre Scheduler Lag: Lag in reading from Client source Kafka Post Scheduler Lag: Lag in reading from Scheduler Sink Kafka Bolts Dashboard: Number of Bolt Processed per second: For every type of bolt, at what throughput it is executing for that topology? Number of Bolt Exceptions per second: For every type of bolt, at what rate it is throwing exceptions for that topology? Bolt Processing Latency: For every type of bolt, at what latency at which it is executing for that topology? Bolt Transfer Percentage: What percentage of events gets transferred from one bolt to other? Scheduler Dashboard: Scheduler API Error percentages for ADD, DELETE, UPDATE service calls Scheduler API Latencies (90 %ile and 99 %ile) for ADD, DELETE, UPDATE service calls Yak Dashboard: Yak Error percentages for GET, UPSERT service calls Yak Latencies (90 %ile and 99 %ile) for GET, UPSERT service calls External Service Call Dashboard: External Service Call Error percentages for DO_RECON service calls External Service Call Latencies (90 %ile and 99 %ile) for DO_RECON service calls Sideline Dashboard: Sideline Error percentages for ADD_TO_SIDELINE service calls Sideline Latencies (90 %ile and 99 %ile) for ADD_TO_SIDELINE service calls Sideline Rate: rate of event going into Sideline Scheduler : Scheduler Service Dashboard for Respective Topology Client Entries QPS and Latencies Number of Events evicted and Eviction Lag Storm : Storm Dashboard provides a view on the following metrics on a top level view: Number of Executors, Workers running for a Topology Events emitted, transferred, failed by the topology in last 10 mins, 3 hrs, 1 day, forever etc. Events emitted, transferred, failed by Spouts with the latencies Events emitted, transferred, failed by Bolts with the latencies Worker information like ip address of the supervisor box, id, uptime etc. Storm topology Configuration Bolts Dashboards are only available for Jar Separated Clients","title":"Dashboard"},{"location":"dashboards/#event-console","text":"The Event Console provides with a console to check the Value of any event in the App Database. It has a Text Box where you need to provide the eventId__TopologyTag. To get the TopologyTag, check Topology Config Bucket. You can hover on the TextBox for a second to get more details.","title":"Event Console"},{"location":"dashboards/#trace-console","text":"The Trace Console provides with a console toget a view of what has happened to the event (Event journey in Nightswatch). It has a Text Box where you need to provide the eventId__TopologyTag. To get the TopologyTag, check Topology Config Bucket. You can hover on the TextBox for a second to get more details. Trace console provides what has happened to the event at what point of time and in which bolt. Shown a diagram below, where you can see logs separated for each bolt and sorted on Timestamp.","title":"Trace Console"},{"location":"dashboards/#sideline-console","text":"The Sideline Console has 2 sub part in it: List Sideline : Select the Topology form the Dropdown and you can optionally select the to and from timestamps from the DateTimePicker and hit the List Sideline Button to get a list of all events sidelined in that time range. You can either unsideline or delete the event from sideline from this view. Unsideline : If you already have a Sideline Row Key, enter in the text box, select the topology and hit unsideline or Delete to perform the respective action on the event. Sideline Console is only available for Jar Separated Clients","title":"Sideline Console"},{"location":"dashboards/#yak-dashboard","text":"Yak dashboard for \"yak-nightswatch\" rs-group Displays Yak metrics like Read / Write TPT, Region Server Metrics, WAL metrics, Disk, Network, IPC details Nightswatch Yak Dashboard Chennai: http://10.47.4.24/dashboard/script/yak.js?prefix=prod-nightswatch-yak rsgroup=yak-prod_id_nightswatch refresh=30s orgId=1 from=now-15m to=now Hyderabad: http://10.24.0.243/dashboard/script/yak.js?prefix=prod-nightswatch-yak rsgroup=yak-prod_id_nightswatch refresh=30s orgId=1 from=now-15m to=now","title":"Yak Dashboard"},{"location":"dashboards/#nightswatch-zookeeper-dashboard","text":"Zookeeper related metrics like #connections, Data Size, #Alive Nodes, #Followers, #Outstanding Requests, #Nodes and Watchers Nightswatch Zookeeper Dashboard Chennai: http://10.47.4.24/dashboard/script/zk_v3.js?refresh=1m orgId=1 from=now-30m to=now app=prod-nightswatch-zookeeper Hyderabad: http://10.24.0.243/dashboard/script/zk_v3.js?refresh=1m orgId=1 from=now-30m to=now app=prod-nightswatch-zookeeper","title":"Nightswatch Zookeeper Dashboard"},{"location":"dashboards/#nightswatch-system-metrics","text":"Core system metrics like CPU usage, Load, Memory usage, Disk, and Network Metrics. Nightswatch System Metrics Dashboard Chennai: http://10.47.4.24/dashboard/script/system_v4.js?app=prod-nightswatch refresh=10s orgId=1 Hyderabad: http://10.24.0.243/dashboard/script/system_v4.js?app=prod-nightswatch refresh=10s orgId=1","title":"Nightswatch System Metrics"},{"location":"debugging/","text":"What has happened to my event in Nightswatch? Find out your Topology Tag. Check your config bucket for topology tag. topology_job_config : { tag : EUR_RECON , ... } Get the event ID of the Event of concern. Lets say your event ID is OD1150448624885720 Use the tracer API to trace the entire Journey of your Event in Nightswatch Check Debugging API Get Trace in this page for the API reference. http:// nw-rest-api-server : rest-port /2.0/nw/trace/EUR_RECON/OD1150448624885720 You can also use the Nightswatch Trace Console to get a view of what has happened to your event. It has a Text Box where you need to provide the eventId__TopologyTag. To get the TopologyTag, check Topology Config Bucket. You can hover on the TextBox for a second to get more details. Tracer Logs Structure On hitting the Tracer Api, the response from tracer API is a JSON. Explained below is the structure of that JSON containing the Tracer Logs. { dateTime : 2019-04-10-19:08:00.621 , id : 0__OD1150653670520580__EUR_RECON , tag : EUR_RECON , source : HANDLER , traces : [ 2019-04-10-19:08:00.622 :: fields read {tuple_info={attemptsMade=5}, source_checkout={checkoutId=OD1150653670520580, europaState=CHECKOUT_EXECUTION_ERROR}} , 2019-04-10-19:08:00.622 :: Making call for Id OD1150653670520580, with attempts made 5 and request ReconRequest{nwReconRequest=NwReconRequest{id= OD1150653670520580 , attemptNumber=5, lastAttempt=true, topologyName= eur-RECON , source=DEFAULT, reason= OD1150653670520580 , extraFieldsMap={checkoutId=OD1150653670520580, europaState=CHECKOUT_EXECUTION_ERROR}}, requestUrl= /callisto/api/v2/checkout/nightswatch/recon , poolName= CALLISTO , isPerf=false, domain= FLIPKART } , 2019-04-10-19:08:00.687 :: Response from CALLISTO : NwReconResponse{responseStatus=RETRY} with Response Status: 200 , 2019-04-10-19:08:00.687 :: Persisting event in Yak 0__OD1150653670520580__EUR_RECON : JobTupleInfo{id= OD1150653670520580 , tag= EUR_RECON , state=SIDELINED, perfId=Optional.of(OD1150653670520580), sourceToFieldsMap={tuple_info={attemptsMade=5}, source_checkout={checkoutId=OD1150653670520580, europaState=CHECKOUT_EXECUTION_ERROR}}, ttl=900000, expiryTs=1554903450586, previousExpiryTs=Optional.of(1554903450567), attemptsMade=5, name= eur-RECON , domain= FLIPKART , version= 29 , isTenantAwareNEvent=true} , 2019-04-10-19:08:00.689 :: emitting on sideline stream , 2019-04-10-19:08:00.689 :: sidelined 0__OD1150653670520580__EUR_RECON, source: scheduler-spout:7580, stream: default, id: {6036652578199257162=-6669858410400714836}, [[B@26052b7f, [B@11350634] ] } On hitting the tracer API we get a list of such objects referring to logs generated by each bolt for a particular event in Nightswatch. We describe each field in the Tracer JSON below: id : This is the ID corresponding to which logs are generated. tag : This is the topology tag source : This tells us the bolt from which these logs are generated. traces : These are the list of logs sorted based on time generated from the above mentioned bolt.","title":"Debugging"},{"location":"debugging/#what-has-happened-to-my-event-in-nightswatch","text":"Find out your Topology Tag. Check your config bucket for topology tag. topology_job_config : { tag : EUR_RECON , ... } Get the event ID of the Event of concern. Lets say your event ID is OD1150448624885720 Use the tracer API to trace the entire Journey of your Event in Nightswatch Check Debugging API Get Trace in this page for the API reference. http:// nw-rest-api-server : rest-port /2.0/nw/trace/EUR_RECON/OD1150448624885720 You can also use the Nightswatch Trace Console to get a view of what has happened to your event. It has a Text Box where you need to provide the eventId__TopologyTag. To get the TopologyTag, check Topology Config Bucket. You can hover on the TextBox for a second to get more details.","title":"What has happened to my event in Nightswatch?"},{"location":"debugging/#tracer-logs-structure","text":"On hitting the Tracer Api, the response from tracer API is a JSON. Explained below is the structure of that JSON containing the Tracer Logs. { dateTime : 2019-04-10-19:08:00.621 , id : 0__OD1150653670520580__EUR_RECON , tag : EUR_RECON , source : HANDLER , traces : [ 2019-04-10-19:08:00.622 :: fields read {tuple_info={attemptsMade=5}, source_checkout={checkoutId=OD1150653670520580, europaState=CHECKOUT_EXECUTION_ERROR}} , 2019-04-10-19:08:00.622 :: Making call for Id OD1150653670520580, with attempts made 5 and request ReconRequest{nwReconRequest=NwReconRequest{id= OD1150653670520580 , attemptNumber=5, lastAttempt=true, topologyName= eur-RECON , source=DEFAULT, reason= OD1150653670520580 , extraFieldsMap={checkoutId=OD1150653670520580, europaState=CHECKOUT_EXECUTION_ERROR}}, requestUrl= /callisto/api/v2/checkout/nightswatch/recon , poolName= CALLISTO , isPerf=false, domain= FLIPKART } , 2019-04-10-19:08:00.687 :: Response from CALLISTO : NwReconResponse{responseStatus=RETRY} with Response Status: 200 , 2019-04-10-19:08:00.687 :: Persisting event in Yak 0__OD1150653670520580__EUR_RECON : JobTupleInfo{id= OD1150653670520580 , tag= EUR_RECON , state=SIDELINED, perfId=Optional.of(OD1150653670520580), sourceToFieldsMap={tuple_info={attemptsMade=5}, source_checkout={checkoutId=OD1150653670520580, europaState=CHECKOUT_EXECUTION_ERROR}}, ttl=900000, expiryTs=1554903450586, previousExpiryTs=Optional.of(1554903450567), attemptsMade=5, name= eur-RECON , domain= FLIPKART , version= 29 , isTenantAwareNEvent=true} , 2019-04-10-19:08:00.689 :: emitting on sideline stream , 2019-04-10-19:08:00.689 :: sidelined 0__OD1150653670520580__EUR_RECON, source: scheduler-spout:7580, stream: default, id: {6036652578199257162=-6669858410400714836}, [[B@26052b7f, [B@11350634] ] } On hitting the tracer API we get a list of such objects referring to logs generated by each bolt for a particular event in Nightswatch. We describe each field in the Tracer JSON below: id : This is the ID corresponding to which logs are generated. tag : This is the topology tag source : This tells us the bolt from which these logs are generated. traces : These are the list of logs sorted based on time generated from the above mentioned bolt.","title":"Tracer Logs Structure"},{"location":"deployment/","text":"Deployment to App Box Create a build of the branch you want to deploy in Jenkins SSH to the App Box For updating the source.list.d, try one of these: Update in sourcelist using upgrade script sudo /etc/init.d/fk-transact-nightswatch upgrade This Script automatically updates the fk-transact-nightswatch.list file and updates the apt-get Update in fk-transact-nightswatch.list using the build number generated sudo vi /etc/apt/sources.list.d/fk-transact-nightswatch.list Modify the build with the build id you got from Jenkins sudo apt-get update Install Nightswatch Service sudo apt-get install fk-transact-nightswatch Start, reload Jar, Stop a topology Please follow Controlling the topology for more details Deployment to Supervisor instances We have an ansible playbook that can be used to deploy on `fk-transact-nightswatch` and `fk-3p-storm` on all supervisor instances. Inventory Inventory in Ansible world refers to the list of instances where a particular playbook has to be run. It is defined by the hosts file. For Nightswatch, the hosts file has been placed at ROOT/dev-support/storm-deployment/inventory/hosts . It contains a list of IP addresses of supervisors on which we wish to deploy in the following format. $xslt [supervisors] 10.32.2.167 10.33.2.234 There is also a script $ROOT/dev-support/storm-deployment/populate-inventory.sh which can be used to fetch a list of supervisors for chennai and hyderabad zone for production app and update the hosts file. Usage: ./PATH_TO/populate_inventory.sh -z ZONE where ZONE is either chennai or hyderabad . Configuration The playbook can be configured using a json file that lists all the options. It is placed at $ROOT/dev-support/storm-deployment/env.json . The file has following options { \"rolling\": 4, \"nightswatch_build\": 2797, \"storm_build\": 21, \"reinstall_storm\": false } rolling : No. of instances ansible should in parallel run the playbook. nightswatch-build : Build number of fk-transact-nightswatch that you wish to install. reinstall-storm : A boolean variable indicating whether fk-3p-storm has to be installed on the instances. storm-build : Build number of prod-id-storm to be installed. This is ignored if reinstall-storm is set to False Running the playbook. Pre-requisites: Install ansible-playbook in your system. Populate inventory/hosts file with appropriate list of IPs Get sudo permission for app prod-nightswatch Playbook Execution: ansible-playbook deploy_to_supervisors.yaml -i inventory/hosts --extra-vars \"@env.json\" -vvv Note: The -vvv option is to print verbose logs for every step. Note: This script only works for master branch. For other branches, please edit $ROOT/dev-support/storm-deployment/reinstall_nightswatch with proper branch name.","title":"Deployment"},{"location":"deployment/#deployment-to-app-box","text":"Create a build of the branch you want to deploy in Jenkins SSH to the App Box For updating the source.list.d, try one of these: Update in sourcelist using upgrade script sudo /etc/init.d/fk-transact-nightswatch upgrade This Script automatically updates the fk-transact-nightswatch.list file and updates the apt-get Update in fk-transact-nightswatch.list using the build number generated sudo vi /etc/apt/sources.list.d/fk-transact-nightswatch.list Modify the build with the build id you got from Jenkins sudo apt-get update Install Nightswatch Service sudo apt-get install fk-transact-nightswatch Start, reload Jar, Stop a topology Please follow Controlling the topology for more details","title":"Deployment to App Box"},{"location":"deployment/#deployment-to-supervisor-instances","text":"We have an ansible playbook that can be used to deploy on `fk-transact-nightswatch` and `fk-3p-storm` on all supervisor instances.","title":"Deployment to Supervisor instances"},{"location":"deployment/#inventory","text":"Inventory in Ansible world refers to the list of instances where a particular playbook has to be run. It is defined by the hosts file. For Nightswatch, the hosts file has been placed at ROOT/dev-support/storm-deployment/inventory/hosts . It contains a list of IP addresses of supervisors on which we wish to deploy in the following format. $xslt [supervisors] 10.32.2.167 10.33.2.234 There is also a script $ROOT/dev-support/storm-deployment/populate-inventory.sh which can be used to fetch a list of supervisors for chennai and hyderabad zone for production app and update the hosts file. Usage: ./PATH_TO/populate_inventory.sh -z ZONE where ZONE is either chennai or hyderabad .","title":"Inventory"},{"location":"deployment/#configuration","text":"The playbook can be configured using a json file that lists all the options. It is placed at $ROOT/dev-support/storm-deployment/env.json . The file has following options { \"rolling\": 4, \"nightswatch_build\": 2797, \"storm_build\": 21, \"reinstall_storm\": false } rolling : No. of instances ansible should in parallel run the playbook. nightswatch-build : Build number of fk-transact-nightswatch that you wish to install. reinstall-storm : A boolean variable indicating whether fk-3p-storm has to be installed on the instances. storm-build : Build number of prod-id-storm to be installed. This is ignored if reinstall-storm is set to False","title":"Configuration"},{"location":"deployment/#running-the-playbook","text":"Pre-requisites: Install ansible-playbook in your system. Populate inventory/hosts file with appropriate list of IPs Get sudo permission for app prod-nightswatch Playbook Execution: ansible-playbook deploy_to_supervisors.yaml -i inventory/hosts --extra-vars \"@env.json\" -vvv Note: The -vvv option is to print verbose logs for every step. Note: This script only works for master branch. For other branches, please edit $ROOT/dev-support/storm-deployment/reinstall_nightswatch with proper branch name.","title":"Running the playbook."},{"location":"introduction/","text":"To provide a platform to enable stateful systems to handle reconciliations. Anywhere, some kind of consistency needs to be maintained across or within system states, it can be termed as reconciliation. Nightswatch, basically, ensures that flows across or within systems should be smooth and happen within some time frame. Nightswatch thus provides automated anomaly handling. When it comes to functionality , the use cases can be broadly classified into the following: Ensuring state transitions happens within pre specified TAT by invoking certain action if that TAT is breached. This is classic scheduler use case. However, Nightswatch provides the ability to retry a fixed number of times. Example: If an order remains in Initialized state and has not approved within a certain defined time, Nightswatch ensures such orders do not remain stuck in Initialized state. State propagation across systems. An entity might have a certain state in one system but hasn\u2019t reached the corresponding state in another system. Example: Recon between PG and checkout. Customer might have paid at the PG but the response did not reach checkout. Now this payment entity has inconsistent state between pg and checkout. Nightswatch ensures this anomaly is resolved within a fixed defined time. Raising alert/taking action, in case even after repeated attempts from Nightswatch the state of the system hasn't moved. These anomalies are detected based on logic provided by the systems who own the use cases. e.g. which state, what time, etc. These are configurable. Once an anomaly is identified it is handled as per the rules and policies provided by clients. e.g. which api to hit, what action to take, etc. Only action to be supported in the future is calling a varadhi api","title":"Introduction"},{"location":"kafkaOffsetManager/","text":"App Box Details Note Select the App Box from the instance group mentioned Chennai App Box Instance Group: storm-app-v3 Hyderabad App Box Instance Group: storm-app Please raise a bastion request for non-sudo permissions for prod-nightswatch app-id Nightswatch Kafka Offset Manager supports only Kafka 2.x versions View Offsets status sudo /etc/init.d/fk-transact-nightswatch offset-manager (topology-name) (broker:port) (partition) describe Options The options are described below: topology-name : Name of the topology. e.g. nw-preprod broker:port : One of the broker ip:port of the kafka cluster belonging to the topology. e.g. 10.33.134.36:9092 partition : Partition for which offsets are to be migrated. Options are \"all\", partition-number describe : This option describes the consumer group belonging the given topology. Example Command For All Partitions sudo /etc/init.d/fk-transact-nightswatch offset-manager nw-preprod 10.33.134.36:9092 all describe Example Output Consumer group nw-preprod-source_nw_preprod has no active members. TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-ID yggrite-201-2 1 3 3 0 - - - yggrite-201-2 0 5 5 0 - - - yggrite-201-2 2 2 2 0 - - - Reset Offsets to a DateTime sudo /etc/init.d/fk-transact-nightswatch offset-manager (topology-name) (broker:port) (partition) datetime (date-time-value) test Options The options are described below: topology-name : Name of the topology. e.g. nw-preprod broker:port : One of the broker ip:port of the kafka cluster belonging to the topology. e.g. 10.33.134.36:9092 partition : Partition for which offsets are to be migrated. Options are \"all\", partition-number datetime : This option migrates all the offsets to the date time mentioned in date-time-value. date-time-value : Format is YYYY-MM-DDTHH:mm:SS.sss GMT test : This is an optional flag. If present, it will only show the results instead of actually executing the commands on the kafka Example Command sudo /etc/init.d/fk-transact-nightswatch offset-manager nw-preprod 10.33.134.36:9092 all datetime 2020-08-07T11:30:00.000 Output TOPIC PARTITION NEW-OFFSET yggrite-201-2 2 1 yggrite-201-2 1 2 yggrite-201-2 0 1 Shift Offsets forward or reverse by an integer sudo /etc/init.d/fk-transact-nightswatch offset-manager (topology-name) (broker:port) (partition) relative (offset-shift) test Options The options are described below: topology-name : Name of the topology. e.g. nw-preprod broker:port : One of the broker ip:port of the kafka cluster belonging to the topology. e.g. 10.33.134.36:9092 partition : Partition for which offsets are to be migrated. Options are \"all\", partition-number relative : This option shifts the offsets forward or reverse by the given integer. offset-shift : +ve if you want to increase offsets and -ve if you want to decrease the offsets. This is limited by EARLIEST_OFFSET and LATEST_OFFSET. test : This is an optional flag. If present, it will only show the results instead of actually executing the commands on the kafka kafka-topic:partition : This is an optional flag. If Offsets are required to be migrated for only 1 partition, then this is to be used. e.g. nw-preprod:0 Example Command sudo /etc/init.d/fk-transact-nightswatch offset-manager nw-preprod 10.33.134.36:9092 0 relative 10 Output TOPIC PARTITION NEW-OFFSET yggrite-201-2 0 11 In case of any issues or help please contact id-dev@flipkart.com","title":"Kafka Offset Manager"},{"location":"kafkaOffsetManager/#app-box-details","text":"Note Select the App Box from the instance group mentioned Chennai App Box Instance Group: storm-app-v3 Hyderabad App Box Instance Group: storm-app Please raise a bastion request for non-sudo permissions for prod-nightswatch app-id Nightswatch Kafka Offset Manager supports only Kafka 2.x versions","title":"App Box Details"},{"location":"kafkaOffsetManager/#view-offsets-status","text":"sudo /etc/init.d/fk-transact-nightswatch offset-manager (topology-name) (broker:port) (partition) describe","title":"View Offsets status"},{"location":"kafkaOffsetManager/#options","text":"The options are described below: topology-name : Name of the topology. e.g. nw-preprod broker:port : One of the broker ip:port of the kafka cluster belonging to the topology. e.g. 10.33.134.36:9092 partition : Partition for which offsets are to be migrated. Options are \"all\", partition-number describe : This option describes the consumer group belonging the given topology.","title":"Options"},{"location":"kafkaOffsetManager/#example-command","text":"","title":"Example Command"},{"location":"kafkaOffsetManager/#for-all-partitions","text":"sudo /etc/init.d/fk-transact-nightswatch offset-manager nw-preprod 10.33.134.36:9092 all describe","title":"For All Partitions"},{"location":"kafkaOffsetManager/#example-output","text":"Consumer group nw-preprod-source_nw_preprod has no active members. TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-ID yggrite-201-2 1 3 3 0 - - - yggrite-201-2 0 5 5 0 - - - yggrite-201-2 2 2 2 0 - - -","title":"Example Output"},{"location":"kafkaOffsetManager/#reset-offsets-to-a-datetime","text":"sudo /etc/init.d/fk-transact-nightswatch offset-manager (topology-name) (broker:port) (partition) datetime (date-time-value) test","title":"Reset Offsets to a DateTime"},{"location":"kafkaOffsetManager/#options_1","text":"The options are described below: topology-name : Name of the topology. e.g. nw-preprod broker:port : One of the broker ip:port of the kafka cluster belonging to the topology. e.g. 10.33.134.36:9092 partition : Partition for which offsets are to be migrated. Options are \"all\", partition-number datetime : This option migrates all the offsets to the date time mentioned in date-time-value. date-time-value : Format is YYYY-MM-DDTHH:mm:SS.sss GMT test : This is an optional flag. If present, it will only show the results instead of actually executing the commands on the kafka","title":"Options"},{"location":"kafkaOffsetManager/#example-command_1","text":"sudo /etc/init.d/fk-transact-nightswatch offset-manager nw-preprod 10.33.134.36:9092 all datetime 2020-08-07T11:30:00.000","title":"Example Command"},{"location":"kafkaOffsetManager/#output","text":"TOPIC PARTITION NEW-OFFSET yggrite-201-2 2 1 yggrite-201-2 1 2 yggrite-201-2 0 1","title":"Output"},{"location":"kafkaOffsetManager/#shift-offsets-forward-or-reverse-by-an-integer","text":"sudo /etc/init.d/fk-transact-nightswatch offset-manager (topology-name) (broker:port) (partition) relative (offset-shift) test","title":"Shift Offsets forward or reverse by an integer"},{"location":"kafkaOffsetManager/#options_2","text":"The options are described below: topology-name : Name of the topology. e.g. nw-preprod broker:port : One of the broker ip:port of the kafka cluster belonging to the topology. e.g. 10.33.134.36:9092 partition : Partition for which offsets are to be migrated. Options are \"all\", partition-number relative : This option shifts the offsets forward or reverse by the given integer. offset-shift : +ve if you want to increase offsets and -ve if you want to decrease the offsets. This is limited by EARLIEST_OFFSET and LATEST_OFFSET. test : This is an optional flag. If present, it will only show the results instead of actually executing the commands on the kafka kafka-topic:partition : This is an optional flag. If Offsets are required to be migrated for only 1 partition, then this is to be used. e.g. nw-preprod:0","title":"Options"},{"location":"kafkaOffsetManager/#example-command_2","text":"sudo /etc/init.d/fk-transact-nightswatch offset-manager nw-preprod 10.33.134.36:9092 0 relative 10","title":"Example Command"},{"location":"kafkaOffsetManager/#output_1","text":"TOPIC PARTITION NEW-OFFSET yggrite-201-2 0 11 In case of any issues or help please contact id-dev@flipkart.com","title":"Output"},{"location":"migration/","text":"Scripts for Migration Box IP: 10.32.146.21 Location: /etc/fk-transact-nightswatch/nw-scripts Lag Monitor Script Usage: cd /etc/fk-transact-nightswatch/nw-scripts java -cp nw-scripts-1.0.0.jar flipkart.cp.transact.nw.kafka.lagmonitor.LagMonitor zkHost:zkport zkPath-to-kafka-offsets kafka-topic kafka-broker:kafka-port path-to-kafka-run.sh java -cp nw-scripts-1.0.0.jar flipkart.cp.transact.nw.kafka.lagmonitor.LagMonitor 10 .32.233.56:2181 /nightswatch-2.0/payments-authcapture-source_payments/payments-authcapture-source_payments unicorn-nw-v2 10 .32.174.239:9092 /etc/fk-transact-nightswatch/nw-scripts Save Zk Entries to a File Usage: cd /etc/fk-transact-nightswatch/nw-scripts java -cp nw-scripts-1.0.0.jar flipkart.cp.transact.nw.scripts.OffsetHandler type zkHost:zkport zkPath-to-kafka-offsets kafka-topic file-location java -cp nw-scripts-1.0.0.jar flipkart.cp.transact.nw.scripts.OffsetHandler save 10 .32.233.56:2181 /nightswatch-2.0/payments-authcapture-source_payments/payments-authcapture-source_payments unicorn-nw-v2 10 .32.174.239:9092 /home/aurobindo.m/migration/europa-pre-RECON Migrate Zk Entries from one location to another in Zookeeper Usage: cd /etc/fk-transact-nightswatch/nw-scripts java -cp nw-scripts-1.0.0.jar flipkart.cp.transact.nw.scripts.OffsetHandler type kafka-topic sourceZkHost:zkport sourceZkPath-to-kafka-offsets destZkHost:zkport destZkPath-to-kafka-offsets file-location java -cp nw-scripts-1.0.0.jar flipkart.cp.transact.nw.scripts.OffsetHandler migrateZ2Z unicorn-nw-v2 10 .32.233.56:2181 /nightswatch/europa-postsch-RECON-nw-erecon 10 .32.233.56:2181 /nightswatch-2.0/europa-postsch-RECON-nw-erecon Migrate Kafka Offsets from Kafka to Zookeeper Usage: cd /etc/fk-transact-nightswatch/nw-scripts java -cp nw-scripts-1.0.0.jar flipkart.cp.transact.nw.scripts.OffsetHandler type kafka-topic sourceZkHost:zkport sourceZkPath-to-kafka-offsets destZkHost:zkport destZkPath-to-kafka-offsets file-location java -cp nw-scripts-1.0.0.jar flipkart.cp.transact.nw.scripts.OffsetHandler migrateZ2Z unicorn-nw-v2 10 .32.233.56:2181 /nightswatch/europa-postsch-RECON-nw-erecon 10 .32.233.56:2181 /nightswatch-2.0/europa-postsch-RECON-nw-erecon Migrate to storm 2.0.0 for Kafka Version 0.8.x Stop 0.9.5 topology in old cluster Save Zk Entries save zk entries in file (initial_backup.log) [In case anything goes wrong] OffsetHandler save Migrate Zk Entries from old Znode to new Znode OffsetHandler migrate Start 2.0.0 Topology in new cluster Migrate to storm 2.0.0 for Kafka Version = 0.10.x Stop 0.9.5 topology in old cluster Save Zk Entries save zk entries in file (initial_backup.log) [In case anything goes wrong] OffsetHandler save Run StormToKafka script Github link Start 2.0.0 Topology in new cluster Migrate to storm 0.9.5 for Kafka Version = 0.10.x Stop 2.0.0 topology in new cluster Save Zk Entries save zk entries in file (initial_backup.log) [In case anything goes wrong] OffsetHandler save Migrate Kafka offsets from Kafka to File (kafka_offsets.log) OffsetHandler migrateK2Z Remove znode containing old kafka offsets from zookeeper rmr zk-dir Load Zk Entries from (kafka_offsets.log) file to Zookeeper OffsetHandler load Start 0.9.5 Topology in old cluster Migrate to storm 0.9.5 for Kafka Version = 0.8.x Stop 2.0.0 topology in new cluster Save Zk Entries save zk entries in file (kafka_offsets.log) [In case anything goes wrong] OffsetHandler save Remove znode containing old kafka offsets from zookeeper rmr zk-dir Migrate Zk Entries from new Znode to old Znode OffsetHandler migrate Start 0.9.5 topology","title":"Storm Migration and RollBack"},{"location":"migration/#scripts-for-migration","text":"Box IP: 10.32.146.21 Location: /etc/fk-transact-nightswatch/nw-scripts","title":"Scripts for Migration"},{"location":"migration/#lag-monitor-script","text":"Usage: cd /etc/fk-transact-nightswatch/nw-scripts java -cp nw-scripts-1.0.0.jar flipkart.cp.transact.nw.kafka.lagmonitor.LagMonitor zkHost:zkport zkPath-to-kafka-offsets kafka-topic kafka-broker:kafka-port path-to-kafka-run.sh java -cp nw-scripts-1.0.0.jar flipkart.cp.transact.nw.kafka.lagmonitor.LagMonitor 10 .32.233.56:2181 /nightswatch-2.0/payments-authcapture-source_payments/payments-authcapture-source_payments unicorn-nw-v2 10 .32.174.239:9092 /etc/fk-transact-nightswatch/nw-scripts","title":"Lag Monitor Script"},{"location":"migration/#save-zk-entries-to-a-file","text":"Usage: cd /etc/fk-transact-nightswatch/nw-scripts java -cp nw-scripts-1.0.0.jar flipkart.cp.transact.nw.scripts.OffsetHandler type zkHost:zkport zkPath-to-kafka-offsets kafka-topic file-location java -cp nw-scripts-1.0.0.jar flipkart.cp.transact.nw.scripts.OffsetHandler save 10 .32.233.56:2181 /nightswatch-2.0/payments-authcapture-source_payments/payments-authcapture-source_payments unicorn-nw-v2 10 .32.174.239:9092 /home/aurobindo.m/migration/europa-pre-RECON","title":"Save Zk Entries to a File"},{"location":"migration/#migrate-zk-entries-from-one-location-to-another-in-zookeeper","text":"Usage: cd /etc/fk-transact-nightswatch/nw-scripts java -cp nw-scripts-1.0.0.jar flipkart.cp.transact.nw.scripts.OffsetHandler type kafka-topic sourceZkHost:zkport sourceZkPath-to-kafka-offsets destZkHost:zkport destZkPath-to-kafka-offsets file-location java -cp nw-scripts-1.0.0.jar flipkart.cp.transact.nw.scripts.OffsetHandler migrateZ2Z unicorn-nw-v2 10 .32.233.56:2181 /nightswatch/europa-postsch-RECON-nw-erecon 10 .32.233.56:2181 /nightswatch-2.0/europa-postsch-RECON-nw-erecon","title":"Migrate Zk Entries from one location to another in Zookeeper"},{"location":"migration/#migrate-kafka-offsets-from-kafka-to-zookeeper","text":"Usage: cd /etc/fk-transact-nightswatch/nw-scripts java -cp nw-scripts-1.0.0.jar flipkart.cp.transact.nw.scripts.OffsetHandler type kafka-topic sourceZkHost:zkport sourceZkPath-to-kafka-offsets destZkHost:zkport destZkPath-to-kafka-offsets file-location java -cp nw-scripts-1.0.0.jar flipkart.cp.transact.nw.scripts.OffsetHandler migrateZ2Z unicorn-nw-v2 10 .32.233.56:2181 /nightswatch/europa-postsch-RECON-nw-erecon 10 .32.233.56:2181 /nightswatch-2.0/europa-postsch-RECON-nw-erecon","title":"Migrate Kafka Offsets from Kafka to Zookeeper"},{"location":"migration/#migrate-to-storm-200-for-kafka-version-08x","text":"Stop 0.9.5 topology in old cluster Save Zk Entries save zk entries in file (initial_backup.log) [In case anything goes wrong] OffsetHandler save Migrate Zk Entries from old Znode to new Znode OffsetHandler migrate Start 2.0.0 Topology in new cluster","title":"Migrate to storm 2.0.0 for Kafka Version 0.8.x"},{"location":"migration/#migrate-to-storm-200-for-kafka-version-010x","text":"Stop 0.9.5 topology in old cluster Save Zk Entries save zk entries in file (initial_backup.log) [In case anything goes wrong] OffsetHandler save Run StormToKafka script Github link Start 2.0.0 Topology in new cluster","title":"Migrate to storm 2.0.0 for Kafka Version &gt;= 0.10.x"},{"location":"migration/#migrate-to-storm-095-for-kafka-version-010x","text":"Stop 2.0.0 topology in new cluster Save Zk Entries save zk entries in file (initial_backup.log) [In case anything goes wrong] OffsetHandler save Migrate Kafka offsets from Kafka to File (kafka_offsets.log) OffsetHandler migrateK2Z Remove znode containing old kafka offsets from zookeeper rmr zk-dir Load Zk Entries from (kafka_offsets.log) file to Zookeeper OffsetHandler load Start 0.9.5 Topology in old cluster","title":"Migrate to storm 0.9.5 for Kafka Version &gt;= 0.10.x"},{"location":"migration/#migrate-to-storm-095-for-kafka-version-08x","text":"Stop 2.0.0 topology in new cluster Save Zk Entries save zk entries in file (kafka_offsets.log) [In case anything goes wrong] OffsetHandler save Remove znode containing old kafka offsets from zookeeper rmr zk-dir Migrate Zk Entries from new Znode to old Znode OffsetHandler migrate Start 0.9.5 topology","title":"Migrate to storm 0.9.5 for Kafka Version &gt;= 0.8.x"},{"location":"nightswatchLocal/","text":"Setup Repo on IntelliJ Clone Github Repo git clone https://github.fkinternal.com/Flipkart/nightswatch.git Open Nightswatch in IntelliJ nightswatch modules pom.xml Run configuration details in IntelliJ to run Nightswatch in local Main Class : flipkart.transact.commons.storm.utils.StormStarter VM Options : -DaopType=GUICE Program Arguements : -l -pn v2 -pp flipkart.cp.transact.nw -tag {topo-name} Working Directory : /Path/to/nightswatch/modules Environment Variables : Classpath for module : nw-biz JRE : default (1.8) Copy configuration files to local Copy contents of /etc/hosts from preprod app box to local Copy /etc/fk-transact-nightswatch/appcfg/nw-yak.json from preprod app box to local box in same location Create /etc/fk-transact-nightswatch/appcfg/nightswatch-app-config.yml file in local box with the contents mentioned below bucket: app.bucket: local-nightswatch-app-config override.bucket: local-nightswatch-override-config Add topology jar to /etc/fk-transact-nightswatch/external_jars/ in local box Rename the topology jar file name to (topology-name.jar) Add client jar dependency in Nightswatch to run in local Add Client Jar Maven dependency to your project Add this dependency in nw-biz pom.xml file dependency groupId {client.group.id} /groupId artifactId {client.artifact.id} /artifactId version {client.version} /version /dependency Things to Remember Decrease the bolts and worker counts to 1 when you run in local Please take non sudo permission for \"preprod-nightswatch\" app id to access preprod box Preprod App Box details Preprod app ip: 10.33.29.112 Preprod app vpc: Fk-Preprod","title":"Setup on Local"},{"location":"nightswatchLocal/#setup-repo-on-intellij","text":"Clone Github Repo git clone https://github.fkinternal.com/Flipkart/nightswatch.git Open Nightswatch in IntelliJ nightswatch modules pom.xml Run configuration details in IntelliJ to run Nightswatch in local Main Class : flipkart.transact.commons.storm.utils.StormStarter VM Options : -DaopType=GUICE Program Arguements : -l -pn v2 -pp flipkart.cp.transact.nw -tag {topo-name} Working Directory : /Path/to/nightswatch/modules Environment Variables : Classpath for module : nw-biz JRE : default (1.8)","title":"Setup Repo on IntelliJ"},{"location":"nightswatchLocal/#copy-configuration-files-to-local","text":"Copy contents of /etc/hosts from preprod app box to local Copy /etc/fk-transact-nightswatch/appcfg/nw-yak.json from preprod app box to local box in same location Create /etc/fk-transact-nightswatch/appcfg/nightswatch-app-config.yml file in local box with the contents mentioned below bucket: app.bucket: local-nightswatch-app-config override.bucket: local-nightswatch-override-config Add topology jar to /etc/fk-transact-nightswatch/external_jars/ in local box Rename the topology jar file name to (topology-name.jar)","title":"Copy configuration files to local"},{"location":"nightswatchLocal/#add-client-jar-dependency-in-nightswatch-to-run-in-local","text":"Add Client Jar Maven dependency to your project Add this dependency in nw-biz pom.xml file dependency groupId {client.group.id} /groupId artifactId {client.artifact.id} /artifactId version {client.version} /version /dependency Things to Remember Decrease the bolts and worker counts to 1 when you run in local Please take non sudo permission for \"preprod-nightswatch\" app id to access preprod box Preprod App Box details Preprod app ip: 10.33.29.112 Preprod app vpc: Fk-Preprod","title":"Add client jar dependency in Nightswatch to run in local"},{"location":"releaseNotes/","text":"Nightswatch Wall dependency groupId flipkart.cp.transact /groupId artifactId nw-wall /artifactId version 3.1.8 /version scope provided /scope /dependency Nightswatch Client dependency groupId flipkart.cp.transact.nightswatch /groupId artifactId nw-client-shaded /artifactId version 2.12 /version /dependency Changes Notes September 14, 2020 Version nw-shaded-client:2.12 Description Clients will be able to add events to Nightswatch using a Rest Endpoint instead of adding to their kafka. April 10, 2019 Version nw-wall:3.1.8 Description Client don't need to write their logic in Nightswatch and rather should provide a jar containing implementations of Interfaces provided by Nightswatch. Thus reducing their dependency on Nightswatch. Nov 28, 2019 Version nw-client:2.9 and nw-client-shade-models:2.10 Description Removed all unnecessary dependencies Only has com.fasterxml.jackson.core:jackson-annotations-2.7.1 dependency Jan 24, 2019 Version nw-client:2.8 and nw-client-shade-models:2.9 Description Nightswatch will send SOURCE (DEFAULT/SIDELINE) as a part of the ReconRequest. Default Flow: NwReconRequest{ id= OD1143966195155820 , attemptNumber=0, lastAttempt=false, topologyName= eur-RECON , source=DEFAULT, reason= OD1143966195155820 , extraFieldsMap={checkoutId=OD1143966195155820, europaState=CHECKOUT_INTERMEDIATE_EXECUTION_COMPLETED} } UnSideline Flow: NwReconRequest{ id= OD1143966195155820 , attemptNumber=0, lastAttempt=false, topologyName= eur-RECON , source=SIDELINE, reason= OD1143966195155820 , extraFieldsMap={checkoutId=OD1143966195155820, europaState=CHECKOUT_INTERMEDIATE_EXECUTION_COMPLETED} }","title":"Release Notes"},{"location":"releaseNotes/#nightswatch-wall","text":"dependency groupId flipkart.cp.transact /groupId artifactId nw-wall /artifactId version 3.1.8 /version scope provided /scope /dependency","title":"Nightswatch Wall"},{"location":"releaseNotes/#nightswatch-client","text":"dependency groupId flipkart.cp.transact.nightswatch /groupId artifactId nw-client-shaded /artifactId version 2.12 /version /dependency","title":"Nightswatch Client"},{"location":"releaseNotes/#changes-notes","text":"","title":"Changes Notes"},{"location":"releaseNotes/#september-14-2020","text":"Version nw-shaded-client:2.12 Description Clients will be able to add events to Nightswatch using a Rest Endpoint instead of adding to their kafka.","title":"September 14, 2020"},{"location":"releaseNotes/#april-10-2019","text":"Version nw-wall:3.1.8 Description Client don't need to write their logic in Nightswatch and rather should provide a jar containing implementations of Interfaces provided by Nightswatch. Thus reducing their dependency on Nightswatch.","title":"April 10, 2019"},{"location":"releaseNotes/#nov-28-2019","text":"Version nw-client:2.9 and nw-client-shade-models:2.10 Description Removed all unnecessary dependencies Only has com.fasterxml.jackson.core:jackson-annotations-2.7.1 dependency","title":"Nov 28, 2019"},{"location":"releaseNotes/#jan-24-2019","text":"Version nw-client:2.8 and nw-client-shade-models:2.9 Description Nightswatch will send SOURCE (DEFAULT/SIDELINE) as a part of the ReconRequest. Default Flow: NwReconRequest{ id= OD1143966195155820 , attemptNumber=0, lastAttempt=false, topologyName= eur-RECON , source=DEFAULT, reason= OD1143966195155820 , extraFieldsMap={checkoutId=OD1143966195155820, europaState=CHECKOUT_INTERMEDIATE_EXECUTION_COMPLETED} } UnSideline Flow: NwReconRequest{ id= OD1143966195155820 , attemptNumber=0, lastAttempt=false, topologyName= eur-RECON , source=SIDELINE, reason= OD1143966195155820 , extraFieldsMap={checkoutId=OD1143966195155820, europaState=CHECKOUT_INTERMEDIATE_EXECUTION_COMPLETED} }","title":"Jan 24, 2019"},{"location":"dev-support/noah/noahSetup/","text":"Install Noah in one of the Zookeeper Instances https://confluence.fkinternal.com/display/FCP/Installing+Noah+Packages Restart Noah controller after installing Noah sudo /usr/bin/braas-controller-restart.sh Update Noah Configs from iaas box Click to download the Noah Config file: payload.json Change 'backup_from_instance_id' key with correct instance id of the zookeeper box where noah is installed Upload noah config for nightswatch noah config update -n nightswtch-config -f payload.json Expected Response: (200, '', 'Config state : VALIDATED. ') Validate if Noah Config is uploaded correctly http://10.47.4.180/configs/nightswtch-config Verify in Noah Dashboard http://10.47.5.200/team/Indradhanush_team","title":"Noah Zookeeper Setup"},{"location":"dev-support/setup/kafkaLagMonitorSetup/","text":"Steps to create Kafka Lag Monitor Box Create a Kafka Lag Monitor Config Bucket Reference Config Bucket: http://10.47.3.62/#/list-keys/prod-nightswatch-kafka-lag-monitor kafkaStormLagMonitorConfig : This is for kafka 0.8.x version where storm stores Offsets in Zookeeper zkIps : Comma Separated values of Zk ips with port where Consumer offsets are stored baseZkPath : Zookeeper path where the Consumer offsets are stored kafkaOffsetsStormLagMonitorConfig : This is for kafka 0.10.x + version where storm stores Offsets in Kafka \"kafkaBrokers : Comma separted values of kafka host with port of the brokers where consumer offset are stored topic : Kafka Topic of concern consumerGroupId : Consumer group id of the Storm Kafka consumer. It is same as spout name Use prod-nightswatch-setup-script.sh to create Kafka Lag Monitor box Config Bucket name is hardcoded in the setup script file provided as prod-nightswatch-kafka-lag-monitor Verify if configurations from config service are reflected in the box cat /etc/fk-w3-transact-lag-monitor/appcfg/lag-monitor.yml Run the following commads to start the lag monitor sudo /etc/init.d/fk-w3-transact-lag-monitor start Verify Lag Monitor is running Check Metrics in suggest and querry APIs Suggest: This is a cosmos API to get all metrics published with a certain prefix Prefix: prod-nightswatch._AbstractLagMonitorTask URL : http://10.47.4.67/api/suggest?type=metrics max=9999 q=prod-nightswatch._AbstractLagMonitorTask Query: This is a cosmos API to get values of metrics published to cosmos URL : http://10.47.0.183/api/query?start=6m-ago end=1m-ago m=max:prod-nightswatch._AbstractLagMonitorTask.eur-RECON-source_checkout","title":"Kafka Lag Monitor Setup"},{"location":"dev-support/setup/kafkaLagMonitorSetup/#steps-to-create-kafka-lag-monitor-box","text":"Create a Kafka Lag Monitor Config Bucket Reference Config Bucket: http://10.47.3.62/#/list-keys/prod-nightswatch-kafka-lag-monitor kafkaStormLagMonitorConfig : This is for kafka 0.8.x version where storm stores Offsets in Zookeeper zkIps : Comma Separated values of Zk ips with port where Consumer offsets are stored baseZkPath : Zookeeper path where the Consumer offsets are stored kafkaOffsetsStormLagMonitorConfig : This is for kafka 0.10.x + version where storm stores Offsets in Kafka \"kafkaBrokers : Comma separted values of kafka host with port of the brokers where consumer offset are stored topic : Kafka Topic of concern consumerGroupId : Consumer group id of the Storm Kafka consumer. It is same as spout name Use prod-nightswatch-setup-script.sh to create Kafka Lag Monitor box Config Bucket name is hardcoded in the setup script file provided as prod-nightswatch-kafka-lag-monitor Verify if configurations from config service are reflected in the box cat /etc/fk-w3-transact-lag-monitor/appcfg/lag-monitor.yml Run the following commads to start the lag monitor sudo /etc/init.d/fk-w3-transact-lag-monitor start","title":"Steps to create Kafka Lag Monitor Box"},{"location":"dev-support/setup/kafkaLagMonitorSetup/#verify-lag-monitor-is-running","text":"Check Metrics in suggest and querry APIs Suggest: This is a cosmos API to get all metrics published with a certain prefix Prefix: prod-nightswatch._AbstractLagMonitorTask URL : http://10.47.4.67/api/suggest?type=metrics max=9999 q=prod-nightswatch._AbstractLagMonitorTask Query: This is a cosmos API to get values of metrics published to cosmos URL : http://10.47.0.183/api/query?start=6m-ago end=1m-ago m=max:prod-nightswatch._AbstractLagMonitorTask.eur-RECON-source_checkout","title":"Verify Lag Monitor is running"},{"location":"dev-support/setup/kafkaSetup/","text":"Steps to create Kafka Lag Monitor Box Create a Kafka Config Bucket having the server and log properties Reference Config Bucket: http://10.47.3.62/#/list-keys/prod-nightswatch-kafka-config Use prod-nightswatch-setup-script.sh to create Kafka Brokers Warning Change the brokerId in the setup-script to a different number for each broker Config Bucket name is hardcoded in the setup script file provided as prod-nightswatch-kafka-config\" Start Kafka process on all the brokers created sudo systemctl start fk-3p-kafka Verify if kafka is started on all the brokers ps -ef | grep kafka Kafka Commands for Management Create a topic ./kafka-topics.sh --zookeeper prod-nightswatch-zookeeper1:2181,prod-nightswatch-zookeeper2:2181,prod-nightswatch-zookeeper3:2181,prod-nightswatch-zookeeper4:2181,prod-nightswatch-zookeeper5:2181/kafka --create --topic (topic-name) --partitions (no-of-partitions) --replication-factor (replication-factor)","title":"Kafka Setup"},{"location":"dev-support/setup/kafkaSetup/#steps-to-create-kafka-lag-monitor-box","text":"Create a Kafka Config Bucket having the server and log properties Reference Config Bucket: http://10.47.3.62/#/list-keys/prod-nightswatch-kafka-config Use prod-nightswatch-setup-script.sh to create Kafka Brokers Warning Change the brokerId in the setup-script to a different number for each broker Config Bucket name is hardcoded in the setup script file provided as prod-nightswatch-kafka-config\" Start Kafka process on all the brokers created sudo systemctl start fk-3p-kafka Verify if kafka is started on all the brokers ps -ef | grep kafka","title":"Steps to create Kafka Lag Monitor Box"},{"location":"dev-support/setup/kafkaSetup/#kafka-commands-for-management","text":"Create a topic ./kafka-topics.sh --zookeeper prod-nightswatch-zookeeper1:2181,prod-nightswatch-zookeeper2:2181,prod-nightswatch-zookeeper3:2181,prod-nightswatch-zookeeper4:2181,prod-nightswatch-zookeeper5:2181/kafka --create --topic (topic-name) --partitions (no-of-partitions) --replication-factor (replication-factor)","title":"Kafka Commands for Management"},{"location":"dev-support/setup/yakSetup/","text":"Steps to setup a Yak cluster Nightswatch Tenant is a part of prod-id-yak-archive cluster Name Chennai Config Bucket Hyderabad Config Bucket Cluster prod-id-yak-archive http://10.47.3.62/#/list-keys/prod-id-yak-archive http://10.24.0.33/#/list-keys/prod-id-yak-archive Tenant prod-id-nightswatch http://10.47.3.62/#/list-keys/yak-prod_id_nightswatch http://10.24.0.33/#/list-keys/yak-prod_id_nightswatch Setup Guide to follow: https://github.fkinternal.com/pages/Flipkart/yak/adminstration/tenant/ Command to create Yak DN ./create_instance.sh ./config/chennai-dc.sh prod-nightswatch-yak i4a-1024.c8-m40 dn aurobindo.m 85828 prod_id_nightswatch prod-id-yak-archive Indradhanush debian-9-guest Fk-Prod prod 1 In Step 4, Update etc-host of both buckets (chennai as well as hyderabad) since Yak Inter-cluster Replication is enabled Things to do before Step 5 of the above document Add DataNode hostnames to \"datanodes\" key of cluster config bucket Add client config bucket name in \"host_populator_buckets\" key of cluster config bucket Commands to use Add RSGroup add_rsgroup 'nightswatch' Move Servers to RSGroup: move_servers_rsgroup 'nightswatch', ['preprod-yak-nightswatch-dn-1:16020', 'preprod-yak-nightswatch-dn-2:16020', 'preprod-yak-nightswatch-dn-1:16020'] Change the hostnames to the newly created hostnames Create NameSpace: create_namespace 'nightswatch', { 'hbase.rsgroup.name' = 'prod_id_nightswatch', 'hbase.yak.payload.threshold' = 102400} Create Tables: create 'nightswatch:app', {NAME = 'cd', BLOOMFILTER = 'ROW', VERSIONS = '1', IN_MEMORY = 'false', KEEP_DELETED_CELLS = 'FALSE', DATA_BLOCK_ENCODING = 'NONE', TTL = '2592000', COMPRESSION = 'GZ', MIN_VERSIONS = '0', BLOCKCACHE = 'true', BLOCKSIZE = '65536', REPLICATION_SCOPE = '0'} create 'nightswatch:sideline', {NAME = 'cf', BLOOMFILTER = 'ROW', VERSIONS = '1', IN_MEMORY = 'false', KEEP_DELETED_CELLS = 'FALSE', DATA_BLOCK_ENCODING = 'NONE', TTL = '2592000', COMPRESSION = 'GZ', MIN_VERSIONS = '0', BLOCKCACHE = 'true', BLOCKSIZE = '65536', REPLICATION_SCOPE = '0'} create 'nightswatch:trace', {NAME = 'cd', BLOOMFILTER = 'ROW', VERSIONS = '1', IN_MEMORY = 'false', KEEP_DELETED_CELLS = 'FALSE', DATA_BLOCK_ENCODING = 'NONE', TTL = '2592000', COMPRESSION = 'GZ', MIN_VERSIONS = '0', BLOCKCACHE = 'true', BLOCKSIZE = '65536', REPLICATION_SCOPE = '0'} Grant permissions to user fk-3p-storm for the namespace \"nightswatch\": grant 'fk-3p-storm', 'RWXCA', '@nightswatch'","title":"Yak Setup"},{"location":"dev-support/setup/yakSetup/#steps-to-setup-a-yak-cluster","text":"Nightswatch Tenant is a part of prod-id-yak-archive cluster Name Chennai Config Bucket Hyderabad Config Bucket Cluster prod-id-yak-archive http://10.47.3.62/#/list-keys/prod-id-yak-archive http://10.24.0.33/#/list-keys/prod-id-yak-archive Tenant prod-id-nightswatch http://10.47.3.62/#/list-keys/yak-prod_id_nightswatch http://10.24.0.33/#/list-keys/yak-prod_id_nightswatch Setup Guide to follow: https://github.fkinternal.com/pages/Flipkart/yak/adminstration/tenant/ Command to create Yak DN ./create_instance.sh ./config/chennai-dc.sh prod-nightswatch-yak i4a-1024.c8-m40 dn aurobindo.m 85828 prod_id_nightswatch prod-id-yak-archive Indradhanush debian-9-guest Fk-Prod prod 1 In Step 4, Update etc-host of both buckets (chennai as well as hyderabad) since Yak Inter-cluster Replication is enabled Things to do before Step 5 of the above document Add DataNode hostnames to \"datanodes\" key of cluster config bucket Add client config bucket name in \"host_populator_buckets\" key of cluster config bucket","title":"Steps to setup a Yak cluster"},{"location":"dev-support/setup/yakSetup/#commands-to-use","text":"Add RSGroup add_rsgroup 'nightswatch' Move Servers to RSGroup: move_servers_rsgroup 'nightswatch', ['preprod-yak-nightswatch-dn-1:16020', 'preprod-yak-nightswatch-dn-2:16020', 'preprod-yak-nightswatch-dn-1:16020'] Change the hostnames to the newly created hostnames Create NameSpace: create_namespace 'nightswatch', { 'hbase.rsgroup.name' = 'prod_id_nightswatch', 'hbase.yak.payload.threshold' = 102400} Create Tables: create 'nightswatch:app', {NAME = 'cd', BLOOMFILTER = 'ROW', VERSIONS = '1', IN_MEMORY = 'false', KEEP_DELETED_CELLS = 'FALSE', DATA_BLOCK_ENCODING = 'NONE', TTL = '2592000', COMPRESSION = 'GZ', MIN_VERSIONS = '0', BLOCKCACHE = 'true', BLOCKSIZE = '65536', REPLICATION_SCOPE = '0'} create 'nightswatch:sideline', {NAME = 'cf', BLOOMFILTER = 'ROW', VERSIONS = '1', IN_MEMORY = 'false', KEEP_DELETED_CELLS = 'FALSE', DATA_BLOCK_ENCODING = 'NONE', TTL = '2592000', COMPRESSION = 'GZ', MIN_VERSIONS = '0', BLOCKCACHE = 'true', BLOCKSIZE = '65536', REPLICATION_SCOPE = '0'} create 'nightswatch:trace', {NAME = 'cd', BLOOMFILTER = 'ROW', VERSIONS = '1', IN_MEMORY = 'false', KEEP_DELETED_CELLS = 'FALSE', DATA_BLOCK_ENCODING = 'NONE', TTL = '2592000', COMPRESSION = 'GZ', MIN_VERSIONS = '0', BLOCKCACHE = 'true', BLOCKSIZE = '65536', REPLICATION_SCOPE = '0'} Grant permissions to user fk-3p-storm for the namespace \"nightswatch\": grant 'fk-3p-storm', 'RWXCA', '@nightswatch'","title":"Commands to use"},{"location":"dev-support/setup/zookeeperSetup/","text":"Steps to create a zookeeper cluster Create a zookeeper config bucket Reference Config Bucket: http://10.24.0.33/#/list-keys/prod-nightswatch-zk Delete the values of the following keys: hosts etc-hosts Use prod-nightswatch-setup-script.sh to create zookeeper boxes Update Zookeeper Bucket name in the setup script echo \"fk-auto-zoo fk-auto-zoo/bucket_name string $BUCKET_NAME\" | sudo -E debconf-set-selections\" After creating the boxes update the Zookeeper config bucket Update etc-hosts with hostname to ip mapping Update hosts details as: { id : $INSTANCE_ID , ip : $IP_ADDRESS , host : $HOSTNAME } Change hostname in all the box to the one in config bucket sudo hostname $HOSTNAME Logout and re-login to the box to verify hostname change Check ip using hostname -i Make sure that hostname -i works or else you won't get cosmos metrics Run the following commads on all boxes sudo apt-get install --yes --allow-unauthenticated fk-auto-zoo --reinstall sudo /etc/init.d/fk-zookeeper-server restart Verify Zookeeper is running Use netcat on the boxes to see the stats of zookeeper echo stat | nc localhost 2181 Check Dashboard to see if newly Added Nodes are reflected in Dashboard Zookeeper Cosmos Dashbaord Chennai Cluster: http://10.47.4.24/dashboard/script/zk_v3.js?app=prod-nightswatch-zookeeper refresh=1m orgId=1 Hyderabad Cluster: http://10.24.0.243/dashboard/script/zk_v3.js?app=prod-nightswatch-zookeeper refresh=1m orgId=1","title":"Zookeeper Setup"},{"location":"dev-support/setup/zookeeperSetup/#steps-to-create-a-zookeeper-cluster","text":"Create a zookeeper config bucket Reference Config Bucket: http://10.24.0.33/#/list-keys/prod-nightswatch-zk Delete the values of the following keys: hosts etc-hosts Use prod-nightswatch-setup-script.sh to create zookeeper boxes Update Zookeeper Bucket name in the setup script echo \"fk-auto-zoo fk-auto-zoo/bucket_name string $BUCKET_NAME\" | sudo -E debconf-set-selections\" After creating the boxes update the Zookeeper config bucket Update etc-hosts with hostname to ip mapping Update hosts details as: { id : $INSTANCE_ID , ip : $IP_ADDRESS , host : $HOSTNAME } Change hostname in all the box to the one in config bucket sudo hostname $HOSTNAME Logout and re-login to the box to verify hostname change Check ip using hostname -i Make sure that hostname -i works or else you won't get cosmos metrics Run the following commads on all boxes sudo apt-get install --yes --allow-unauthenticated fk-auto-zoo --reinstall sudo /etc/init.d/fk-zookeeper-server restart","title":"Steps to create a zookeeper cluster"},{"location":"dev-support/setup/zookeeperSetup/#verify-zookeeper-is-running","text":"Use netcat on the boxes to see the stats of zookeeper echo stat | nc localhost 2181 Check Dashboard to see if newly Added Nodes are reflected in Dashboard Zookeeper Cosmos Dashbaord Chennai Cluster: http://10.47.4.24/dashboard/script/zk_v3.js?app=prod-nightswatch-zookeeper refresh=1m orgId=1 Hyderabad Cluster: http://10.24.0.243/dashboard/script/zk_v3.js?app=prod-nightswatch-zookeeper refresh=1m orgId=1","title":"Verify Zookeeper is running"},{"location":"faq/basicFaq/","text":"On Which Supervisors are my topology running? We have provided an API to figure out the supervisor box ips on which the topology is running. Please refer Get Supervisors API from Debugging API table. Where can I find my topology logs? Once you figure out all the supervisor box where your topology is running, you'll be able to find the logs at this path: /var/log/fk-3p-storm/worker-6700.log How does the worker logs look like? 2020-07-14 13:02:28.499 [Thread-37-BUILDER-executor[875, 875]] 399-FK:OD1191564288295410__EUR_RECON DefaultBuilderAction [INFO] This is the log message This is how a log message looks in Nightswatch Storm worker box. The worker log can be segregated into 6 components: - Date: Date of the log (IST) in format yyyy-mm-dd - Time: Time of the log (IST) in format hh:mm:ss.ms - Thread: Thread id of the thread generating the log - Event-ID: Every log is associated with an event that is currently being processed. - Class: The class name of teh class that produces the log - Log message: The log message Where can I see the performance of my topology? Topology can be monitored on different Dashboards: Cosmos Dashboard: For Kafka Lags, Service Call latencies. Hystrix Dashboard: For all Service Call metrics, Bolt execution count metrics Storm Dashboard: For Topology performance in terms of bolt acks and fails. All Dashboards mentioned above are in Dashboards Page How do I restart my topology? Please follow this guide How do I deploy a quick fix in my topology? Follow the steps mentioned below: Change the code in your repository. Upload a new release version in Flipkart Artifactory. Change the jar version in config bucket. For more info see Configs to be provided . Restart the topology. Follow this guide to restart the topo with updated code.","title":"Basic FAQ"},{"location":"faq/basicFaq/#on-which-supervisors-are-my-topology-running","text":"We have provided an API to figure out the supervisor box ips on which the topology is running. Please refer Get Supervisors API from Debugging API table.","title":"On Which Supervisors are my topology running?"},{"location":"faq/basicFaq/#where-can-i-find-my-topology-logs","text":"Once you figure out all the supervisor box where your topology is running, you'll be able to find the logs at this path: /var/log/fk-3p-storm/worker-6700.log","title":"Where can I find my topology logs?"},{"location":"faq/basicFaq/#how-does-the-worker-logs-look-like","text":"2020-07-14 13:02:28.499 [Thread-37-BUILDER-executor[875, 875]] 399-FK:OD1191564288295410__EUR_RECON DefaultBuilderAction [INFO] This is the log message This is how a log message looks in Nightswatch Storm worker box. The worker log can be segregated into 6 components: - Date: Date of the log (IST) in format yyyy-mm-dd - Time: Time of the log (IST) in format hh:mm:ss.ms - Thread: Thread id of the thread generating the log - Event-ID: Every log is associated with an event that is currently being processed. - Class: The class name of teh class that produces the log - Log message: The log message","title":"How does the worker logs look like?"},{"location":"faq/basicFaq/#where-can-i-see-the-performance-of-my-topology","text":"Topology can be monitored on different Dashboards: Cosmos Dashboard: For Kafka Lags, Service Call latencies. Hystrix Dashboard: For all Service Call metrics, Bolt execution count metrics Storm Dashboard: For Topology performance in terms of bolt acks and fails. All Dashboards mentioned above are in Dashboards Page","title":"Where can I see the performance of my topology?"},{"location":"faq/basicFaq/#how-do-i-restart-my-topology","text":"Please follow this guide","title":"How do I restart my topology?"},{"location":"faq/basicFaq/#how-do-i-deploy-a-quick-fix-in-my-topology","text":"Follow the steps mentioned below: Change the code in your repository. Upload a new release version in Flipkart Artifactory. Change the jar version in config bucket. For more info see Configs to be provided . Restart the topology. Follow this guide to restart the topo with updated code.","title":"How do I deploy a quick fix in my topology?"},{"location":"faq/migrationFaq/","text":"Migrate from 2.0.0 to 0.9.5 MIgrate from 0.9.5 to 2.0.0","title":"migrationFaq"},{"location":"faq/migrationFaq/#migrate-from-200-to-095","text":"","title":"Migrate from 2.0.0 to 0.9.5"},{"location":"faq/migrationFaq/#migrate-from-095-to-200","text":"","title":"MIgrate from 0.9.5 to 2.0.0"},{"location":"faq/nightswatchFaq/","text":"What is my Topology Tag I am not getting calls from Nightswatch Places to look: In storm UI, Check if topology is running. If yes, check for Bolt failures. If any, open a box that runs that bolt and check logs to find out the reason of the failure. Check Client Endpoint details in Nightswatch App config bucket to check endpoint details are correct. Check source Kafka details from Nightswatch App config bucket . Make sure you are sending data into the source kafka. I am getting calls from Nightswatch after scheduled time Check for lags at the prescheduler End from Prescheduler Kafka Cosmos Dashboard Check for worker lags at scheduler end in Scheduler consolidated dashboard I didn't get call for a particular event Follow the Debugging Steps mentioned in Debugging Page","title":"Nightswatch FAQ"},{"location":"faq/nightswatchFaq/#what-is-my-topology-tag","text":"","title":"What is my Topology Tag"},{"location":"faq/nightswatchFaq/#i-am-not-getting-calls-from-nightswatch","text":"Places to look: In storm UI, Check if topology is running. If yes, check for Bolt failures. If any, open a box that runs that bolt and check logs to find out the reason of the failure. Check Client Endpoint details in Nightswatch App config bucket to check endpoint details are correct. Check source Kafka details from Nightswatch App config bucket . Make sure you are sending data into the source kafka.","title":"I am not getting calls from Nightswatch"},{"location":"faq/nightswatchFaq/#i-am-getting-calls-from-nightswatch-after-scheduled-time","text":"Check for lags at the prescheduler End from Prescheduler Kafka Cosmos Dashboard Check for worker lags at scheduler end in Scheduler consolidated dashboard","title":"I am getting calls from Nightswatch after scheduled time"},{"location":"faq/nightswatchFaq/#i-didnt-get-call-for-a-particular-event","text":"Follow the Debugging Steps mentioned in Debugging Page","title":"I didn't get call for a particular event"},{"location":"faq/stormFaq/","text":"What is a Nimbus? Nimbus is a master node of Storm cluster. All other nodes in the cluster are called as worker nodes. Master node is responsible for distributing data among all the worker nodes, assign tasks to worker nodes and monitoring failures. What is a Supervisor? The nodes that follow instructions given by the nimbus are called as Supervisors. A supervisor has multiple worker processes and it governs worker processes to complete the tasks assigned by the nimbus. What is a Worker process? A worker process will execute tasks related to a specific topology. A worker process will not run a task by itself, instead it creates executors and asks them to perform a particular task. A worker process will have multiple executors. What is an Executor? An executor is nothing but a single thread spawn by a worker process. An executor runs one or more tasks but only for a specific spout or bolt. What is a Spout? Source of data in Storm. Generally, Storm accepts input data from raw data sources like Streaming APIs, Apache Kafka queue etc. Spouts are threads that perform the activity of readi from the data source and handing over to the bolts What is a Bolt? Bolts are logical processing units. Spouts pass data to bolts and bolts process and produce a new output stream. Bolts can perform the operations of filtering, aggregation, joining, interacting with data sources and databases. Bolt receives data and emits to one or more bolts. How do I restart Storm? There are 3 components of storm: nimbus, ui, supervisor Restarting them is through init.d script. Command: Stop: sudo /etc/init.d/fk-3p-storm stop component Start: sudo /etc/init.d/fk-3p-storm start component Restart is not working for storm. What to do? Check the path for the component pid file (supervisor.pid, nimbus.pid, ui.pid). /var/run/fk-3p-storm/ Check if component process is not running ps -ef | grep component If no process is running, remove the pid file and restart the process To restart, follow steps at Restart Storm Why is Storm UI not opening? UI process runs on the nimbus box. Probable reason for UI not working is UI process died. Restarting the UI process may work in such cases. Steps for Debugging: Check UI logs in Nimbus box. Logs Location is /var/log/fk-3p-storm/ui.log Use below mentioned commands to restart the Storm UI. Stop UI: sudo /etc/init.d/fk-3p-storm stop ui Start UI: sudo /etc/init.d/fk-3p-storm start ui Why is my topology name not present in Storm UI? If you are not able to see your topology in Storm UI, it means that you topology is not running. It should have other implication too. You should be seeing Zero data in Hystrix and Cosmos Dashboards. You should be seeing an increase in Kafka Lag in Cosmos Dashboard, if you are pushing data in your source Kafka. You should not be seeing any traffic going to Scheduler Dashboard. All Dashboards mentioned above are in Dashboards Page Why am I seeing Exception Stack trace in Storm UI topology page? Once a topology is restarted, storm assigns a new topology id to the topology. So if the topology id is incorrect or obsolete, Storm UI throws 500 Server Error Try opening correct topology page from Storm UI home page . Why am I seeing Failure in Spouts? Spouts are responsible of reading messages from Kafka and sending them to bolts to act on. Once the bolt completes its action, it returns an ACK. This ack is forwarded to the kafka by the spout. If your bolt is taking too long to act on a message, or you spout took to long to give it to the bolt or get it from the bolt, it will result in timeout, thereby resulting in a spout failure. Try the following steps for resolution: Increase bolt.count and restart the topology. Decrease source.spout.max.pending.count for that spout. If the spout failure is very less in comparison to Acks, we dont need to worry. A failure means that message is retried. Storm guarantees we do not miss any message in the process Why am I seeing Failure in Bolts? Similar to spouts, if we see any bolt is taking longer time than usual, it may result in bolt failure. Also, if anu exception was thrown in any bolt, we can see bolt failures. Ideally, there should not be any failure but a small number is still acceptible taking into account if we are processing very huge number of events. In this case also storm will retry untill the bolt acks after sucessfully executing. So the topology will get stuck at the point where the exception occured untill, the issue is resolved. There is too much lag in my topology. What should be my configuration to quickly eat up the lag?","title":"Storm FAQ"},{"location":"faq/stormFaq/#what-is-a-nimbus","text":"Nimbus is a master node of Storm cluster. All other nodes in the cluster are called as worker nodes. Master node is responsible for distributing data among all the worker nodes, assign tasks to worker nodes and monitoring failures.","title":"What is a Nimbus?"},{"location":"faq/stormFaq/#what-is-a-supervisor","text":"The nodes that follow instructions given by the nimbus are called as Supervisors. A supervisor has multiple worker processes and it governs worker processes to complete the tasks assigned by the nimbus.","title":"What is a Supervisor?"},{"location":"faq/stormFaq/#what-is-a-worker-process","text":"A worker process will execute tasks related to a specific topology. A worker process will not run a task by itself, instead it creates executors and asks them to perform a particular task. A worker process will have multiple executors.","title":"What is a Worker process?"},{"location":"faq/stormFaq/#what-is-an-executor","text":"An executor is nothing but a single thread spawn by a worker process. An executor runs one or more tasks but only for a specific spout or bolt.","title":"What is an Executor?"},{"location":"faq/stormFaq/#what-is-a-spout","text":"Source of data in Storm. Generally, Storm accepts input data from raw data sources like Streaming APIs, Apache Kafka queue etc. Spouts are threads that perform the activity of readi from the data source and handing over to the bolts","title":"What is a Spout?"},{"location":"faq/stormFaq/#what-is-a-bolt","text":"Bolts are logical processing units. Spouts pass data to bolts and bolts process and produce a new output stream. Bolts can perform the operations of filtering, aggregation, joining, interacting with data sources and databases. Bolt receives data and emits to one or more bolts.","title":"What is a Bolt?"},{"location":"faq/stormFaq/#how-do-i-restart-storm","text":"There are 3 components of storm: nimbus, ui, supervisor Restarting them is through init.d script. Command: Stop: sudo /etc/init.d/fk-3p-storm stop component Start: sudo /etc/init.d/fk-3p-storm start component","title":"How do I restart Storm?"},{"location":"faq/stormFaq/#restart-is-not-working-for-storm-what-to-do","text":"Check the path for the component pid file (supervisor.pid, nimbus.pid, ui.pid). /var/run/fk-3p-storm/ Check if component process is not running ps -ef | grep component If no process is running, remove the pid file and restart the process To restart, follow steps at Restart Storm","title":"Restart is not working for storm. What to do?"},{"location":"faq/stormFaq/#why-is-storm-ui-not-opening","text":"UI process runs on the nimbus box. Probable reason for UI not working is UI process died. Restarting the UI process may work in such cases. Steps for Debugging: Check UI logs in Nimbus box. Logs Location is /var/log/fk-3p-storm/ui.log Use below mentioned commands to restart the Storm UI. Stop UI: sudo /etc/init.d/fk-3p-storm stop ui Start UI: sudo /etc/init.d/fk-3p-storm start ui","title":"Why is Storm UI not opening?"},{"location":"faq/stormFaq/#why-is-my-topology-name-not-present-in-storm-ui","text":"If you are not able to see your topology in Storm UI, it means that you topology is not running. It should have other implication too. You should be seeing Zero data in Hystrix and Cosmos Dashboards. You should be seeing an increase in Kafka Lag in Cosmos Dashboard, if you are pushing data in your source Kafka. You should not be seeing any traffic going to Scheduler Dashboard. All Dashboards mentioned above are in Dashboards Page","title":"Why is my topology name not present in Storm UI?"},{"location":"faq/stormFaq/#why-am-i-seeing-exception-stack-trace-in-storm-ui-topology-page","text":"Once a topology is restarted, storm assigns a new topology id to the topology. So if the topology id is incorrect or obsolete, Storm UI throws 500 Server Error Try opening correct topology page from Storm UI home page .","title":"Why am I seeing Exception Stack trace in Storm UI topology page?"},{"location":"faq/stormFaq/#why-am-i-seeing-failure-in-spouts","text":"Spouts are responsible of reading messages from Kafka and sending them to bolts to act on. Once the bolt completes its action, it returns an ACK. This ack is forwarded to the kafka by the spout. If your bolt is taking too long to act on a message, or you spout took to long to give it to the bolt or get it from the bolt, it will result in timeout, thereby resulting in a spout failure. Try the following steps for resolution: Increase bolt.count and restart the topology. Decrease source.spout.max.pending.count for that spout. If the spout failure is very less in comparison to Acks, we dont need to worry. A failure means that message is retried. Storm guarantees we do not miss any message in the process","title":"Why am I seeing Failure in Spouts?"},{"location":"faq/stormFaq/#why-am-i-seeing-failure-in-bolts","text":"Similar to spouts, if we see any bolt is taking longer time than usual, it may result in bolt failure. Also, if anu exception was thrown in any bolt, we can see bolt failures. Ideally, there should not be any failure but a small number is still acceptible taking into account if we are processing very huge number of events. In this case also storm will retry untill the bolt acks after sucessfully executing. So the topology will get stuck at the point where the exception occured untill, the issue is resolved.","title":"Why am I seeing Failure in Bolts?"},{"location":"faq/stormFaq/#there-is-too-much-lag-in-my-topology-what-should-be-my-configuration-to-quickly-eat-up-the-lag","text":"","title":"There is too much lag in my topology. What should be my configuration to quickly eat up the lag?"},{"location":"onboarding/alertAndMonitoring/","text":"Alerts We have provided a script to add alerts for every clients in their own team. Please refer Alerts Page for more details. List of Scripted Alerts: Kafka Lag Monitor Alerts on Lag ( REQUIRED ) Scheduler Hystrix Alerts on Latency and Error Yak Hystrix Alerts on Latency and Error External Service call Hystrix Alerts on Latency and Error ( REQUIRED ) Sideline Hystrix Alerts on Latency and Error Sideline Rate Alerts ( REQUIRED ) Monitoring We have documented all dashboards in Nightswatch Dashboards Page","title":"Alerts and Monitoring"},{"location":"onboarding/alertAndMonitoring/#alerts","text":"We have provided a script to add alerts for every clients in their own team. Please refer Alerts Page for more details. List of Scripted Alerts: Kafka Lag Monitor Alerts on Lag ( REQUIRED ) Scheduler Hystrix Alerts on Latency and Error Yak Hystrix Alerts on Latency and Error External Service call Hystrix Alerts on Latency and Error ( REQUIRED ) Sideline Hystrix Alerts on Latency and Error Sideline Rate Alerts ( REQUIRED )","title":"Alerts"},{"location":"onboarding/alertAndMonitoring/#monitoring","text":"We have documented all dashboards in Nightswatch Dashboards Page","title":"Monitoring"},{"location":"onboarding/clientJarSeparation/","text":"Interfaces provided by Nightswatch Each and every client is expected to provide Nightswatch with a maven dependency of a jar that would contain the 3 Things: Deserializer: To deserialize the Client Object that we receive from Client Kafka Converter: To convert the client POJO to Nightswatch JobTupleInfo (Object that Nightswatch understand) Strategy: checkForwardExpression, checkReverseExpression, calculateTTL Nightswatch Wall Dependency Client need to add nw-wall dependency provided by Nightswatch which contains the Interface that Nightswatch has provided for Cients to provide their implementations with. dependency groupId flipkart.cp.transact /groupId artifactId nw-wall /artifactId version ??? /version scope provided /scope /dependency Note that the scope of the nw-wall jar should be provided For details about the versions please refer to Release Notes Deserializer Implement the act() method and add the Deserialization Logic Receives request as byte[], sends response as Client POJO. Annotate you implemented class with ActionName.client_deserializer @ActionMeta ( name = ActionName . client_deserializer ) public class ClientDeserializer extends AbstractDeserializationAction ClientEvent { @Override public ActionResponse ClientEvent act ( DeserializerRequest request ) throws ActionException { ClientEvent clientEvent = JsonUtils . getObjectMapper (). readValue ( request . getObjectToActOn (), ClientEvent . class ); return new ActionResponse ( clientEvent ); } } Converter Implement the subAct() method and add the Converting Logic Receives request as Client POJO, sends response as Nightswatch POJO (aka JobTupleInfo). Please make sure to set the below mentioned values in JobTupleInfo: name: topologyName tag: topologyTag id: ID of the event perfId: This should be equal to the Id that resembles Perf Events for your events. domain: Could be FLIPKART or EMERALD sourceToFieldsMap: Data from your POJO that is of interest for calculation of forward, reverse expression or ttl, or required by clients to decide upon how to fix the anomaly. Use UpsertParameter() to add keys in sourceToFieldsMap jobTupleInfo.upsertParameter( sourceSpoutName , key , value ); Annotate you implemented class with ActionName.client_converter @ActionMeta ( name = ActionName . client_converter ) public class ClientConverterAction extends SuperConverterAction YggriteEvent { @Override public JobTupleInfo subAct ( ActionRequest ClientEvent request ) throws ActionException { ClientEvent clientEvent = request . getObjectToActOn (); JobTupleInfo jobTupleInfo = new JobTupleInfo ( topologyName ); jobTupleInfo . setName ( topologyName ); jobTupleInfo . setId ( id ); jobTupleInfo . setPerfId ( Optional . of ( id )); jobTupleInfo . upsertParameter ( sourceSpoutName , id , clientEvent . getId ()); jobTupleInfo . upsertParameter ( sourceSpoutName , val , clientEvent . getVal ()); jobTupleInfo . upsertParameter ( sourceSpoutName , expr , clientEvent . getExpr ()); jobTupleInfo . upsertParameter ( sourceSpoutName , test , clientEvent . getTest ()); return jobTupleInfo ; } } Strategy Clients are required to write Java code for checkForwardExpression, checkReverseExpression, calculateTTL. Implement the Strategy Interface. There are 3 methods to be Overridden. You will get the JobTupleInfo in the method as a parameter. Based on JobTupleInfo, You can decide to check for Forward Expression (if event is to be scheduled i.e. its an non terminal event) check for Reverse Expression (if event is to removed from scheduler i.e. its an terminal event) What should be the TTl for events when we schedule it to Scheduler. public class ClientStrategy implements Strategy { @Override public boolean checkForwardExpression ( JobTupleInfo jobTupleInfo ) throws NightswatchException { return jobTupleInfo . getParameter ( sourceSpoutName , expr ). equals ( FORWARD ); } @Override public boolean checkReverseExpression ( JobTupleInfo jobTupleInfo ) throws NightswatchException { return jobTupleInfo . getParameter ( sourceSpoutName , expr ). equals ( REVERSE ); } @Override public long calculateTTL ( JobTupleInfo jobTupleInfo ) throws NightswatchException { /* TTL is in millisecond */ return jobTupleInfo . getAttemptsMade () == 0 ? 600000L : 60000L ; } } Shaded client jar Clients are expected to present a shaded Jar whaich will have all the dependencies used by the client as shaded. Upload the shaded JAR to the Flipkart Artifactory. Use maven shaded plugin shade the jar. For more details on Maven shade plugin refer to http://maven.apache.org/plugins/maven-shade-plugin/examples/executable-jar.html Jar should not be SNAPSHOT Ensure that the version of the shaded jar is not SNAPSHOT. We accept only RELEASE versions. build plugins plugin groupId org.apache.maven.plugins /groupId artifactId maven-shade-plugin /artifactId version 3.2.1 /version executions execution phase package /phase goals goal shade /goal /goals configuration filters filter artifact *:* /artifact excludes exclude META-INF/*.SF /exclude exclude META-INF/*.DSA /exclude exclude META-INF/*.RSA /exclude /excludes /filter /filters relocations relocation pattern com.id.nw /pattern shadedPattern client.com.id.nw /shadedPattern /relocation !-- Shade your classes with a common pattern -- /relocations /configuration /execution /executions /plugin /plugins /build","title":"Client Jar Separation"},{"location":"onboarding/clientJarSeparation/#interfaces-provided-by-nightswatch","text":"Each and every client is expected to provide Nightswatch with a maven dependency of a jar that would contain the 3 Things: Deserializer: To deserialize the Client Object that we receive from Client Kafka Converter: To convert the client POJO to Nightswatch JobTupleInfo (Object that Nightswatch understand) Strategy: checkForwardExpression, checkReverseExpression, calculateTTL","title":"Interfaces provided by Nightswatch"},{"location":"onboarding/clientJarSeparation/#nightswatch-wall-dependency","text":"Client need to add nw-wall dependency provided by Nightswatch which contains the Interface that Nightswatch has provided for Cients to provide their implementations with. dependency groupId flipkart.cp.transact /groupId artifactId nw-wall /artifactId version ??? /version scope provided /scope /dependency Note that the scope of the nw-wall jar should be provided For details about the versions please refer to Release Notes","title":"Nightswatch Wall Dependency"},{"location":"onboarding/clientJarSeparation/#deserializer","text":"Implement the act() method and add the Deserialization Logic Receives request as byte[], sends response as Client POJO. Annotate you implemented class with ActionName.client_deserializer @ActionMeta ( name = ActionName . client_deserializer ) public class ClientDeserializer extends AbstractDeserializationAction ClientEvent { @Override public ActionResponse ClientEvent act ( DeserializerRequest request ) throws ActionException { ClientEvent clientEvent = JsonUtils . getObjectMapper (). readValue ( request . getObjectToActOn (), ClientEvent . class ); return new ActionResponse ( clientEvent ); } }","title":"Deserializer"},{"location":"onboarding/clientJarSeparation/#converter","text":"Implement the subAct() method and add the Converting Logic Receives request as Client POJO, sends response as Nightswatch POJO (aka JobTupleInfo). Please make sure to set the below mentioned values in JobTupleInfo: name: topologyName tag: topologyTag id: ID of the event perfId: This should be equal to the Id that resembles Perf Events for your events. domain: Could be FLIPKART or EMERALD sourceToFieldsMap: Data from your POJO that is of interest for calculation of forward, reverse expression or ttl, or required by clients to decide upon how to fix the anomaly. Use UpsertParameter() to add keys in sourceToFieldsMap jobTupleInfo.upsertParameter( sourceSpoutName , key , value ); Annotate you implemented class with ActionName.client_converter @ActionMeta ( name = ActionName . client_converter ) public class ClientConverterAction extends SuperConverterAction YggriteEvent { @Override public JobTupleInfo subAct ( ActionRequest ClientEvent request ) throws ActionException { ClientEvent clientEvent = request . getObjectToActOn (); JobTupleInfo jobTupleInfo = new JobTupleInfo ( topologyName ); jobTupleInfo . setName ( topologyName ); jobTupleInfo . setId ( id ); jobTupleInfo . setPerfId ( Optional . of ( id )); jobTupleInfo . upsertParameter ( sourceSpoutName , id , clientEvent . getId ()); jobTupleInfo . upsertParameter ( sourceSpoutName , val , clientEvent . getVal ()); jobTupleInfo . upsertParameter ( sourceSpoutName , expr , clientEvent . getExpr ()); jobTupleInfo . upsertParameter ( sourceSpoutName , test , clientEvent . getTest ()); return jobTupleInfo ; } }","title":"Converter"},{"location":"onboarding/clientJarSeparation/#strategy","text":"Clients are required to write Java code for checkForwardExpression, checkReverseExpression, calculateTTL. Implement the Strategy Interface. There are 3 methods to be Overridden. You will get the JobTupleInfo in the method as a parameter. Based on JobTupleInfo, You can decide to check for Forward Expression (if event is to be scheduled i.e. its an non terminal event) check for Reverse Expression (if event is to removed from scheduler i.e. its an terminal event) What should be the TTl for events when we schedule it to Scheduler. public class ClientStrategy implements Strategy { @Override public boolean checkForwardExpression ( JobTupleInfo jobTupleInfo ) throws NightswatchException { return jobTupleInfo . getParameter ( sourceSpoutName , expr ). equals ( FORWARD ); } @Override public boolean checkReverseExpression ( JobTupleInfo jobTupleInfo ) throws NightswatchException { return jobTupleInfo . getParameter ( sourceSpoutName , expr ). equals ( REVERSE ); } @Override public long calculateTTL ( JobTupleInfo jobTupleInfo ) throws NightswatchException { /* TTL is in millisecond */ return jobTupleInfo . getAttemptsMade () == 0 ? 600000L : 60000L ; } }","title":"Strategy"},{"location":"onboarding/clientJarSeparation/#shaded-client-jar","text":"Clients are expected to present a shaded Jar whaich will have all the dependencies used by the client as shaded. Upload the shaded JAR to the Flipkart Artifactory. Use maven shaded plugin shade the jar. For more details on Maven shade plugin refer to http://maven.apache.org/plugins/maven-shade-plugin/examples/executable-jar.html Jar should not be SNAPSHOT Ensure that the version of the shaded jar is not SNAPSHOT. We accept only RELEASE versions. build plugins plugin groupId org.apache.maven.plugins /groupId artifactId maven-shade-plugin /artifactId version 3.2.1 /version executions execution phase package /phase goals goal shade /goal /goals configuration filters filter artifact *:* /artifact excludes exclude META-INF/*.SF /exclude exclude META-INF/*.DSA /exclude exclude META-INF/*.RSA /exclude /excludes /filter /filters relocations relocation pattern com.id.nw /pattern shadedPattern client.com.id.nw /shadedPattern /relocation !-- Shade your classes with a common pattern -- /relocations /configuration /execution /executions /plugin /plugins /build","title":"Shaded client jar"},{"location":"onboarding/configsToBeProvided/","text":"Topology Config Bucket This bucket will contain information regarding the topology: Source Kafka Information: Info regarding Kafka topic, Zookeeper hosts, Kafka brokers, etc. Client Endpoint Details: Client API, Client Pool Name (mini proxy config will have pool configuration for every client endpoint pool) Client Jar details: Group Id, Artifact ID, Version of the client jar that has been pushed to Flipkart Artifactory group.id: The group id of the client jar artifact.id: The artifact Id of the client jar version: The version of the client jar shade.prefix: The prefix used for shading the client jar Http Configs: pool.name: name of the pool. host: Ip Address of the host / ELB Endpoint port: Port on which the service is listening connection.timeout: in milliseconds operation.timeout: in milliseconds max.connections: Number of connection Storm Topology Configs: Spout Counts, Bolt counts, Worker counts, Scheduler details etc. Please create a client under Scheduler based on the Qps that the topology is expecting. To create a Scheduler Client send a mail to id-dev@ Job Configs: tag: An unique name of the topology (usually same as the name) deserializer.action.clazz: path of the deserializerAction converter.action.clazz: path of the converterAction strategy.clazz: path of the Strategy Class max.attempt.number: Max number of retries that Client need from Nightswatch handle.pt: flag if true, handle PT events perf.factor: integer, by what factor PT events to be entered into the system. If perf.factor is 1, All events are allowed If perf.factor is 2, half of the events are allowed If perf.factor is n, 1/n of the events are allowed enable.sideline: Flag to enable or disable Sideline feature. Events will be discarded after all attempts are exhausted. enable.rest: Flag to enable or disable Nightswatch rest ingestion. Reference Config Bucket: http://10.47.3.62/#/list-keys/nw-preprod-config-bucket Once the config bucket is created with necessary configs, please send a mail to id-dev@flipkart.com along with the config bucket and topology-name.","title":"Configs to be provided"},{"location":"onboarding/configsToBeProvided/#topology-config-bucket","text":"This bucket will contain information regarding the topology: Source Kafka Information: Info regarding Kafka topic, Zookeeper hosts, Kafka brokers, etc. Client Endpoint Details: Client API, Client Pool Name (mini proxy config will have pool configuration for every client endpoint pool) Client Jar details: Group Id, Artifact ID, Version of the client jar that has been pushed to Flipkart Artifactory group.id: The group id of the client jar artifact.id: The artifact Id of the client jar version: The version of the client jar shade.prefix: The prefix used for shading the client jar Http Configs: pool.name: name of the pool. host: Ip Address of the host / ELB Endpoint port: Port on which the service is listening connection.timeout: in milliseconds operation.timeout: in milliseconds max.connections: Number of connection Storm Topology Configs: Spout Counts, Bolt counts, Worker counts, Scheduler details etc. Please create a client under Scheduler based on the Qps that the topology is expecting. To create a Scheduler Client send a mail to id-dev@ Job Configs: tag: An unique name of the topology (usually same as the name) deserializer.action.clazz: path of the deserializerAction converter.action.clazz: path of the converterAction strategy.clazz: path of the Strategy Class max.attempt.number: Max number of retries that Client need from Nightswatch handle.pt: flag if true, handle PT events perf.factor: integer, by what factor PT events to be entered into the system. If perf.factor is 1, All events are allowed If perf.factor is 2, half of the events are allowed If perf.factor is n, 1/n of the events are allowed enable.sideline: Flag to enable or disable Sideline feature. Events will be discarded after all attempts are exhausted. enable.rest: Flag to enable or disable Nightswatch rest ingestion. Reference Config Bucket: http://10.47.3.62/#/list-keys/nw-preprod-config-bucket Once the config bucket is created with necessary configs, please send a mail to id-dev@flipkart.com along with the config bucket and topology-name.","title":"Topology Config Bucket"},{"location":"onboarding/handler/","text":"Client API Details Clients are expected to expose an api (/nightswatch/recon) that Nightswatch can hit to inform the client about the anomaly. Whenever the scheduler pops out an event, it is handed over to the handler bolt, which then hits /nightswatch/recon api. Nightswatch provides information about the anomaly to the client in the form of NightswatchReconRequest class. String id ; int attemptNumber ; boolean lastAttempt ; String topologyName ; NwOriginSource source ; String reason ; Map String , Object extraFieldsMap ; where enum NwOriginSource { DEFAULT , SIDELINE } Nightswatch expects the response from the client in form of NwReconResponse class NwReconResponseStatus responseStatus ; where enum NwReconResponseStatus { RETRY , TERMINATE } Nightswatch relies on the response code send by the clients to understand the status of the API call. Response Code Response Status Action to be taken 2xx RETRY Retry after scheduled time 2xx TERMINATE Reconciliation Completed 5xx or 429 Storm level retry (execute the bolt once more) 4xx or Others Retry after scheduled time","title":"Client API Details"},{"location":"onboarding/handler/#client-api-details","text":"Clients are expected to expose an api (/nightswatch/recon) that Nightswatch can hit to inform the client about the anomaly. Whenever the scheduler pops out an event, it is handed over to the handler bolt, which then hits /nightswatch/recon api. Nightswatch provides information about the anomaly to the client in the form of NightswatchReconRequest class. String id ; int attemptNumber ; boolean lastAttempt ; String topologyName ; NwOriginSource source ; String reason ; Map String , Object extraFieldsMap ; where enum NwOriginSource { DEFAULT , SIDELINE } Nightswatch expects the response from the client in form of NwReconResponse class NwReconResponseStatus responseStatus ; where enum NwReconResponseStatus { RETRY , TERMINATE } Nightswatch relies on the response code send by the clients to understand the status of the API call. Response Code Response Status Action to be taken 2xx RETRY Retry after scheduled time 2xx TERMINATE Reconciliation Completed 5xx or 429 Storm level retry (execute the bolt once more) 4xx or Others Retry after scheduled time","title":"Client API Details"},{"location":"onboarding/nightswatchRestIngestion/","text":"Introduction Nightswatch has provided a Rest Interface to the clients. With this Rest Interface coming into picture, Clients will be able to add events to Nightswatch using a Rest Endpoint instead of adding to their kafka and providing us with their kafka endpoint. Architecture To summarize: Clients call Nightswatch Rest to add events to Nightswatch Nightswatch Rest add the event to Client Kafka Topic Nightswatch Storm reads event from that Kafka topic as usual Requirements from clients Clients need to provide Nightswatch regarding the following: BAU/Peak QPS of Data to be ingested to Nightswatch Average Size of Data to be put to Nightswatch Configs to be provided In the topology configuration, clients need to provide a flag to enable or disable rest ingestion enable.rest: Flag to enable or disable Nightswatch rest ingestion. Kafka Source Event Object Client need to add nw-client-shaded dependency provided by Nightswatch which contains the Nightswatch Source Request Class. dependency groupId flipkart.cp.transact.nightswatch /groupId artifactId nw-client-shaded /artifactId version 2.12 /version /dependency Nightswatch Rest API details For Rest Endpoint Details, Refer Nightswatch API API URL: /2.0/nw/{topo-name}/add API Body: { id : PT12345678903 , topologyName : nw-preprod , sourceSpoutName : source_preprod , perfId : PT12345678903 , domain : FLIPKART , sourceToFieldsMap : { id : PT12345678903 , val : 1001 , expression : REVERSE , testCase : SCHEDULER_SYNC }} Nightswatch Source Request private String id ; private String topologyName ; private String sourceSpoutName ; private String perfId ; private String domain ; private Map String , Object sourceToFieldsMap ; id: ID of the event topologyName: Name of the topology sourceSpoutName: Name of the source spout. One topology might consist of several flows. Each spout is a starting point for a flow. perfId: ID to be used to determine whether event is a perf event domain: Domain of Event. Flipkart or Emerald sourceToFieldsMap: Map of String to Object to filled or other necessary values that are required in by clients in strategy or at the client call. In case of any queries, please send a mail to id-dev@flipkart.com.","title":"Nightswatch Rest Ingestion"},{"location":"onboarding/nightswatchRestIngestion/#introduction","text":"Nightswatch has provided a Rest Interface to the clients. With this Rest Interface coming into picture, Clients will be able to add events to Nightswatch using a Rest Endpoint instead of adding to their kafka and providing us with their kafka endpoint.","title":"Introduction"},{"location":"onboarding/nightswatchRestIngestion/#architecture","text":"To summarize: Clients call Nightswatch Rest to add events to Nightswatch Nightswatch Rest add the event to Client Kafka Topic Nightswatch Storm reads event from that Kafka topic as usual","title":"Architecture"},{"location":"onboarding/nightswatchRestIngestion/#requirements-from-clients","text":"Clients need to provide Nightswatch regarding the following: BAU/Peak QPS of Data to be ingested to Nightswatch Average Size of Data to be put to Nightswatch","title":"Requirements from clients"},{"location":"onboarding/nightswatchRestIngestion/#configs-to-be-provided","text":"In the topology configuration, clients need to provide a flag to enable or disable rest ingestion enable.rest: Flag to enable or disable Nightswatch rest ingestion.","title":"Configs to be provided"},{"location":"onboarding/nightswatchRestIngestion/#kafka-source-event-object","text":"Client need to add nw-client-shaded dependency provided by Nightswatch which contains the Nightswatch Source Request Class. dependency groupId flipkart.cp.transact.nightswatch /groupId artifactId nw-client-shaded /artifactId version 2.12 /version /dependency","title":"Kafka Source Event Object"},{"location":"onboarding/nightswatchRestIngestion/#nightswatch-rest-api-details","text":"For Rest Endpoint Details, Refer Nightswatch API API URL: /2.0/nw/{topo-name}/add API Body: { id : PT12345678903 , topologyName : nw-preprod , sourceSpoutName : source_preprod , perfId : PT12345678903 , domain : FLIPKART , sourceToFieldsMap : { id : PT12345678903 , val : 1001 , expression : REVERSE , testCase : SCHEDULER_SYNC }}","title":"Nightswatch Rest API details"},{"location":"onboarding/nightswatchRestIngestion/#nightswatch-source-request","text":"private String id ; private String topologyName ; private String sourceSpoutName ; private String perfId ; private String domain ; private Map String , Object sourceToFieldsMap ; id: ID of the event topologyName: Name of the topology sourceSpoutName: Name of the source spout. One topology might consist of several flows. Each spout is a starting point for a flow. perfId: ID to be used to determine whether event is a perf event domain: Domain of Event. Flipkart or Emerald sourceToFieldsMap: Map of String to Object to filled or other necessary values that are required in by clients in strategy or at the client call. In case of any queries, please send a mail to id-dev@flipkart.com.","title":"Nightswatch Source Request"},{"location":"runbooks/boltsRunbook/","text":"Deser Converter Bolt Exception Description Deser Converter Bolt is responsiible for deserialization of the byte[] received from source Kafka and converting it to JobTupleInfo (Nightswatch Model) Detection Alerts : Nightswatch_Deser_Converter_Bolt_Exception_Alerts Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: No. of Exceptions per Second (One Minute Rate) Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos Possible Reasoning for such a Behaviour Deserialization Failure Null Pointer Exceptions This code is provided by clients Where do I find Topology Logs Where can I find the topology logs? How does the topology log look like? Impact Kafka Lag growth will be observed on Deser Converter Failures. Recovery The Deser Converter Code is provided by the Clients. So We have to inform Clients in case of such issue. Builder Bolt Description On receiving Data from DeserConverter Bolt, Builder performs the following: - Check if same data exists in DB - Check with Client provided Strategy to make scheduling decisions - Upsert data into Yak Detection Alerts : Nightswatch_Builder_Bolt_Exception_Alerts Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: No. of Exceptions per Second (One Minute Rate) Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos Possible Reasoning for such a Behaviour Yak failures (Should also get Yak failure alerts) Client Strategy Failures (This code is provided by clients) Where do I find Topology Logs Where can I find the topology logs? How does the topology log look like? Impact Kafka Lag growth. Recovery Follow Yak Runbooks in case of yak failures Inform clients in case of Strategy failure Scheduler Bolt Description On receiving data from Builder bolts, Scheduler bolts does the scheduler API call. Detection Alerts : Nightswatch_Scheduler_Bolt_Exception_Alerts Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: No. of Exceptions per Second (One Minute Rate) Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos Possible Reasoning for such a Behaviour Scheduler failures (Should also get Scheduler failure alerts) Where do I find Topology Logs Where can I find the topology logs? How does the topology log look like? Impact Kafka Lag growth. Recovery Follow Scheduler Runbooks in case of yak failures Inform clients in case of Strategy failure SchedulerKeyToTupleConverter Bolt Description SchedulerKeyToTupleConverter reads the Scheduler key from Scheduler Kafka, fetches data from yak and serializes it into JobTupleInfo. Detection Alerts : Nightswatch_SchedulerKeyToTupleConverter_Bolt_Exception_Alerts Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: No. of Exceptions per Second (One Minute Rate) Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos Possible Reasoning for such a Behaviour Yak failures (Should also get Yak failure alerts) Where do I find Topology Logs Where can I find the topology logs? How does the topology log look like? Impact Kafka Lag growth. Recovery Follow Yak Runbooks in case of yak failures Handler Bolt Description Data Received from SchedulerKeyToTupleConverter is then send to client via an API call (ExternalServiceCall) and based on the response of API call, we sideline or update in DB and send tuple to builder. Detection Alerts : Nightswatch_Handler_Bolt_Exception_Alerts Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: No. of Exceptions per Second (One Minute Rate) Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos Possible Reasoning for such a Behaviour ExternalServiceCall Failure (Should also get ExternalServiceCall failure alerts) Yak failures (Should also get Yak failure alerts) Where do I find Topology Logs Where can I find the topology logs? How does the topology log look like? Impact Kafka Lag growth. Recovery Follow Yak Runbooks in case of yak failures Follow ExternalServiceCall Runbooks in case of ExternalServiceCall failures","title":"Bolts Runbook"},{"location":"runbooks/boltsRunbook/#deser-converter-bolt-exception","text":"","title":"Deser Converter Bolt Exception"},{"location":"runbooks/boltsRunbook/#description","text":"Deser Converter Bolt is responsiible for deserialization of the byte[] received from source Kafka and converting it to JobTupleInfo (Nightswatch Model)","title":"Description"},{"location":"runbooks/boltsRunbook/#detection","text":"Alerts : Nightswatch_Deser_Converter_Bolt_Exception_Alerts Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: No. of Exceptions per Second (One Minute Rate) Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos","title":"Detection"},{"location":"runbooks/boltsRunbook/#possible-reasoning-for-such-a-behaviour","text":"Deserialization Failure Null Pointer Exceptions This code is provided by clients","title":"Possible Reasoning for such a Behaviour"},{"location":"runbooks/boltsRunbook/#where-do-i-find-topology-logs","text":"Where can I find the topology logs? How does the topology log look like?","title":"Where do I find Topology Logs"},{"location":"runbooks/boltsRunbook/#impact","text":"Kafka Lag growth will be observed on Deser Converter Failures.","title":"Impact"},{"location":"runbooks/boltsRunbook/#recovery","text":"The Deser Converter Code is provided by the Clients. So We have to inform Clients in case of such issue.","title":"Recovery"},{"location":"runbooks/boltsRunbook/#builder-bolt","text":"","title":"Builder Bolt"},{"location":"runbooks/boltsRunbook/#description_1","text":"On receiving Data from DeserConverter Bolt, Builder performs the following: - Check if same data exists in DB - Check with Client provided Strategy to make scheduling decisions - Upsert data into Yak","title":"Description"},{"location":"runbooks/boltsRunbook/#detection_1","text":"Alerts : Nightswatch_Builder_Bolt_Exception_Alerts Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: No. of Exceptions per Second (One Minute Rate) Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos","title":"Detection"},{"location":"runbooks/boltsRunbook/#possible-reasoning-for-such-a-behaviour_1","text":"Yak failures (Should also get Yak failure alerts) Client Strategy Failures (This code is provided by clients)","title":"Possible Reasoning for such a Behaviour"},{"location":"runbooks/boltsRunbook/#where-do-i-find-topology-logs_1","text":"Where can I find the topology logs? How does the topology log look like?","title":"Where do I find Topology Logs"},{"location":"runbooks/boltsRunbook/#impact_1","text":"Kafka Lag growth.","title":"Impact"},{"location":"runbooks/boltsRunbook/#recovery_1","text":"Follow Yak Runbooks in case of yak failures Inform clients in case of Strategy failure","title":"Recovery"},{"location":"runbooks/boltsRunbook/#scheduler-bolt","text":"","title":"Scheduler Bolt"},{"location":"runbooks/boltsRunbook/#description_2","text":"On receiving data from Builder bolts, Scheduler bolts does the scheduler API call.","title":"Description"},{"location":"runbooks/boltsRunbook/#detection_2","text":"Alerts : Nightswatch_Scheduler_Bolt_Exception_Alerts Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: No. of Exceptions per Second (One Minute Rate) Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos","title":"Detection"},{"location":"runbooks/boltsRunbook/#possible-reasoning-for-such-a-behaviour_2","text":"Scheduler failures (Should also get Scheduler failure alerts)","title":"Possible Reasoning for such a Behaviour"},{"location":"runbooks/boltsRunbook/#where-do-i-find-topology-logs_2","text":"Where can I find the topology logs? How does the topology log look like?","title":"Where do I find Topology Logs"},{"location":"runbooks/boltsRunbook/#impact_2","text":"Kafka Lag growth.","title":"Impact"},{"location":"runbooks/boltsRunbook/#recovery_2","text":"Follow Scheduler Runbooks in case of yak failures Inform clients in case of Strategy failure","title":"Recovery"},{"location":"runbooks/boltsRunbook/#schedulerkeytotupleconverter-bolt","text":"","title":"SchedulerKeyToTupleConverter Bolt"},{"location":"runbooks/boltsRunbook/#description_3","text":"SchedulerKeyToTupleConverter reads the Scheduler key from Scheduler Kafka, fetches data from yak and serializes it into JobTupleInfo.","title":"Description"},{"location":"runbooks/boltsRunbook/#detection_3","text":"Alerts : Nightswatch_SchedulerKeyToTupleConverter_Bolt_Exception_Alerts Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: No. of Exceptions per Second (One Minute Rate) Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos","title":"Detection"},{"location":"runbooks/boltsRunbook/#possible-reasoning-for-such-a-behaviour_3","text":"Yak failures (Should also get Yak failure alerts)","title":"Possible Reasoning for such a Behaviour"},{"location":"runbooks/boltsRunbook/#where-do-i-find-topology-logs_3","text":"Where can I find the topology logs? How does the topology log look like?","title":"Where do I find Topology Logs"},{"location":"runbooks/boltsRunbook/#impact_3","text":"Kafka Lag growth.","title":"Impact"},{"location":"runbooks/boltsRunbook/#recovery_3","text":"Follow Yak Runbooks in case of yak failures","title":"Recovery"},{"location":"runbooks/boltsRunbook/#handler-bolt","text":"","title":"Handler Bolt"},{"location":"runbooks/boltsRunbook/#description_4","text":"Data Received from SchedulerKeyToTupleConverter is then send to client via an API call (ExternalServiceCall) and based on the response of API call, we sideline or update in DB and send tuple to builder.","title":"Description"},{"location":"runbooks/boltsRunbook/#detection_4","text":"Alerts : Nightswatch_Handler_Bolt_Exception_Alerts Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: No. of Exceptions per Second (One Minute Rate) Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos","title":"Detection"},{"location":"runbooks/boltsRunbook/#possible-reasoning-for-such-a-behaviour_4","text":"ExternalServiceCall Failure (Should also get ExternalServiceCall failure alerts) Yak failures (Should also get Yak failure alerts)","title":"Possible Reasoning for such a Behaviour"},{"location":"runbooks/boltsRunbook/#where-do-i-find-topology-logs_4","text":"Where can I find the topology logs? How does the topology log look like?","title":"Where do I find Topology Logs"},{"location":"runbooks/boltsRunbook/#impact_4","text":"Kafka Lag growth.","title":"Impact"},{"location":"runbooks/boltsRunbook/#recovery_4","text":"Follow Yak Runbooks in case of yak failures Follow ExternalServiceCall Runbooks in case of ExternalServiceCall failures","title":"Recovery"},{"location":"runbooks/externalServiceCallRunbook/","text":"Description Nightswatch External Service Call is used to inform clients about the anomaly Detection Alerts : To be set up by Clients. Refer Alerts Page to set up alerts Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: External Service call Dashboard (90 Percentile Latency) Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos Possible Reasoning for such a Behaviour Latent Client Thread / CPU choking in Nightswatch Check the Thread Dashboard of External Service Call in the Above Cosmos Dashboard. Where do I find Topology Logs Where can I find the topology logs? How does the topology log look like? Impact Latent External Service Call can result in Timeouts. This is result in Bolt retry. Could cause congestion in Storm Lag. Error Percentage Increase will result in Bolt retry. This will result in Lag Pile up. Recovery The External Call Failures and Latencies lies with the Clients. So We have to inform Clients in case of such issue.","title":"External Service Call Runbooks"},{"location":"runbooks/externalServiceCallRunbook/#description","text":"Nightswatch External Service Call is used to inform clients about the anomaly","title":"Description"},{"location":"runbooks/externalServiceCallRunbook/#detection","text":"Alerts : To be set up by Clients. Refer Alerts Page to set up alerts Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: External Service call Dashboard (90 Percentile Latency) Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos","title":"Detection"},{"location":"runbooks/externalServiceCallRunbook/#possible-reasoning-for-such-a-behaviour","text":"Latent Client Thread / CPU choking in Nightswatch Check the Thread Dashboard of External Service Call in the Above Cosmos Dashboard.","title":"Possible Reasoning for such a Behaviour"},{"location":"runbooks/externalServiceCallRunbook/#where-do-i-find-topology-logs","text":"Where can I find the topology logs? How does the topology log look like?","title":"Where do I find Topology Logs"},{"location":"runbooks/externalServiceCallRunbook/#impact","text":"Latent External Service Call can result in Timeouts. This is result in Bolt retry. Could cause congestion in Storm Lag. Error Percentage Increase will result in Bolt retry. This will result in Lag Pile up.","title":"Impact"},{"location":"runbooks/externalServiceCallRunbook/#recovery","text":"The External Call Failures and Latencies lies with the Clients. So We have to inform Clients in case of such issue.","title":"Recovery"},{"location":"runbooks/hystrixMonitorRunbook/","text":"Client Kafka Lag Description Client Kafka Lag Monitor Alerts means that there are events in Client kafka which is still not consumed by Nightswatch Kafka Client Detection Alerts : Alerts are setup for each topology Nightswatch_(toplogy-name)_client_kafka_lag_monitor_alerts For more details about the alert check Formula used for Alerting on Kafka Lag Monitor Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: Source Kafka Offset Lag Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos Possible Reasoning for such a Behaviour Any Bolts Failure in Storm will be retried. Now if one of the bolt is failing in Storm, that tuple will be retried unless the Bolt succeed. This would mean we will events piling up in that partition which had the event for which bolt failures happened. There are 3 Bolt from client kafka. Bolts dashboards can be found in nightswatch dashboard. Refer Dashboards Page Deser Converter Bolt: Logic is provided by clients. Lots of Issues happens because of deserialization failure at client deserializer, where client did not bump up model versions at the client jar thus, resulting in Deser Failure Builder Bolt: Builder bolt connects to Yak. There could be Yak failures leading to such issues. Refer Dashboards Page for Nightswatch Yak dashboard. Builder also makes a Strategy call at client jar. There could be issues in that method call Scheduler Bolt: Scheduler Bolts connects to scheduler through REST. Failure at Scheduler can also lead to lag. Refer Dashboards Page for Nightswatch Scheduler dashboard. It could also be the case that huge amount of data has been ingested into Client Kafka in a very short span of time. In such scenario, Client Consumer lag might increase as Nightswatch will not be able to eat up all the events all together. Such scenarios happen during Flash Sales or NFR Testing. Debugging Step 1: Check Bolts Section in the above mentioned dashboard to get a view of exceptions. Dashboard Panel: No. of Exceptions per Second (One Minute Rate) Step 2: Check logs in worker boxes Where can I find the topology logs? How does the topology log look like? Check logs to find out the necessary reason of the issue Impact A Lag at Client Kafka would mean that Nightswatch is not consuming client event. Also, to note Nightswatch is an async system and it is not always bad to have lags. It is just that we should be aware of the reason behind the lag and should try to resolve it if it is a repercussion of some other component failure in Nightswatch. Clients would be impacted if Lags keep on increasing and so, it should be resolved at priority Recovery Use the Debugging steps mentioned above to isolate the issue. Inform Clients if it is a client issue (Deserializer, Converter or Strategy). Fix the component failure if Issue is because of any of the Component failure at Nightswatch Scheduler Yak Scheduler Kafka Lag Description Scheduler Kafka Lag Monitor Alerts means that there are events popping out from Scheduler kafka which is still not consumed by Nightswatch Detection Alerts : Alerts are setup for each topology Nightswatch_(toplogy-name)_scheduler_kafka_lag_monitor_alerts For more details about the alert check Formula used for Alerting on Kafka Lag Monitor Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: Scheduler Kafka Offset Lag Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos Possible Reasoning for such a Behaviour Any Bolts Failure in Storm will be retried. Now if one of the bolt is failing in Storm, that tuple will be retried unless the Bolt succeed. This would mean we will events piling up in that partition which had the event for which bolt failures happened. There are 3 Bolt from client kafka. Bolts dashboards can be found in nightswatch dashboard. Refer Dashboards Page Scheduler Key to tuple Converter Bolt: This bolt connects to Yak. There could be Yak failures leading to such issues. Refer Dashboards Page for Nightswatch Yak dashboard. Handler Bolt: Handler Bolts connects to client through REST. Failure at Client API can also lead to lag. Refer Dashboards Page for Nightswatch External Service Call dashboard. We also do hit the Sideline DB. Check Sideline dashboard ( Dashboards Page ) to verify no issue at yak sideline table. It could also be the case that huge amount of data has been popped out into Scheduler Kafka in a very short span of time. In such scenario, lag might increase as Nightswatch will not be able to eat up all the events all together. Such scenarios happens during Flash Sales or NFR Testing. Debugging Step 1: Check Bolts Section in the above mentioned dashboard to get a view of exceptions. Dashboard Panel: No. of Exceptions per Second (One Minute Rate) Step 2: Check logs in worker boxes Where can I find the topology logs? How does the topology log look like? Check logs to find out the necessary reason of the issue Impact A Lag at Scheduler Kafka would mean that Nightswatch is not calling client withing the scheduled time. This is a breach of contract and shouldl be addresed as quickly as possible. Clients would be impacted if Lags keep on increasing and so, it should be resolved at priority Recovery Use the Debugging steps mentioned above to isolate the issue. Inform Clients if it is a client issue (External Service call). Fix the component failure if Issue is because of any of the Component failure at Nightswatch Yak","title":"Hystrix Monitor Runbooks"},{"location":"runbooks/hystrixMonitorRunbook/#client-kafka-lag","text":"","title":"Client Kafka Lag"},{"location":"runbooks/hystrixMonitorRunbook/#description","text":"Client Kafka Lag Monitor Alerts means that there are events in Client kafka which is still not consumed by Nightswatch Kafka Client","title":"Description"},{"location":"runbooks/hystrixMonitorRunbook/#detection","text":"Alerts : Alerts are setup for each topology Nightswatch_(toplogy-name)_client_kafka_lag_monitor_alerts For more details about the alert check Formula used for Alerting on Kafka Lag Monitor Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: Source Kafka Offset Lag Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos","title":"Detection"},{"location":"runbooks/hystrixMonitorRunbook/#possible-reasoning-for-such-a-behaviour","text":"Any Bolts Failure in Storm will be retried. Now if one of the bolt is failing in Storm, that tuple will be retried unless the Bolt succeed. This would mean we will events piling up in that partition which had the event for which bolt failures happened. There are 3 Bolt from client kafka. Bolts dashboards can be found in nightswatch dashboard. Refer Dashboards Page Deser Converter Bolt: Logic is provided by clients. Lots of Issues happens because of deserialization failure at client deserializer, where client did not bump up model versions at the client jar thus, resulting in Deser Failure Builder Bolt: Builder bolt connects to Yak. There could be Yak failures leading to such issues. Refer Dashboards Page for Nightswatch Yak dashboard. Builder also makes a Strategy call at client jar. There could be issues in that method call Scheduler Bolt: Scheduler Bolts connects to scheduler through REST. Failure at Scheduler can also lead to lag. Refer Dashboards Page for Nightswatch Scheduler dashboard. It could also be the case that huge amount of data has been ingested into Client Kafka in a very short span of time. In such scenario, Client Consumer lag might increase as Nightswatch will not be able to eat up all the events all together. Such scenarios happen during Flash Sales or NFR Testing.","title":"Possible Reasoning for such a Behaviour"},{"location":"runbooks/hystrixMonitorRunbook/#debugging","text":"Step 1: Check Bolts Section in the above mentioned dashboard to get a view of exceptions. Dashboard Panel: No. of Exceptions per Second (One Minute Rate) Step 2: Check logs in worker boxes Where can I find the topology logs? How does the topology log look like? Check logs to find out the necessary reason of the issue","title":"Debugging"},{"location":"runbooks/hystrixMonitorRunbook/#impact","text":"A Lag at Client Kafka would mean that Nightswatch is not consuming client event. Also, to note Nightswatch is an async system and it is not always bad to have lags. It is just that we should be aware of the reason behind the lag and should try to resolve it if it is a repercussion of some other component failure in Nightswatch. Clients would be impacted if Lags keep on increasing and so, it should be resolved at priority","title":"Impact"},{"location":"runbooks/hystrixMonitorRunbook/#recovery","text":"Use the Debugging steps mentioned above to isolate the issue. Inform Clients if it is a client issue (Deserializer, Converter or Strategy). Fix the component failure if Issue is because of any of the Component failure at Nightswatch Scheduler Yak","title":"Recovery"},{"location":"runbooks/hystrixMonitorRunbook/#scheduler-kafka-lag","text":"","title":"Scheduler Kafka Lag"},{"location":"runbooks/hystrixMonitorRunbook/#description_1","text":"Scheduler Kafka Lag Monitor Alerts means that there are events popping out from Scheduler kafka which is still not consumed by Nightswatch","title":"Description"},{"location":"runbooks/hystrixMonitorRunbook/#detection_1","text":"Alerts : Alerts are setup for each topology Nightswatch_(toplogy-name)_scheduler_kafka_lag_monitor_alerts For more details about the alert check Formula used for Alerting on Kafka Lag Monitor Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: Scheduler Kafka Offset Lag Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos","title":"Detection"},{"location":"runbooks/hystrixMonitorRunbook/#possible-reasoning-for-such-a-behaviour_1","text":"Any Bolts Failure in Storm will be retried. Now if one of the bolt is failing in Storm, that tuple will be retried unless the Bolt succeed. This would mean we will events piling up in that partition which had the event for which bolt failures happened. There are 3 Bolt from client kafka. Bolts dashboards can be found in nightswatch dashboard. Refer Dashboards Page Scheduler Key to tuple Converter Bolt: This bolt connects to Yak. There could be Yak failures leading to such issues. Refer Dashboards Page for Nightswatch Yak dashboard. Handler Bolt: Handler Bolts connects to client through REST. Failure at Client API can also lead to lag. Refer Dashboards Page for Nightswatch External Service Call dashboard. We also do hit the Sideline DB. Check Sideline dashboard ( Dashboards Page ) to verify no issue at yak sideline table. It could also be the case that huge amount of data has been popped out into Scheduler Kafka in a very short span of time. In such scenario, lag might increase as Nightswatch will not be able to eat up all the events all together. Such scenarios happens during Flash Sales or NFR Testing.","title":"Possible Reasoning for such a Behaviour"},{"location":"runbooks/hystrixMonitorRunbook/#debugging_1","text":"Step 1: Check Bolts Section in the above mentioned dashboard to get a view of exceptions. Dashboard Panel: No. of Exceptions per Second (One Minute Rate) Step 2: Check logs in worker boxes Where can I find the topology logs? How does the topology log look like? Check logs to find out the necessary reason of the issue","title":"Debugging"},{"location":"runbooks/hystrixMonitorRunbook/#impact_1","text":"A Lag at Scheduler Kafka would mean that Nightswatch is not calling client withing the scheduled time. This is a breach of contract and shouldl be addresed as quickly as possible. Clients would be impacted if Lags keep on increasing and so, it should be resolved at priority","title":"Impact"},{"location":"runbooks/hystrixMonitorRunbook/#recovery_1","text":"Use the Debugging steps mentioned above to isolate the issue. Inform Clients if it is a client issue (External Service call). Fix the component failure if Issue is because of any of the Component failure at Nightswatch Yak","title":"Recovery"},{"location":"runbooks/kafkaLagMonitorRunbook/","text":"Client Kafka Lag Description Client Kafka Lag Monitor Alerts means that there are events in Client kafka which is still not consumed by Nightswatch Kafka Client Detection Alerts : Alerts are setup for each topology Nightswatch_(toplogy-name)_client_kafka_lag_monitor_alerts For more details about the alert check Formula used for Alerting on Kafka Lag Monitor Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: Source Kafka Offset Lag Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos Possible Reasoning for such a Behaviour Any Bolts Failure in Storm will be retried. Now if one of the bolt is failing in Storm, that tuple will be retried unless the Bolt succeed. This would mean we will events piling up in that partition which had the event for which bolt failures happened. There are 3 Bolt from client kafka. Bolts dashboards can be found in nightswatch dashboard. Refer Dashboards Page Deser Converter Bolt: Logic is provided by clients. Lots of Issues happens because of deserialization failure at client deserializer, where client did not bump up model versions at the client jar thus, resulting in Deser Failure Builder Bolt: Builder bolt connects to Yak. There could be Yak failures leading to such issues. Refer Dashboards Page for Nightswatch Yak dashboard. Builder also makes a Strategy call at client jar. There could be issues in that method call Scheduler Bolt: Scheduler Bolts connects to scheduler through REST. Failure at Scheduler can also lead to lag. Refer Dashboards Page for Nightswatch Scheduler dashboard. It could also be the case that huge amount of data has been ingested into Client Kafka in a very short span of time. In such scenario, Client Consumer lag might increase as Nightswatch will not be able to eat up all the events all together. Such scenarios happen during Flash Sales or NFR Testing. Debugging Step 1: Check Bolts Section in the above mentioned dashboard to get a view of exceptions. Dashboard Panel: No. of Exceptions per Second (One Minute Rate) Step 2: Check logs in worker boxes Where can I find the topology logs? How does the topology log look like? Check logs to find out the necessary reason of the issue Impact A Lag at Client Kafka would mean that Nightswatch is not consuming client event. Also, to note Nightswatch is an async system and it is not always bad to have lags. It is just that we should be aware of the reason behind the lag and should try to resolve it if it is a repercussion of some other component failure in Nightswatch. Clients would be impacted if Lags keep on increasing and so, it should be resolved at priority Recovery Use the Debugging steps mentioned above to isolate the issue. Inform Clients if it is a client issue (Deserializer, Converter or Strategy). Fix the component failure if Issue is because of any of the Component failure at Nightswatch Scheduler Yak Scheduler Kafka Lag Description Scheduler Kafka Lag Monitor Alerts means that there are events popping out from Scheduler kafka which is still not consumed by Nightswatch Detection Alerts : Alerts are setup for each topology Nightswatch_(toplogy-name)_scheduler_kafka_lag_monitor_alerts For more details about the alert check Formula used for Alerting on Kafka Lag Monitor Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: Scheduler Kafka Offset Lag Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos Possible Reasoning for such a Behaviour Any Bolts Failure in Storm will be retried. Now if one of the bolt is failing in Storm, that tuple will be retried unless the Bolt succeed. This would mean we will events piling up in that partition which had the event for which bolt failures happened. There are 3 Bolt from client kafka. Bolts dashboards can be found in nightswatch dashboard. Refer Dashboards Page Scheduler Key to tuple Converter Bolt: This bolt connects to Yak. There could be Yak failures leading to such issues. Refer Dashboards Page for Nightswatch Yak dashboard. Handler Bolt: Handler Bolts connects to client through REST. Failure at Client API can also lead to lag. Refer Dashboards Page for Nightswatch External Service Call dashboard. We also do hit the Sideline DB. Check Sideline dashboard ( Dashboards Page ) to verify no issue at yak sideline table. It could also be the case that huge amount of data has been popped out into Scheduler Kafka in a very short span of time. In such scenario, lag might increase as Nightswatch will not be able to eat up all the events all together. Such scenarios happens during Flash Sales or NFR Testing. Debugging Step 1: Check Bolts Section in the above mentioned dashboard to get a view of exceptions. Dashboard Panel: No. of Exceptions per Second (One Minute Rate) Step 2: Check logs in worker boxes Where can I find the topology logs? How does the topology log look like? Check logs to find out the necessary reason of the issue Impact A Lag at Scheduler Kafka would mean that Nightswatch is not calling client withing the scheduled time. This is a breach of contract and shouldl be addresed as quickly as possible. Clients would be impacted if Lags keep on increasing and so, it should be resolved at priority Recovery Use the Debugging steps mentioned above to isolate the issue. Inform Clients if it is a client issue (External Service call). Fix the component failure if Issue is because of any of the Component failure at Nightswatch Yak","title":"Kafka Lag Monitor Runbooks"},{"location":"runbooks/kafkaLagMonitorRunbook/#client-kafka-lag","text":"","title":"Client Kafka Lag"},{"location":"runbooks/kafkaLagMonitorRunbook/#description","text":"Client Kafka Lag Monitor Alerts means that there are events in Client kafka which is still not consumed by Nightswatch Kafka Client","title":"Description"},{"location":"runbooks/kafkaLagMonitorRunbook/#detection","text":"Alerts : Alerts are setup for each topology Nightswatch_(toplogy-name)_client_kafka_lag_monitor_alerts For more details about the alert check Formula used for Alerting on Kafka Lag Monitor Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: Source Kafka Offset Lag Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos","title":"Detection"},{"location":"runbooks/kafkaLagMonitorRunbook/#possible-reasoning-for-such-a-behaviour","text":"Any Bolts Failure in Storm will be retried. Now if one of the bolt is failing in Storm, that tuple will be retried unless the Bolt succeed. This would mean we will events piling up in that partition which had the event for which bolt failures happened. There are 3 Bolt from client kafka. Bolts dashboards can be found in nightswatch dashboard. Refer Dashboards Page Deser Converter Bolt: Logic is provided by clients. Lots of Issues happens because of deserialization failure at client deserializer, where client did not bump up model versions at the client jar thus, resulting in Deser Failure Builder Bolt: Builder bolt connects to Yak. There could be Yak failures leading to such issues. Refer Dashboards Page for Nightswatch Yak dashboard. Builder also makes a Strategy call at client jar. There could be issues in that method call Scheduler Bolt: Scheduler Bolts connects to scheduler through REST. Failure at Scheduler can also lead to lag. Refer Dashboards Page for Nightswatch Scheduler dashboard. It could also be the case that huge amount of data has been ingested into Client Kafka in a very short span of time. In such scenario, Client Consumer lag might increase as Nightswatch will not be able to eat up all the events all together. Such scenarios happen during Flash Sales or NFR Testing.","title":"Possible Reasoning for such a Behaviour"},{"location":"runbooks/kafkaLagMonitorRunbook/#debugging","text":"Step 1: Check Bolts Section in the above mentioned dashboard to get a view of exceptions. Dashboard Panel: No. of Exceptions per Second (One Minute Rate) Step 2: Check logs in worker boxes Where can I find the topology logs? How does the topology log look like? Check logs to find out the necessary reason of the issue","title":"Debugging"},{"location":"runbooks/kafkaLagMonitorRunbook/#impact","text":"A Lag at Client Kafka would mean that Nightswatch is not consuming client event. Also, to note Nightswatch is an async system and it is not always bad to have lags. It is just that we should be aware of the reason behind the lag and should try to resolve it if it is a repercussion of some other component failure in Nightswatch. Clients would be impacted if Lags keep on increasing and so, it should be resolved at priority","title":"Impact"},{"location":"runbooks/kafkaLagMonitorRunbook/#recovery","text":"Use the Debugging steps mentioned above to isolate the issue. Inform Clients if it is a client issue (Deserializer, Converter or Strategy). Fix the component failure if Issue is because of any of the Component failure at Nightswatch Scheduler Yak","title":"Recovery"},{"location":"runbooks/kafkaLagMonitorRunbook/#scheduler-kafka-lag","text":"","title":"Scheduler Kafka Lag"},{"location":"runbooks/kafkaLagMonitorRunbook/#description_1","text":"Scheduler Kafka Lag Monitor Alerts means that there are events popping out from Scheduler kafka which is still not consumed by Nightswatch","title":"Description"},{"location":"runbooks/kafkaLagMonitorRunbook/#detection_1","text":"Alerts : Alerts are setup for each topology Nightswatch_(toplogy-name)_scheduler_kafka_lag_monitor_alerts For more details about the alert check Formula used for Alerting on Kafka Lag Monitor Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: Scheduler Kafka Offset Lag Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos","title":"Detection"},{"location":"runbooks/kafkaLagMonitorRunbook/#possible-reasoning-for-such-a-behaviour_1","text":"Any Bolts Failure in Storm will be retried. Now if one of the bolt is failing in Storm, that tuple will be retried unless the Bolt succeed. This would mean we will events piling up in that partition which had the event for which bolt failures happened. There are 3 Bolt from client kafka. Bolts dashboards can be found in nightswatch dashboard. Refer Dashboards Page Scheduler Key to tuple Converter Bolt: This bolt connects to Yak. There could be Yak failures leading to such issues. Refer Dashboards Page for Nightswatch Yak dashboard. Handler Bolt: Handler Bolts connects to client through REST. Failure at Client API can also lead to lag. Refer Dashboards Page for Nightswatch External Service Call dashboard. We also do hit the Sideline DB. Check Sideline dashboard ( Dashboards Page ) to verify no issue at yak sideline table. It could also be the case that huge amount of data has been popped out into Scheduler Kafka in a very short span of time. In such scenario, lag might increase as Nightswatch will not be able to eat up all the events all together. Such scenarios happens during Flash Sales or NFR Testing.","title":"Possible Reasoning for such a Behaviour"},{"location":"runbooks/kafkaLagMonitorRunbook/#debugging_1","text":"Step 1: Check Bolts Section in the above mentioned dashboard to get a view of exceptions. Dashboard Panel: No. of Exceptions per Second (One Minute Rate) Step 2: Check logs in worker boxes Where can I find the topology logs? How does the topology log look like? Check logs to find out the necessary reason of the issue","title":"Debugging"},{"location":"runbooks/kafkaLagMonitorRunbook/#impact_1","text":"A Lag at Scheduler Kafka would mean that Nightswatch is not calling client withing the scheduled time. This is a breach of contract and shouldl be addresed as quickly as possible. Clients would be impacted if Lags keep on increasing and so, it should be resolved at priority","title":"Impact"},{"location":"runbooks/kafkaLagMonitorRunbook/#recovery_1","text":"Use the Debugging steps mentioned above to isolate the issue. Inform Clients if it is a client issue (External Service call). Fix the component failure if Issue is because of any of the Component failure at Nightswatch Yak","title":"Recovery"},{"location":"runbooks/kafkaLagRunbook/","text":"Client Kafka Lag Description Client Kafka Lag Monitor Alerts means that there are events in Client kafka which is still not consumed by Nightswatch Kafka Client Detection Alerts : Alerts are setup for each topology Nightswatch_(toplogy-name)_client_kafka_lag_monitor_alerts For more details about the alert check Formula used for Alerting on Kafka Lag Monitor Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: Source Kafka Offset Lag Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos Possible Reasoning for such a Behaviour Any Bolts Failure in Storm will be retried. Now if one of the bolt is failing in Storm, that tuple will be retried unless the Bolt succeed. This would mean we will events piling up in that partition which had the event for which bolt failures happened. There are 3 Bolt from client kafka. Bolts dashboards can be found in nightswatch dashboard. Refer Dashboards Page Deser Converter Bolt: Logic is provided by clients. Lots of Issues happens because of deserialization failure at client deserializer, where client did not bump up model versions at the client jar thus, resulting in Deser Failure Builder Bolt: Builder bolt connects to Yak. There could be Yak failures leading to such issues. Refer Dashboards Page for Nightswatch Yak dashboard. Builder also makes a Strategy call at client jar. There could be issues in that method call Scheduler Bolt: Scheduler Bolts connects to scheduler through REST. Failure at Scheduler can also lead to lag. Refer Dashboards Page for Nightswatch Scheduler dashboard. It could also be the case that huge amount of data has been ingested into Client Kafka in a very short span of time. In such scenario, Client Consumer lag might increase as Nightswatch will not be able to eat up all the events all together. Such scenarios happen during Flash Sales or NFR Testing. Debugging Step 1: Check Bolts Section in the above mentioned dashboard to get a view of exceptions. Dashboard Panel: No. of Exceptions per Second (One Minute Rate) Step 2: Check logs in worker boxes Where can I find the topology logs? How does the topology log look like? Check logs to find out the necessary reason of the issue Impact A Lag at Client Kafka would mean that Nightswatch is not consuming client event. Also, to note Nightswatch is an async system and it is not always bad to have lags. It is just that we should be aware of the reason behind the lag and should try to resolve it if it is a repercussion of some other component failure in Nightswatch. Clients would be impacted if Lags keep on increasing and so, it should be resolved at priority Recovery Use the Debugging steps mentioned above to isolate the issue. Inform Clients if it is a client issue (Deserializer, Converter or Strategy). Fix the component failure if Issue is because of any of the Component failure at Nightswatch Scheduler Yak Scheduler Kafka Lag Description Scheduler Kafka Lag Monitor Alerts means that there are events popping out from Scheduler kafka which is still not consumed by Nightswatch Detection Alerts : Alerts are setup for each topology Nightswatch_(toplogy-name)_scheduler_kafka_lag_monitor_alerts For more details about the alert check Formula used for Alerting on Kafka Lag Monitor Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: Scheduler Kafka Offset Lag Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos Possible Reasoning for such a Behaviour Any Bolts Failure in Storm will be retried. Now if one of the bolt is failing in Storm, that tuple will be retried unless the Bolt succeed. This would mean we will events piling up in that partition which had the event for which bolt failures happened. There are 3 Bolt from client kafka. Bolts dashboards can be found in nightswatch dashboard. Refer Dashboards Page Scheduler Key to tuple Converter Bolt: This bolt connects to Yak. There could be Yak failures leading to such issues. Refer Dashboards Page for Nightswatch Yak dashboard. Handler Bolt: Handler Bolts connects to client through REST. Failure at Client API can also lead to lag. Refer Dashboards Page for Nightswatch External Service Call dashboard. We also do hit the Sideline DB. Check Sideline dashboard ( Dashboards Page ) to verify no issue at yak sideline table. It could also be the case that huge amount of data has been popped out into Scheduler Kafka in a very short span of time. In such scenario, lag might increase as Nightswatch will not be able to eat up all the events all together. Such scenarios happens during Flash Sales or NFR Testing. Debugging Step 1: Check Bolts Section in the above mentioned dashboard to get a view of exceptions. Dashboard Panel: No. of Exceptions per Second (One Minute Rate) Step 2: Check logs in worker boxes Where can I find the topology logs? How does the topology log look like? Check logs to find out the necessary reason of the issue Impact A Lag at Scheduler Kafka would mean that Nightswatch is not calling client withing the scheduled time. This is a breach of contract and shouldl be addresed as quickly as possible. Clients would be impacted if Lags keep on increasing and so, it should be resolved at priority Recovery Use the Debugging steps mentioned above to isolate the issue. Inform Clients if it is a client issue (External Service call). Fix the component failure if Issue is because of any of the Component failure at Nightswatch Yak","title":"Kafka Lag Runbooks"},{"location":"runbooks/kafkaLagRunbook/#client-kafka-lag","text":"","title":"Client Kafka Lag"},{"location":"runbooks/kafkaLagRunbook/#description","text":"Client Kafka Lag Monitor Alerts means that there are events in Client kafka which is still not consumed by Nightswatch Kafka Client","title":"Description"},{"location":"runbooks/kafkaLagRunbook/#detection","text":"Alerts : Alerts are setup for each topology Nightswatch_(toplogy-name)_client_kafka_lag_monitor_alerts For more details about the alert check Formula used for Alerting on Kafka Lag Monitor Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: Source Kafka Offset Lag Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos","title":"Detection"},{"location":"runbooks/kafkaLagRunbook/#possible-reasoning-for-such-a-behaviour","text":"Any Bolts Failure in Storm will be retried. Now if one of the bolt is failing in Storm, that tuple will be retried unless the Bolt succeed. This would mean we will events piling up in that partition which had the event for which bolt failures happened. There are 3 Bolt from client kafka. Bolts dashboards can be found in nightswatch dashboard. Refer Dashboards Page Deser Converter Bolt: Logic is provided by clients. Lots of Issues happens because of deserialization failure at client deserializer, where client did not bump up model versions at the client jar thus, resulting in Deser Failure Builder Bolt: Builder bolt connects to Yak. There could be Yak failures leading to such issues. Refer Dashboards Page for Nightswatch Yak dashboard. Builder also makes a Strategy call at client jar. There could be issues in that method call Scheduler Bolt: Scheduler Bolts connects to scheduler through REST. Failure at Scheduler can also lead to lag. Refer Dashboards Page for Nightswatch Scheduler dashboard. It could also be the case that huge amount of data has been ingested into Client Kafka in a very short span of time. In such scenario, Client Consumer lag might increase as Nightswatch will not be able to eat up all the events all together. Such scenarios happen during Flash Sales or NFR Testing.","title":"Possible Reasoning for such a Behaviour"},{"location":"runbooks/kafkaLagRunbook/#debugging","text":"Step 1: Check Bolts Section in the above mentioned dashboard to get a view of exceptions. Dashboard Panel: No. of Exceptions per Second (One Minute Rate) Step 2: Check logs in worker boxes Where can I find the topology logs? How does the topology log look like? Check logs to find out the necessary reason of the issue","title":"Debugging"},{"location":"runbooks/kafkaLagRunbook/#impact","text":"A Lag at Client Kafka would mean that Nightswatch is not consuming client event. Also, to note Nightswatch is an async system and it is not always bad to have lags. It is just that we should be aware of the reason behind the lag and should try to resolve it if it is a repercussion of some other component failure in Nightswatch. Clients would be impacted if Lags keep on increasing and so, it should be resolved at priority","title":"Impact"},{"location":"runbooks/kafkaLagRunbook/#recovery","text":"Use the Debugging steps mentioned above to isolate the issue. Inform Clients if it is a client issue (Deserializer, Converter or Strategy). Fix the component failure if Issue is because of any of the Component failure at Nightswatch Scheduler Yak","title":"Recovery"},{"location":"runbooks/kafkaLagRunbook/#scheduler-kafka-lag","text":"","title":"Scheduler Kafka Lag"},{"location":"runbooks/kafkaLagRunbook/#description_1","text":"Scheduler Kafka Lag Monitor Alerts means that there are events popping out from Scheduler kafka which is still not consumed by Nightswatch","title":"Description"},{"location":"runbooks/kafkaLagRunbook/#detection_1","text":"Alerts : Alerts are setup for each topology Nightswatch_(toplogy-name)_scheduler_kafka_lag_monitor_alerts For more details about the alert check Formula used for Alerting on Kafka Lag Monitor Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: Scheduler Kafka Offset Lag Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos","title":"Detection"},{"location":"runbooks/kafkaLagRunbook/#possible-reasoning-for-such-a-behaviour_1","text":"Any Bolts Failure in Storm will be retried. Now if one of the bolt is failing in Storm, that tuple will be retried unless the Bolt succeed. This would mean we will events piling up in that partition which had the event for which bolt failures happened. There are 3 Bolt from client kafka. Bolts dashboards can be found in nightswatch dashboard. Refer Dashboards Page Scheduler Key to tuple Converter Bolt: This bolt connects to Yak. There could be Yak failures leading to such issues. Refer Dashboards Page for Nightswatch Yak dashboard. Handler Bolt: Handler Bolts connects to client through REST. Failure at Client API can also lead to lag. Refer Dashboards Page for Nightswatch External Service Call dashboard. We also do hit the Sideline DB. Check Sideline dashboard ( Dashboards Page ) to verify no issue at yak sideline table. It could also be the case that huge amount of data has been popped out into Scheduler Kafka in a very short span of time. In such scenario, lag might increase as Nightswatch will not be able to eat up all the events all together. Such scenarios happens during Flash Sales or NFR Testing.","title":"Possible Reasoning for such a Behaviour"},{"location":"runbooks/kafkaLagRunbook/#debugging_1","text":"Step 1: Check Bolts Section in the above mentioned dashboard to get a view of exceptions. Dashboard Panel: No. of Exceptions per Second (One Minute Rate) Step 2: Check logs in worker boxes Where can I find the topology logs? How does the topology log look like? Check logs to find out the necessary reason of the issue","title":"Debugging"},{"location":"runbooks/kafkaLagRunbook/#impact_1","text":"A Lag at Scheduler Kafka would mean that Nightswatch is not calling client withing the scheduled time. This is a breach of contract and shouldl be addresed as quickly as possible. Clients would be impacted if Lags keep on increasing and so, it should be resolved at priority","title":"Impact"},{"location":"runbooks/kafkaLagRunbook/#recovery_1","text":"Use the Debugging steps mentioned above to isolate the issue. Inform Clients if it is a client issue (External Service call). Fix the component failure if Issue is because of any of the Component failure at Nightswatch Yak","title":"Recovery"},{"location":"runbooks/nightswatchDashboardRunbook/","text":"Nightswatch Dashboard Instance Down Description Nightswatch Dashboard is a single Collated Website containing all the dashboards like Hystrix, Cosmos, Storm, Scheduler. Along with that we have Yak, Trace Consoles. Detection Alerts : Ping Alerts Dashboard : Chennai : http://10.47.101.5:8000/ch.html?vip=10.47.102.190 Hyderabad : http://10.47.101.5:8000/hyd.html?vip=10.24.7.187 Possible Reasoning for such a Behaviour Scheduled / Unscheduled Maintenance (Box is down) Impact No Dashboard will mean, we have to rely on bare cosmos dashboards or Hystrix Dashboard. the collated view will be gone. Recovery RTO: 5mins Please follow the steps mention in Maintenance Steps for Nightswatch Dashboard Validation Verify Dashboard is started Chennai : http://10.47.101.5:8000/ch.html?vip=10.47.102.190 Hyderabad : http://10.47.101.5:8000/hyd.html?vip=10.24.7.187","title":"Nightswatch Dashboard Runbooks"},{"location":"runbooks/nightswatchDashboardRunbook/#nightswatch-dashboard-instance-down","text":"","title":"Nightswatch Dashboard Instance Down"},{"location":"runbooks/nightswatchDashboardRunbook/#description","text":"Nightswatch Dashboard is a single Collated Website containing all the dashboards like Hystrix, Cosmos, Storm, Scheduler. Along with that we have Yak, Trace Consoles.","title":"Description"},{"location":"runbooks/nightswatchDashboardRunbook/#detection","text":"Alerts : Ping Alerts Dashboard : Chennai : http://10.47.101.5:8000/ch.html?vip=10.47.102.190 Hyderabad : http://10.47.101.5:8000/hyd.html?vip=10.24.7.187","title":"Detection"},{"location":"runbooks/nightswatchDashboardRunbook/#possible-reasoning-for-such-a-behaviour","text":"Scheduled / Unscheduled Maintenance (Box is down)","title":"Possible Reasoning for such a Behaviour"},{"location":"runbooks/nightswatchDashboardRunbook/#impact","text":"No Dashboard will mean, we have to rely on bare cosmos dashboards or Hystrix Dashboard. the collated view will be gone.","title":"Impact"},{"location":"runbooks/nightswatchDashboardRunbook/#recovery","text":"RTO: 5mins Please follow the steps mention in Maintenance Steps for Nightswatch Dashboard","title":"Recovery"},{"location":"runbooks/nightswatchDashboardRunbook/#validation","text":"Verify Dashboard is started Chennai : http://10.47.101.5:8000/ch.html?vip=10.47.102.190 Hyderabad : http://10.47.101.5:8000/hyd.html?vip=10.24.7.187","title":"Validation"},{"location":"runbooks/nightswatchRestRunbook/","text":"Nightswatch Rest Service Instance Down Description Nightswatch Rest provides us some important API's that is used for Sidelining, Debugging, powerring Dashboard. Detection Alerts : Ping Alerts Dashboard : Chennai : http://10.47.101.5:8000/ch.html?vip=10.47.102.118 Hyderabad : http://10.47.101.5:8000/hyd.html?vip=10.24.7.64 Possible Reasoning for such a Behaviour Scheduled / Unscheduled Maintenance (Box is down) High Resource Utilization (Process is down) Where do I find Rest Service Logs Rest Service Instance Group: AppId: prod-nightswatch Chennai: rest-service-v2 Hyderabad: rest-service-v1 Log Location: Access Logs: /var/log/flipkart/transact/nightswatch/access.log Service Logs: /var/log/flipkart/transact/nightswatch/nightswatch.log Error Logs: /var/log/flipkart/transact/nightswatch/error.log Impact Elb might become unhealthy if 1 box / process goes down If entire service goes down, it will impact: No Unsidelining No Trace (Debugging API) No Dashboards Recovery RTO: 5mins Please follow the steps mention in Maintenance Steps for Nightswatch Rest Validation Verify Nimbus is started Chennai : http://10.47.101.5:8000/ch.html?vip=10.47.103.132 Hyderabad : http://10.47.101.5:8000/hyd.html?vip=10.24.7.28","title":"Nightswatch Rest Runbooks"},{"location":"runbooks/nightswatchRestRunbook/#nightswatch-rest-service-instance-down","text":"","title":"Nightswatch Rest Service Instance Down"},{"location":"runbooks/nightswatchRestRunbook/#description","text":"Nightswatch Rest provides us some important API's that is used for Sidelining, Debugging, powerring Dashboard.","title":"Description"},{"location":"runbooks/nightswatchRestRunbook/#detection","text":"Alerts : Ping Alerts Dashboard : Chennai : http://10.47.101.5:8000/ch.html?vip=10.47.102.118 Hyderabad : http://10.47.101.5:8000/hyd.html?vip=10.24.7.64","title":"Detection"},{"location":"runbooks/nightswatchRestRunbook/#possible-reasoning-for-such-a-behaviour","text":"Scheduled / Unscheduled Maintenance (Box is down) High Resource Utilization (Process is down)","title":"Possible Reasoning for such a Behaviour"},{"location":"runbooks/nightswatchRestRunbook/#where-do-i-find-rest-service-logs","text":"Rest Service Instance Group: AppId: prod-nightswatch Chennai: rest-service-v2 Hyderabad: rest-service-v1 Log Location: Access Logs: /var/log/flipkart/transact/nightswatch/access.log Service Logs: /var/log/flipkart/transact/nightswatch/nightswatch.log Error Logs: /var/log/flipkart/transact/nightswatch/error.log","title":"Where do I find Rest Service Logs"},{"location":"runbooks/nightswatchRestRunbook/#impact","text":"Elb might become unhealthy if 1 box / process goes down If entire service goes down, it will impact: No Unsidelining No Trace (Debugging API) No Dashboards","title":"Impact"},{"location":"runbooks/nightswatchRestRunbook/#recovery","text":"RTO: 5mins Please follow the steps mention in Maintenance Steps for Nightswatch Rest","title":"Recovery"},{"location":"runbooks/nightswatchRestRunbook/#validation","text":"Verify Nimbus is started Chennai : http://10.47.101.5:8000/ch.html?vip=10.47.103.132 Hyderabad : http://10.47.101.5:8000/hyd.html?vip=10.24.7.28","title":"Validation"},{"location":"runbooks/schedulerRunbook/","text":"Scheduler Hystrix Latency 90 Percentile Description Nightswatch connects to Scheduler through REST API Calls to add, update or delete events in Scheduler Service. High latent scheduler calls can lead to degrading the performance of the system. So, we should be closely monitoring the Scheduler Calls Detection Alerts : Alerts are setup for each topology Nightswatch_(toplogy-name)_scheduler_hystrix_latencyExecute_percentile_90 For more details about the alert check Formula Used for Alerting on Hystrix Metrics Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: Scheduler Dashboard (90 Percentile Latency) Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos Possible Reasoning for such a Behaviour Latent Scheduler : Scheduler Service is latent. This means there is some issue at scheduler service. Debugging Step 1: Check Scheduler Dashboard Dashboard Panel: Client Entries latency Chennai: http://10.47.102.190/dashboard/(topology-name)/Scheduler Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Scheduler Inform id-dev@ if there is latency issue in Scheduler service Step 2: Check logs in worker boxes Where can I find the topology logs? How does the topology log look like? Check logs to find out the necessary reason of the issue Impact Latent Scheduler calls can lead to timeouts which would result in bolt failures. If bolts are failing continuously, it could lead to a pile up at the source Kafka. This would result in Client Kafka Offset Lag. Clients would be impacted if Lags keep on increasing and so, it should be resolved at priority Recovery Use the Debugging steps mentioned above to isolate the issue. Inform id-dev@ if any issue at scheduler side Scheduler Hystrix Error Percentage Description Nightswatch connects to Scheduler through REST API Calls to add, update or delete events in Scheduler Service. Scheduler calls Failures can lead to degrading the performance of the system. So, we should be closely monitoring the Scheduler Calls Detection Alerts : Alerts are setup for each topology Nightswatch_(toplogy-name)_scheduler_hystrix_errorPercentage For more details about the alert check Formula Used for Alerting on Hystrix Metrics Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: Scheduler Dashboard (Error Percentage) Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos Possible Reasoning for such a Behaviour Latent Scheduler : Scheduler Service is latent. This means there is some issue at scheduler service. Scheduler is Down : Scheduler Service is down. This means there is all calls to Scheduler will fail. Bug in Scheduler : Scheduler Service is having some issue resulting in failures. Debugging Step 1: Check Scheduler Dashboard Dashboard Panel: Client Entries latency Chennai: http://10.47.102.190/dashboard/(topology-name)/Scheduler Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Scheduler Inform id-dev@ if there is latency issue in Scheduler service Step 2: Check logs in worker boxes Where can I find the topology logs? How does the topology log look like? Check logs to find out the necessary reason of the issue Impact Scheduler Failures would result in bolt failures. If bolts are failing continuously, it could lead to a pile up at the source Kafka. This would result in Client Kafka Offset Lag. Clients would be impacted if Lags keep on increasing and so, it should be resolved at priority Recovery Use the Debugging steps mentioned above to isolate the issue. Inform id-dev@ if any issue at scheduler side","title":"Scheduler Runbooks"},{"location":"runbooks/schedulerRunbook/#scheduler-hystrix-latency-90-percentile","text":"","title":"Scheduler Hystrix Latency 90 Percentile"},{"location":"runbooks/schedulerRunbook/#description","text":"Nightswatch connects to Scheduler through REST API Calls to add, update or delete events in Scheduler Service. High latent scheduler calls can lead to degrading the performance of the system. So, we should be closely monitoring the Scheduler Calls","title":"Description"},{"location":"runbooks/schedulerRunbook/#detection","text":"Alerts : Alerts are setup for each topology Nightswatch_(toplogy-name)_scheduler_hystrix_latencyExecute_percentile_90 For more details about the alert check Formula Used for Alerting on Hystrix Metrics Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: Scheduler Dashboard (90 Percentile Latency) Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos","title":"Detection"},{"location":"runbooks/schedulerRunbook/#possible-reasoning-for-such-a-behaviour","text":"Latent Scheduler : Scheduler Service is latent. This means there is some issue at scheduler service.","title":"Possible Reasoning for such a Behaviour"},{"location":"runbooks/schedulerRunbook/#debugging","text":"Step 1: Check Scheduler Dashboard Dashboard Panel: Client Entries latency Chennai: http://10.47.102.190/dashboard/(topology-name)/Scheduler Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Scheduler Inform id-dev@ if there is latency issue in Scheduler service Step 2: Check logs in worker boxes Where can I find the topology logs? How does the topology log look like? Check logs to find out the necessary reason of the issue","title":"Debugging"},{"location":"runbooks/schedulerRunbook/#impact","text":"Latent Scheduler calls can lead to timeouts which would result in bolt failures. If bolts are failing continuously, it could lead to a pile up at the source Kafka. This would result in Client Kafka Offset Lag. Clients would be impacted if Lags keep on increasing and so, it should be resolved at priority","title":"Impact"},{"location":"runbooks/schedulerRunbook/#recovery","text":"Use the Debugging steps mentioned above to isolate the issue. Inform id-dev@ if any issue at scheduler side","title":"Recovery"},{"location":"runbooks/schedulerRunbook/#scheduler-hystrix-error-percentage","text":"","title":"Scheduler Hystrix Error Percentage"},{"location":"runbooks/schedulerRunbook/#description_1","text":"Nightswatch connects to Scheduler through REST API Calls to add, update or delete events in Scheduler Service. Scheduler calls Failures can lead to degrading the performance of the system. So, we should be closely monitoring the Scheduler Calls","title":"Description"},{"location":"runbooks/schedulerRunbook/#detection_1","text":"Alerts : Alerts are setup for each topology Nightswatch_(toplogy-name)_scheduler_hystrix_errorPercentage For more details about the alert check Formula Used for Alerting on Hystrix Metrics Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: Scheduler Dashboard (Error Percentage) Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos","title":"Detection"},{"location":"runbooks/schedulerRunbook/#possible-reasoning-for-such-a-behaviour_1","text":"Latent Scheduler : Scheduler Service is latent. This means there is some issue at scheduler service. Scheduler is Down : Scheduler Service is down. This means there is all calls to Scheduler will fail. Bug in Scheduler : Scheduler Service is having some issue resulting in failures.","title":"Possible Reasoning for such a Behaviour"},{"location":"runbooks/schedulerRunbook/#debugging_1","text":"Step 1: Check Scheduler Dashboard Dashboard Panel: Client Entries latency Chennai: http://10.47.102.190/dashboard/(topology-name)/Scheduler Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Scheduler Inform id-dev@ if there is latency issue in Scheduler service Step 2: Check logs in worker boxes Where can I find the topology logs? How does the topology log look like? Check logs to find out the necessary reason of the issue","title":"Debugging"},{"location":"runbooks/schedulerRunbook/#impact_1","text":"Scheduler Failures would result in bolt failures. If bolts are failing continuously, it could lead to a pile up at the source Kafka. This would result in Client Kafka Offset Lag. Clients would be impacted if Lags keep on increasing and so, it should be resolved at priority","title":"Impact"},{"location":"runbooks/schedulerRunbook/#recovery_1","text":"Use the Debugging steps mentioned above to isolate the issue. Inform id-dev@ if any issue at scheduler side","title":"Recovery"},{"location":"runbooks/sidelineGrowthRunbook/","text":"Description Nightswatch Sideline Growth happens when there is heavy Sidelining Happening from Client End. To know more about Sideline, please follow Nightswatch Architecture Detection Alerts : To be set up by Clients. Refer Alerts Page to set up alerts Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: Sideline Dashboard (Statistics) Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos Possible Reasoning for such a Behaviour Client unable to resolve the anomaly Client return 4xx Where do I find Topology Logs Where can I find the topology logs? How does the topology log look like? Impact Sideline growth indicates client is not able to perform the recon correctly. Recovery The Client Call Response lies with the Clients, which is used to decide on whether to sideline. So We have to inform Clients in case of such issue.","title":"Sideline Growth Runbooks"},{"location":"runbooks/sidelineGrowthRunbook/#description","text":"Nightswatch Sideline Growth happens when there is heavy Sidelining Happening from Client End. To know more about Sideline, please follow Nightswatch Architecture","title":"Description"},{"location":"runbooks/sidelineGrowthRunbook/#detection","text":"Alerts : To be set up by Clients. Refer Alerts Page to set up alerts Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: Sideline Dashboard (Statistics) Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos","title":"Detection"},{"location":"runbooks/sidelineGrowthRunbook/#possible-reasoning-for-such-a-behaviour","text":"Client unable to resolve the anomaly Client return 4xx","title":"Possible Reasoning for such a Behaviour"},{"location":"runbooks/sidelineGrowthRunbook/#where-do-i-find-topology-logs","text":"Where can I find the topology logs? How does the topology log look like?","title":"Where do I find Topology Logs"},{"location":"runbooks/sidelineGrowthRunbook/#impact","text":"Sideline growth indicates client is not able to perform the recon correctly.","title":"Impact"},{"location":"runbooks/sidelineGrowthRunbook/#recovery","text":"The Client Call Response lies with the Clients, which is used to decide on whether to sideline. So We have to inform Clients in case of such issue.","title":"Recovery"},{"location":"runbooks/sidelineRunbook/","text":"Sideline Hystrix Latency 90 Percentile Description Nightswatch uses Yak as Sideline Database. Thus, it connects to yak using Yak client which connects to yak using rest. High latent yak calls can lead to degrading the performance of the system. So, we should be closely monitoring the Yak Calls Detection Alerts : Alerts are setup for each topology Nightswatch_(toplogy-name)_sideline_hystrix_latencyExecute_percentile_90 For more details about the alert check Formula Used for Alerting on Hystrix Metrics Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: Sideline Dashboard (90 Percentile Latency) Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos Possible Reasoning for such a Behaviour Latent Yak : Yak is latent. This means there is some issue at Yak. Debugging Step 1: Check Yak Dashboard Chennai: http://10.47.4.24/dashboard/script/yak.js?prefix=prod-nightswatch-yak rsgroup=yak-prod_id_nightswatch refresh=30s orgId=1 from=now-15m to=now Hyderabad: http://10.24.0.243/dashboard/script/yak.js?prefix=prod-nightswatch-yak rsgroup=yak-prod_id_nightswatch refresh=30s orgId=1 from=now-15m to=now Dashboard Panel of interest: Latency (99th percentile) IPC Call time (99th percentile) Inform id-dev@ if there is latency issue in Yak Step 2: Check logs in worker boxes Where can I find the topology logs? How does the topology log look like? Check logs to find out the necessary reason of the issue Impact Latent Yak calls can lead to timeouts which would result in bolt failures. If bolts are failing continuously, it could lead to a pile up at the source Kafka. This would result in Client Kafka Offset Lag. Clients would be impacted if Lags keep on increasing and so, it should be resolved at priority Recovery Use the Debugging steps mentioned above to isolate the issue. Inform id-dev@ if any issue at scheduler side Sideline Hystrix Error Percentage Description Nightswatch uses Yak as Sideline Database. Thus, it connects to yak using Yak client which connects to yak using rest. Yak Failures can lead to degrading the performance of the system. So, we should be closely monitoring the Yak Calls Detection Alerts : Alerts are setup for each topology Nightswatch_(toplogy-name)_sideline_hystrix_errorPercentage For more details about the alert check Formula Used for Alerting on Hystrix Metrics Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: Sideline Dashboard (Error Percentage) Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos Possible Reasoning for such a Behaviour Unknown Host : New Data Nodes been added but not got reflected in /etc/hosts of all supervisor boxes Latent Yak : Yak is latent. This means there is some issue at Yak. Yak is Down : Yak is down. This means there is all calls to Yak will fail. Debugging Step 1: Check Yak Dashboard Chennai: http://10.47.4.24/dashboard/script/yak.js?prefix=prod-nightswatch-yak rsgroup=yak-prod_id_nightswatch refresh=30s orgId=1 from=now-15m to=now Hyderabad: http://10.24.0.243/dashboard/script/yak.js?prefix=prod-nightswatch-yak rsgroup=yak-prod_id_nightswatch refresh=30s orgId=1 from=now-15m to=now Dashboard Panel of interest: Write - TPT, Read TPT Exception Count Inform id-dev@ if there is latency issue in Yak Step 2: Check logs in worker boxes Where can I find the topology logs? How does the topology log look like? Check logs to find out the necessary reason of the issue Impact Yak Failures would result in bolt failures. If bolts are failing continuously, it could lead to a pile up at the source Kafka. This would result in Client Kafka Offset Lag. Clients would be impacted if Lags keep on increasing and so, it should be resolved at priority Recovery Use the Debugging steps mentioned above to isolate the issue. Inform id-dev@ if any issue at scheduler side","title":"Sideline Runbook"},{"location":"runbooks/sidelineRunbook/#sideline-hystrix-latency-90-percentile","text":"","title":"Sideline Hystrix Latency 90 Percentile"},{"location":"runbooks/sidelineRunbook/#description","text":"Nightswatch uses Yak as Sideline Database. Thus, it connects to yak using Yak client which connects to yak using rest. High latent yak calls can lead to degrading the performance of the system. So, we should be closely monitoring the Yak Calls","title":"Description"},{"location":"runbooks/sidelineRunbook/#detection","text":"Alerts : Alerts are setup for each topology Nightswatch_(toplogy-name)_sideline_hystrix_latencyExecute_percentile_90 For more details about the alert check Formula Used for Alerting on Hystrix Metrics Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: Sideline Dashboard (90 Percentile Latency) Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos","title":"Detection"},{"location":"runbooks/sidelineRunbook/#possible-reasoning-for-such-a-behaviour","text":"Latent Yak : Yak is latent. This means there is some issue at Yak.","title":"Possible Reasoning for such a Behaviour"},{"location":"runbooks/sidelineRunbook/#debugging","text":"Step 1: Check Yak Dashboard Chennai: http://10.47.4.24/dashboard/script/yak.js?prefix=prod-nightswatch-yak rsgroup=yak-prod_id_nightswatch refresh=30s orgId=1 from=now-15m to=now Hyderabad: http://10.24.0.243/dashboard/script/yak.js?prefix=prod-nightswatch-yak rsgroup=yak-prod_id_nightswatch refresh=30s orgId=1 from=now-15m to=now Dashboard Panel of interest: Latency (99th percentile) IPC Call time (99th percentile) Inform id-dev@ if there is latency issue in Yak Step 2: Check logs in worker boxes Where can I find the topology logs? How does the topology log look like? Check logs to find out the necessary reason of the issue","title":"Debugging"},{"location":"runbooks/sidelineRunbook/#impact","text":"Latent Yak calls can lead to timeouts which would result in bolt failures. If bolts are failing continuously, it could lead to a pile up at the source Kafka. This would result in Client Kafka Offset Lag. Clients would be impacted if Lags keep on increasing and so, it should be resolved at priority","title":"Impact"},{"location":"runbooks/sidelineRunbook/#recovery","text":"Use the Debugging steps mentioned above to isolate the issue. Inform id-dev@ if any issue at scheduler side","title":"Recovery"},{"location":"runbooks/sidelineRunbook/#sideline-hystrix-error-percentage","text":"","title":"Sideline Hystrix Error Percentage"},{"location":"runbooks/sidelineRunbook/#description_1","text":"Nightswatch uses Yak as Sideline Database. Thus, it connects to yak using Yak client which connects to yak using rest. Yak Failures can lead to degrading the performance of the system. So, we should be closely monitoring the Yak Calls","title":"Description"},{"location":"runbooks/sidelineRunbook/#detection_1","text":"Alerts : Alerts are setup for each topology Nightswatch_(toplogy-name)_sideline_hystrix_errorPercentage For more details about the alert check Formula Used for Alerting on Hystrix Metrics Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: Sideline Dashboard (Error Percentage) Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos","title":"Detection"},{"location":"runbooks/sidelineRunbook/#possible-reasoning-for-such-a-behaviour_1","text":"Unknown Host : New Data Nodes been added but not got reflected in /etc/hosts of all supervisor boxes Latent Yak : Yak is latent. This means there is some issue at Yak. Yak is Down : Yak is down. This means there is all calls to Yak will fail.","title":"Possible Reasoning for such a Behaviour"},{"location":"runbooks/sidelineRunbook/#debugging_1","text":"Step 1: Check Yak Dashboard Chennai: http://10.47.4.24/dashboard/script/yak.js?prefix=prod-nightswatch-yak rsgroup=yak-prod_id_nightswatch refresh=30s orgId=1 from=now-15m to=now Hyderabad: http://10.24.0.243/dashboard/script/yak.js?prefix=prod-nightswatch-yak rsgroup=yak-prod_id_nightswatch refresh=30s orgId=1 from=now-15m to=now Dashboard Panel of interest: Write - TPT, Read TPT Exception Count Inform id-dev@ if there is latency issue in Yak Step 2: Check logs in worker boxes Where can I find the topology logs? How does the topology log look like? Check logs to find out the necessary reason of the issue","title":"Debugging"},{"location":"runbooks/sidelineRunbook/#impact_1","text":"Yak Failures would result in bolt failures. If bolts are failing continuously, it could lead to a pile up at the source Kafka. This would result in Client Kafka Offset Lag. Clients would be impacted if Lags keep on increasing and so, it should be resolved at priority","title":"Impact"},{"location":"runbooks/sidelineRunbook/#recovery_1","text":"Use the Debugging steps mentioned above to isolate the issue. Inform id-dev@ if any issue at scheduler side","title":"Recovery"},{"location":"runbooks/stormRunbook/","text":"Nimbus Went Down Description Refer What is Nimbus? for more details on Nimbus. Detection Alerts : Ping Alerts Dashboard : Chennai : http://10.47.4.24/dashboard/script/storm_nimbus_v1.js?orgId=1 refresh=1m panelId=2 fullscreen Hyderabad : http://24.0.243/dashboard/script/storm_nimbus_v1.js?orgId=1 refresh=1m panelId=2 fullscreen Possible Reasoning for such a Behaviour Scheduled / Unscheduled Maintenance (Box is down) High Resource Utilization (Process is down) Nimbus is a daemon, so even if the process goes down, it will respawn Where do I find Nimbus Logs /var/log/fk-3p-storm/nimbus.log Impact No existing topologies can be stopped / restarted No new topologies can be started If a supervisor node fails then the reassignments are not performed Existing Topologies continues to run as is Recovery RTO: 5mins Please follow the steps mention in Maintenance Steps for Storm Nimbus Validation Verify Nimbus is started Chennai : http://10.47.101.5:8000/ch.html?vip=10.47.103.132 Hyderabad : http://10.47.101.5:8000/hyd.html?vip=10.24.7.28 Supervisor Went Down Description Refer What is Supervisor? for more details on Supervisor. Detection Alerts : Ping Alerts Dashboard : Chennai: http://10.47.3.62/dashboard/script/storm_nimbus_v1.js?orgId=1 refresh=1m panelId=4 fullscreen Hyderabad : http://10.24.0.243/dashboard/script/storm_nimbus_v1.js?orgId=1 refresh=1m panelId=4 fullscreen Possible Reasoning for such a Behaviour Scheduled / Unscheduled Maintenance (Box is down) High Resource Utilization (Process is down) Scheduler is a daemon, so even if the process goes down, it will respawn Where do I find Supervisor Logs /var/log/fk-3p-storm/supervisor.log Impact Existing Topologies continues to run as is. Nimbus will make sure the task is reassigned to some other supervisor Recovery RTO: 5mins Please follow the steps mention in Maintenance Steps for Storm Supervisor Validation Verify supervisor is started Chennai : curl http://10.47.103.132/api/v1/supervisor/summary --silent | jq '.supervisors[].id' Hyderabad : curl http://10.24.7.28/api/v1/supervisor/summary --silent | jq '.supervisors[].id'","title":"Storm Runbooks"},{"location":"runbooks/stormRunbook/#nimbus-went-down","text":"","title":"Nimbus Went Down"},{"location":"runbooks/stormRunbook/#description","text":"Refer What is Nimbus? for more details on Nimbus.","title":"Description"},{"location":"runbooks/stormRunbook/#detection","text":"Alerts : Ping Alerts Dashboard : Chennai : http://10.47.4.24/dashboard/script/storm_nimbus_v1.js?orgId=1 refresh=1m panelId=2 fullscreen Hyderabad : http://24.0.243/dashboard/script/storm_nimbus_v1.js?orgId=1 refresh=1m panelId=2 fullscreen","title":"Detection"},{"location":"runbooks/stormRunbook/#possible-reasoning-for-such-a-behaviour","text":"Scheduled / Unscheduled Maintenance (Box is down) High Resource Utilization (Process is down) Nimbus is a daemon, so even if the process goes down, it will respawn","title":"Possible Reasoning for such a Behaviour"},{"location":"runbooks/stormRunbook/#where-do-i-find-nimbus-logs","text":"/var/log/fk-3p-storm/nimbus.log","title":"Where do I find Nimbus Logs"},{"location":"runbooks/stormRunbook/#impact","text":"No existing topologies can be stopped / restarted No new topologies can be started If a supervisor node fails then the reassignments are not performed Existing Topologies continues to run as is","title":"Impact"},{"location":"runbooks/stormRunbook/#recovery","text":"RTO: 5mins Please follow the steps mention in Maintenance Steps for Storm Nimbus","title":"Recovery"},{"location":"runbooks/stormRunbook/#validation","text":"Verify Nimbus is started Chennai : http://10.47.101.5:8000/ch.html?vip=10.47.103.132 Hyderabad : http://10.47.101.5:8000/hyd.html?vip=10.24.7.28","title":"Validation"},{"location":"runbooks/stormRunbook/#supervisor-went-down","text":"","title":"Supervisor Went Down"},{"location":"runbooks/stormRunbook/#description_1","text":"Refer What is Supervisor? for more details on Supervisor.","title":"Description"},{"location":"runbooks/stormRunbook/#detection_1","text":"Alerts : Ping Alerts Dashboard : Chennai: http://10.47.3.62/dashboard/script/storm_nimbus_v1.js?orgId=1 refresh=1m panelId=4 fullscreen Hyderabad : http://10.24.0.243/dashboard/script/storm_nimbus_v1.js?orgId=1 refresh=1m panelId=4 fullscreen","title":"Detection"},{"location":"runbooks/stormRunbook/#possible-reasoning-for-such-a-behaviour_1","text":"Scheduled / Unscheduled Maintenance (Box is down) High Resource Utilization (Process is down) Scheduler is a daemon, so even if the process goes down, it will respawn","title":"Possible Reasoning for such a Behaviour"},{"location":"runbooks/stormRunbook/#where-do-i-find-supervisor-logs","text":"/var/log/fk-3p-storm/supervisor.log","title":"Where do I find Supervisor Logs"},{"location":"runbooks/stormRunbook/#impact_1","text":"Existing Topologies continues to run as is. Nimbus will make sure the task is reassigned to some other supervisor","title":"Impact"},{"location":"runbooks/stormRunbook/#recovery_1","text":"RTO: 5mins Please follow the steps mention in Maintenance Steps for Storm Supervisor","title":"Recovery"},{"location":"runbooks/stormRunbook/#validation_1","text":"Verify supervisor is started Chennai : curl http://10.47.103.132/api/v1/supervisor/summary --silent | jq '.supervisors[].id' Hyderabad : curl http://10.24.7.28/api/v1/supervisor/summary --silent | jq '.supervisors[].id'","title":"Validation"},{"location":"runbooks/yakDNRunbook/","text":"Yak DN/RS Went Down Description Nightswatch uses Yak as Database. Thus, it connects to yak using Yak client which connects to yak using rest. Detection Alerts : Ping Alerts Dashboard : Check Hbase UI and Hadoop UI https://github.fkinternal.com/pages/Flipkart/yak/clusters/archive/monitoring/ Runbook Follow the Steps mentioned in Yak Documentation. https://github.fkinternal.com/pages/Flipkart/yak/runbook/06-regionserver/#node-goes-unservicable Verification Verify Data Node is setup in Hbase Master Dashboard http://(namenode_elb)/dfshealth.html#tab-datanode http://(hmaster_elb)/master-status Check Yak Monitoring Guide to get correct Dashbaords","title":"Yak Data Node RunBook"},{"location":"runbooks/yakDNRunbook/#yak-dnrs-went-down","text":"","title":"Yak DN/RS Went Down"},{"location":"runbooks/yakDNRunbook/#description","text":"Nightswatch uses Yak as Database. Thus, it connects to yak using Yak client which connects to yak using rest.","title":"Description"},{"location":"runbooks/yakDNRunbook/#detection","text":"Alerts : Ping Alerts Dashboard : Check Hbase UI and Hadoop UI https://github.fkinternal.com/pages/Flipkart/yak/clusters/archive/monitoring/","title":"Detection"},{"location":"runbooks/yakDNRunbook/#runbook","text":"Follow the Steps mentioned in Yak Documentation. https://github.fkinternal.com/pages/Flipkart/yak/runbook/06-regionserver/#node-goes-unservicable","title":"Runbook"},{"location":"runbooks/yakDNRunbook/#verification","text":"Verify Data Node is setup in Hbase Master Dashboard http://(namenode_elb)/dfshealth.html#tab-datanode http://(hmaster_elb)/master-status Check Yak Monitoring Guide to get correct Dashbaords","title":"Verification"},{"location":"runbooks/yakRunbook/","text":"Yak Hystrix Latency 90 Percentile Description Nightswatch uses Yak as Database. Thus, it connects to yak using Yak client which connects to yak using rest. High latent yak calls can lead to degrading the performance of the system. So, we should be closely monitoring the Yak Calls Detection Alerts : Alerts are setup for each topology Nightswatch_(toplogy-name)_yak_hystrix_latencyExecute_percentile_90 For more details about the alert check Formula Used for Alerting on Hystrix Metrics Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: Yak Dashboard (90 Percentile Latency) Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos Possible Reasoning for such a Behaviour Latent Yak : Yak is latent. This means there is some issue at Yak. Debugging Step 1: Check Yak Dashboard Chennai: http://10.47.4.24/dashboard/script/yak.js?prefix=prod-nightswatch-yak rsgroup=yak-prod_id_nightswatch refresh=30s orgId=1 from=now-15m to=now Hyderabad: http://10.24.0.243/dashboard/script/yak.js?prefix=prod-nightswatch-yak rsgroup=yak-prod_id_nightswatch refresh=30s orgId=1 from=now-15m to=now Dashboard Panel of interest: Latency (99th percentile) IPC Call time (99th percentile) Inform id-dev@ if there is latency issue in Yak Step 2: Check logs in worker boxes Where can I find the topology logs? How does the topology log look like? Check logs to find out the necessary reason of the issue Impact Latent Yak calls can lead to timeouts which would result in bolt failures. If bolts are failing continuously, it could lead to a pile up at the source Kafka. This would result in Client Kafka Offset Lag. Clients would be impacted if Lags keep on increasing and so, it should be resolved at priority Recovery Use the Debugging steps mentioned above to isolate the issue. Inform id-dev@ if any issue at scheduler side Yak Hystrix Error Percentage Description Nightswatch uses Yak as Database. Thus, it connects to yak using Yak client which connects to yak using rest. Yak Failures can lead to degrading the performance of the system. So, we should be closely monitoring the Yak Calls Detection Alerts : Alerts are setup for each topology Nightswatch_(toplogy-name)_yak_hystrix_errorPercentage For more details about the alert check Formula Used for Alerting on Hystrix Metrics Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: Yak Dashboard (Error Percentage) Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos Possible Reasoning for such a Behaviour Unknown Host : New Data Nodes been added but not got reflected in /etc/hosts of all supervisor boxes Latent Yak : Yak is latent. This means there is some issue at Yak. Yak is Down : Yak is down. This means there is all calls to Yak will fail. Debugging Step 1: Check Yak Dashboard Chennai: http://10.47.4.24/dashboard/script/yak.js?prefix=prod-nightswatch-yak rsgroup=yak-prod_id_nightswatch refresh=30s orgId=1 from=now-15m to=now Hyderabad: http://10.24.0.243/dashboard/script/yak.js?prefix=prod-nightswatch-yak rsgroup=yak-prod_id_nightswatch refresh=30s orgId=1 from=now-15m to=now Dashboard Panel of interest: Write - TPT, Read TPT Exception Count Inform id-dev@ if there is latency issue in Yak Step 2: Check logs in worker boxes Where can I find the topology logs? How does the topology log look like? Check logs to find out the necessary reason of the issue Impact Yak Failures would result in bolt failures. If bolts are failing continuously, it could lead to a pile up at the source Kafka. This would result in Client Kafka Offset Lag. Clients would be impacted if Lags keep on increasing and so, it should be resolved at priority Recovery Use the Debugging steps mentioned above to isolate the issue. Inform id-dev@ if any issue at scheduler side","title":"Yak Runbooks"},{"location":"runbooks/yakRunbook/#yak-hystrix-latency-90-percentile","text":"","title":"Yak Hystrix Latency 90 Percentile"},{"location":"runbooks/yakRunbook/#description","text":"Nightswatch uses Yak as Database. Thus, it connects to yak using Yak client which connects to yak using rest. High latent yak calls can lead to degrading the performance of the system. So, we should be closely monitoring the Yak Calls","title":"Description"},{"location":"runbooks/yakRunbook/#detection","text":"Alerts : Alerts are setup for each topology Nightswatch_(toplogy-name)_yak_hystrix_latencyExecute_percentile_90 For more details about the alert check Formula Used for Alerting on Hystrix Metrics Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: Yak Dashboard (90 Percentile Latency) Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos","title":"Detection"},{"location":"runbooks/yakRunbook/#possible-reasoning-for-such-a-behaviour","text":"Latent Yak : Yak is latent. This means there is some issue at Yak.","title":"Possible Reasoning for such a Behaviour"},{"location":"runbooks/yakRunbook/#debugging","text":"Step 1: Check Yak Dashboard Chennai: http://10.47.4.24/dashboard/script/yak.js?prefix=prod-nightswatch-yak rsgroup=yak-prod_id_nightswatch refresh=30s orgId=1 from=now-15m to=now Hyderabad: http://10.24.0.243/dashboard/script/yak.js?prefix=prod-nightswatch-yak rsgroup=yak-prod_id_nightswatch refresh=30s orgId=1 from=now-15m to=now Dashboard Panel of interest: Latency (99th percentile) IPC Call time (99th percentile) Inform id-dev@ if there is latency issue in Yak Step 2: Check logs in worker boxes Where can I find the topology logs? How does the topology log look like? Check logs to find out the necessary reason of the issue","title":"Debugging"},{"location":"runbooks/yakRunbook/#impact","text":"Latent Yak calls can lead to timeouts which would result in bolt failures. If bolts are failing continuously, it could lead to a pile up at the source Kafka. This would result in Client Kafka Offset Lag. Clients would be impacted if Lags keep on increasing and so, it should be resolved at priority","title":"Impact"},{"location":"runbooks/yakRunbook/#recovery","text":"Use the Debugging steps mentioned above to isolate the issue. Inform id-dev@ if any issue at scheduler side","title":"Recovery"},{"location":"runbooks/yakRunbook/#yak-hystrix-error-percentage","text":"","title":"Yak Hystrix Error Percentage"},{"location":"runbooks/yakRunbook/#description_1","text":"Nightswatch uses Yak as Database. Thus, it connects to yak using Yak client which connects to yak using rest. Yak Failures can lead to degrading the performance of the system. So, we should be closely monitoring the Yak Calls","title":"Description"},{"location":"runbooks/yakRunbook/#detection_1","text":"Alerts : Alerts are setup for each topology Nightswatch_(toplogy-name)_yak_hystrix_errorPercentage For more details about the alert check Formula Used for Alerting on Hystrix Metrics Dashboard : Refer to Nightswatch Topology level Dashboard to see the Metric behaviour. Refer Dashboards Page Dashboard Panel: Yak Dashboard (Error Percentage) Chennai: http://10.47.102.190/dashboard/(topology-name)/Cosmos Hyderabad: http://10.24.7.187/dashboard/(topology-name)/Cosmos","title":"Detection"},{"location":"runbooks/yakRunbook/#possible-reasoning-for-such-a-behaviour_1","text":"Unknown Host : New Data Nodes been added but not got reflected in /etc/hosts of all supervisor boxes Latent Yak : Yak is latent. This means there is some issue at Yak. Yak is Down : Yak is down. This means there is all calls to Yak will fail.","title":"Possible Reasoning for such a Behaviour"},{"location":"runbooks/yakRunbook/#debugging_1","text":"Step 1: Check Yak Dashboard Chennai: http://10.47.4.24/dashboard/script/yak.js?prefix=prod-nightswatch-yak rsgroup=yak-prod_id_nightswatch refresh=30s orgId=1 from=now-15m to=now Hyderabad: http://10.24.0.243/dashboard/script/yak.js?prefix=prod-nightswatch-yak rsgroup=yak-prod_id_nightswatch refresh=30s orgId=1 from=now-15m to=now Dashboard Panel of interest: Write - TPT, Read TPT Exception Count Inform id-dev@ if there is latency issue in Yak Step 2: Check logs in worker boxes Where can I find the topology logs? How does the topology log look like? Check logs to find out the necessary reason of the issue","title":"Debugging"},{"location":"runbooks/yakRunbook/#impact_1","text":"Yak Failures would result in bolt failures. If bolts are failing continuously, it could lead to a pile up at the source Kafka. This would result in Client Kafka Offset Lag. Clients would be impacted if Lags keep on increasing and so, it should be resolved at priority","title":"Impact"},{"location":"runbooks/yakRunbook/#recovery_1","text":"Use the Debugging steps mentioned above to isolate the issue. Inform id-dev@ if any issue at scheduler side","title":"Recovery"},{"location":"runbooks/zookeeperRunbook/","text":"Zookeeper Node Went Down Description Zookeeper is used by a cluster (group of nodes) to coordinate between each other and maintaining shared data with robust synchronization techniques. Nimbus is stateless, so it depends on ZooKeeper to monitor the working node status. Detection Alerts : Ping Alerts, Zookeeper Alive Nodes Dashboard : Chennai : http://10.47.4.24/dashboard/script/zk_v4.js?refresh=1m orgId=1 from=now-30m to=now app=prod-nightswatch-zookeeper Hyderabad : http://10.24.0.243/dashboard/script/zk_v4.js?refresh=1m orgId=1 from=now-30m to=now app=prod-nightswatch-zookeeper Impact Zookeeper Single Node going down won't have much impact on the entire system, unless the quorum is broken If zookeeper quorum breaks, the zookeeper cluster will go down resulting in Entire storm going down Runbook Success RTO (Single Node): 15mins RTO (Cluster): 1 hr Follow the Steps mentioned in Zookeeper Maintenance Guide . Verification Verify Zookeeper is up Use netcat on the box to see the stats of zookeeper echo stat | nc localhost 2181","title":"Zookeeper Runbooks"},{"location":"runbooks/zookeeperRunbook/#zookeeper-node-went-down","text":"","title":"Zookeeper Node Went Down"},{"location":"runbooks/zookeeperRunbook/#description","text":"Zookeeper is used by a cluster (group of nodes) to coordinate between each other and maintaining shared data with robust synchronization techniques. Nimbus is stateless, so it depends on ZooKeeper to monitor the working node status.","title":"Description"},{"location":"runbooks/zookeeperRunbook/#detection","text":"Alerts : Ping Alerts, Zookeeper Alive Nodes Dashboard : Chennai : http://10.47.4.24/dashboard/script/zk_v4.js?refresh=1m orgId=1 from=now-30m to=now app=prod-nightswatch-zookeeper Hyderabad : http://10.24.0.243/dashboard/script/zk_v4.js?refresh=1m orgId=1 from=now-30m to=now app=prod-nightswatch-zookeeper","title":"Detection"},{"location":"runbooks/zookeeperRunbook/#impact","text":"Zookeeper Single Node going down won't have much impact on the entire system, unless the quorum is broken If zookeeper quorum breaks, the zookeeper cluster will go down resulting in Entire storm going down","title":"Impact"},{"location":"runbooks/zookeeperRunbook/#runbook","text":"Success RTO (Single Node): 15mins RTO (Cluster): 1 hr Follow the Steps mentioned in Zookeeper Maintenance Guide .","title":"Runbook"},{"location":"runbooks/zookeeperRunbook/#verification","text":"Verify Zookeeper is up Use netcat on the box to see the stats of zookeeper echo stat | nc localhost 2181","title":"Verification"},{"location":"runbooks/maintenance/nwDashboardMaintenance/","text":"Maintenance Steps for Nightswatch Dashboard Instance Groups App Id: prod-nightswatch Chennai: nightswatch-dashboard-v4 Hyderabad: nightswatch-dashboard Stop the dashboard process in the box sudo /etc/init.d/nightswatch-dashboard stop Destroy the instance from kloud-cli Resize the instance group Start the dashboard process in the Supervisor sudo /etc/init.d/nightswatch-dashboard start Verify Dashboard is started Chennai : http://10.47.101.5:8000/ch.html?vip=10.47.102.190 Hyderabad : http://10.47.101.5:8000/hyd.html?vip=10.24.7.187","title":"Nightswatch Dashboard"},{"location":"runbooks/maintenance/nwDashboardMaintenance/#maintenance-steps-for-nightswatch-dashboard","text":"Instance Groups App Id: prod-nightswatch Chennai: nightswatch-dashboard-v4 Hyderabad: nightswatch-dashboard Stop the dashboard process in the box sudo /etc/init.d/nightswatch-dashboard stop Destroy the instance from kloud-cli Resize the instance group Start the dashboard process in the Supervisor sudo /etc/init.d/nightswatch-dashboard start Verify Dashboard is started Chennai : http://10.47.101.5:8000/ch.html?vip=10.47.102.190 Hyderabad : http://10.47.101.5:8000/hyd.html?vip=10.24.7.187","title":"Maintenance Steps for Nightswatch Dashboard"},{"location":"runbooks/maintenance/nwRestMaintenance/","text":"Maintenance Steps for Nightswatch Rest Service Instance Groups App Id: prod-nightswatch Chennai: rest-service-v2 Hyderabad: rest-service-v1 Stop the Nightswatch rest process in the box sudo /etc/init.d/fk-transact-nightswatch stop nightswatch Destroy the instance from kloud-cli Resize the instance group Start the Nightswatch rest process in the box sudo /etc/init.d/fk-transact-nightswatch start nightswatch Verify Dashboard is started Chennai : http://10.47.101.5:8000/ch.html?vip=10.47.102.118 Hyderabad : http://10.47.101.5:8000/hyd.html?vip=10.24.7.64","title":"Nightswatch Rest"},{"location":"runbooks/maintenance/nwRestMaintenance/#maintenance-steps-for-nightswatch-rest-service","text":"Instance Groups App Id: prod-nightswatch Chennai: rest-service-v2 Hyderabad: rest-service-v1 Stop the Nightswatch rest process in the box sudo /etc/init.d/fk-transact-nightswatch stop nightswatch Destroy the instance from kloud-cli Resize the instance group Start the Nightswatch rest process in the box sudo /etc/init.d/fk-transact-nightswatch start nightswatch Verify Dashboard is started Chennai : http://10.47.101.5:8000/ch.html?vip=10.47.102.118 Hyderabad : http://10.47.101.5:8000/hyd.html?vip=10.24.7.64","title":"Maintenance Steps for Nightswatch Rest Service"},{"location":"runbooks/maintenance/stormMaintenance/","text":"Maintenance Steps for Storm App Instance Details App Id: prod-nightswatch Chennai App Box Instance Group: storm-app-v3 Hyderabad App Box Instance Group: storm-app Stop the supervisor and worker process in the box sudo /etc/init.d/fk-3p-storm stop supervisor ps -ef | grep storm.daemon.worker.Worker sudo kill -9 (worker pid) Destroy the instance from kloud-cli Delete the host entry of the box in prod-nightswatch-etc-hosts config bucket Resize the instance group Create a host entry of the newly created box in prod-nightswatch-etc-hosts config bucket Start the supervisor process in the box sudo /etc/init.d/fk-3p-storm start supervisor Verify supervisor is started Chennai : curl http://10.47.103.132/api/v1/supervisor/summary --silent | jq '.supervisors[].id' Hyderabad : curl http://10.24.7.28/api/v1/supervisor/summary --silent | jq '.supervisors[].id' Maintenance Steps for Storm Supervisor Instance Details App Id: prod-nightswatch Chennai: storm-supervisors-v9 Hyderabad: storm-supervisors-v1 Stop the supervisor and worker process in the box sudo /etc/init.d/fk-3p-storm stop supervisor ps -ef | grep storm.daemon.worker.Worker sudo kill -9 (worker pid) Destroy the instance from kloud-cli Delete the host entry of the box in prod-nightswatch-etc-hosts config bucket Resize the instance group Create a host entry of the newly created box in prod-nightswatch-etc-hosts config bucket Start the supervisor process in the box sudo /etc/init.d/fk-3p-storm start supervisor Verify supervisor is started Chennai : curl http://10.47.103.132/api/v1/supervisor/summary --silent | jq '.supervisors[].id' Hyderabad : curl http://10.24.7.28/api/v1/supervisor/summary --silent | jq '.supervisors[].id' Stopping and Starting Multiple Supervisor Processes We have an ansible playbook that can be used to stop and start supervisor and worker processes in multiple instances. It is placed in $ROOT/dev-support/storm-deployment/restart_supervisor.yaml Running the Playbook Inventory Inventory in Ansible world refers to the list of instances where a particular playbook has to be run. It is defined by the hosts file. For Nightswatch, the hosts file has been placed at ROOT/dev-support/storm-deployment/inventory/hosts . It contains a list of IP addresses of supervisors on which we wish to deploy in the following format. [supervisors] 10.32.2.167 10.33.2.234 There is also a script $ROOT/dev-support/storm-deployment/populate-inventory.sh which can be used to fetch a list of supervisors for chennai and hyderabad zone for production app and update the hosts file. Usage: ./PATH_TO/populate_inventory.sh -z ZONE where ZONE is either chennai or hyderabad . Configuration The playbook can be configured using a json file that lists all the options. It is placed at $ROOT/dev-support/storm-deployment/env.json . The file has following options { \"rolling\": 4, \"nightswatch_build\": 2797, \"storm_build\": 21, \"reinstall_storm\": false } rolling : No. of instances ansible should in parallel run the playbook. Note: Rest of the options in the env.json file will be ignored for this playbook. Running the playbook. Pre-requisites: Install ansible-playbook in your system. Populate inventory/hosts file with appropriate list of IPs Get sudo permission for app prod-nightswatch Playbook Execution: ansible-playbook restart_supervisors.yaml -i inventory/hosts --extra-vars \"@env.json\" -vvv Note: The -vvv option is to print verbose logs for every step. Maintenance Steps for Storm Nimbus Instance Groups App Id: prod-nightswatch Chennai: storm-nimbus Hyderabad: storm-nimbus-v1 Nimbus and UI process are hosted on the same box Stop the nimbus process in the box sudo /etc/init.d/fk-3p-storm stop nimbus Stop the ui process in the box sudo /etc/init.d/fk-3p-storm stop ui Destroy the instance from kloud-cli Delete the host entry of the box in prod-nightswatch-etc-hosts config bucket Resize the instance group Create a host entry of the newly created box in prod-nightswatch-etc-hosts config bucket Change the nimbus.seeds key in prod-nightswatch-storm-config config bucket with new nimbus ip Start the nimbus process in the box Make sure the new nimbus ip is reflected in /etc/fk-3p-storm/conf/storm.yaml sudo /etc/init.d/fk-3p-storm start nimbus Start the ui process in the box sudo /etc/init.d/fk-3p-storm start ui Verify Nimbus is started Chennai : http://10.47.101.5:8000/ch.html?vip=10.47.103.132 Hyderabad : http://10.47.101.5:8000/hyd.html?vip=10.24.7.28 Restart all the supervisors after starting the nimbus Make sure the new nimbus ip is reflected in /etc/fk-3p-storm/conf/storm.yaml sudo /etc/init.d/fk-3p-storm stop supervisor sudo /etc/init.d/fk-3p-storm start supervisor","title":"Storm"},{"location":"runbooks/maintenance/stormMaintenance/#maintenance-steps-for-storm-app","text":"Instance Details App Id: prod-nightswatch Chennai App Box Instance Group: storm-app-v3 Hyderabad App Box Instance Group: storm-app Stop the supervisor and worker process in the box sudo /etc/init.d/fk-3p-storm stop supervisor ps -ef | grep storm.daemon.worker.Worker sudo kill -9 (worker pid) Destroy the instance from kloud-cli Delete the host entry of the box in prod-nightswatch-etc-hosts config bucket Resize the instance group Create a host entry of the newly created box in prod-nightswatch-etc-hosts config bucket Start the supervisor process in the box sudo /etc/init.d/fk-3p-storm start supervisor Verify supervisor is started Chennai : curl http://10.47.103.132/api/v1/supervisor/summary --silent | jq '.supervisors[].id' Hyderabad : curl http://10.24.7.28/api/v1/supervisor/summary --silent | jq '.supervisors[].id'","title":"Maintenance Steps for Storm App"},{"location":"runbooks/maintenance/stormMaintenance/#maintenance-steps-for-storm-supervisor","text":"Instance Details App Id: prod-nightswatch Chennai: storm-supervisors-v9 Hyderabad: storm-supervisors-v1 Stop the supervisor and worker process in the box sudo /etc/init.d/fk-3p-storm stop supervisor ps -ef | grep storm.daemon.worker.Worker sudo kill -9 (worker pid) Destroy the instance from kloud-cli Delete the host entry of the box in prod-nightswatch-etc-hosts config bucket Resize the instance group Create a host entry of the newly created box in prod-nightswatch-etc-hosts config bucket Start the supervisor process in the box sudo /etc/init.d/fk-3p-storm start supervisor Verify supervisor is started Chennai : curl http://10.47.103.132/api/v1/supervisor/summary --silent | jq '.supervisors[].id' Hyderabad : curl http://10.24.7.28/api/v1/supervisor/summary --silent | jq '.supervisors[].id'","title":"Maintenance Steps for Storm Supervisor"},{"location":"runbooks/maintenance/stormMaintenance/#stopping-and-starting-multiple-supervisor-processes","text":"We have an ansible playbook that can be used to stop and start supervisor and worker processes in multiple instances. It is placed in $ROOT/dev-support/storm-deployment/restart_supervisor.yaml","title":"Stopping and Starting Multiple Supervisor Processes"},{"location":"runbooks/maintenance/stormMaintenance/#running-the-playbook","text":"","title":"Running the Playbook"},{"location":"runbooks/maintenance/stormMaintenance/#inventory","text":"Inventory in Ansible world refers to the list of instances where a particular playbook has to be run. It is defined by the hosts file. For Nightswatch, the hosts file has been placed at ROOT/dev-support/storm-deployment/inventory/hosts . It contains a list of IP addresses of supervisors on which we wish to deploy in the following format. [supervisors] 10.32.2.167 10.33.2.234 There is also a script $ROOT/dev-support/storm-deployment/populate-inventory.sh which can be used to fetch a list of supervisors for chennai and hyderabad zone for production app and update the hosts file. Usage: ./PATH_TO/populate_inventory.sh -z ZONE where ZONE is either chennai or hyderabad .","title":"Inventory"},{"location":"runbooks/maintenance/stormMaintenance/#configuration","text":"The playbook can be configured using a json file that lists all the options. It is placed at $ROOT/dev-support/storm-deployment/env.json . The file has following options { \"rolling\": 4, \"nightswatch_build\": 2797, \"storm_build\": 21, \"reinstall_storm\": false } rolling : No. of instances ansible should in parallel run the playbook. Note: Rest of the options in the env.json file will be ignored for this playbook.","title":"Configuration"},{"location":"runbooks/maintenance/stormMaintenance/#running-the-playbook_1","text":"Pre-requisites: Install ansible-playbook in your system. Populate inventory/hosts file with appropriate list of IPs Get sudo permission for app prod-nightswatch Playbook Execution: ansible-playbook restart_supervisors.yaml -i inventory/hosts --extra-vars \"@env.json\" -vvv Note: The -vvv option is to print verbose logs for every step.","title":"Running the playbook."},{"location":"runbooks/maintenance/stormMaintenance/#maintenance-steps-for-storm-nimbus","text":"Instance Groups App Id: prod-nightswatch Chennai: storm-nimbus Hyderabad: storm-nimbus-v1 Nimbus and UI process are hosted on the same box Stop the nimbus process in the box sudo /etc/init.d/fk-3p-storm stop nimbus Stop the ui process in the box sudo /etc/init.d/fk-3p-storm stop ui Destroy the instance from kloud-cli Delete the host entry of the box in prod-nightswatch-etc-hosts config bucket Resize the instance group Create a host entry of the newly created box in prod-nightswatch-etc-hosts config bucket Change the nimbus.seeds key in prod-nightswatch-storm-config config bucket with new nimbus ip Start the nimbus process in the box Make sure the new nimbus ip is reflected in /etc/fk-3p-storm/conf/storm.yaml sudo /etc/init.d/fk-3p-storm start nimbus Start the ui process in the box sudo /etc/init.d/fk-3p-storm start ui Verify Nimbus is started Chennai : http://10.47.101.5:8000/ch.html?vip=10.47.103.132 Hyderabad : http://10.47.101.5:8000/hyd.html?vip=10.24.7.28 Restart all the supervisors after starting the nimbus Make sure the new nimbus ip is reflected in /etc/fk-3p-storm/conf/storm.yaml sudo /etc/init.d/fk-3p-storm stop supervisor sudo /etc/init.d/fk-3p-storm start supervisor","title":"Maintenance Steps for Storm Nimbus"},{"location":"runbooks/maintenance/yakMaintenance/","text":"Maintenance Steps for Nightswatch Yak Data Nodes Instance Details App Id: prod-nightswatch-yak Before maintenance, we need to gracefully shut down region server and data node Stop the Region Server sudo /etc/init.d/yak regionserver stop Stop the Data Node sudo /etc/init.d/yak datanode stop After the maintenance, follow the guide below: https://github.fkinternal.com/pages/Flipkart/yak/adminstration/tenant/replace-node/ Verify Data Node is setup in Hbase Master Dashboard http://(namenode_elb)/dfshealth.html#tab-datanode http://(hmaster_elb)/master-status In case of any issue, contact id-dev@","title":"Yak"},{"location":"runbooks/maintenance/yakMaintenance/#maintenance-steps-for-nightswatch-yak-data-nodes","text":"Instance Details App Id: prod-nightswatch-yak Before maintenance, we need to gracefully shut down region server and data node Stop the Region Server sudo /etc/init.d/yak regionserver stop Stop the Data Node sudo /etc/init.d/yak datanode stop After the maintenance, follow the guide below: https://github.fkinternal.com/pages/Flipkart/yak/adminstration/tenant/replace-node/ Verify Data Node is setup in Hbase Master Dashboard http://(namenode_elb)/dfshealth.html#tab-datanode http://(hmaster_elb)/master-status In case of any issue, contact id-dev@","title":"Maintenance Steps for Nightswatch Yak Data Nodes"},{"location":"runbooks/maintenance/zookeeperMaintenance/","text":"Maintenance Steps for Nightswatch Zookeeper Instance Details Chennai App Id: prod-nightswatch-zookeeper Hyderabad App Id: prod-nightswatch-zookeeper Stop zookeeper instance sudo /etc/init.d/fk-zookeeper-server stop Verify if the quorum size is 3; else add Zookeeper node before stopping this Remove host from config bucket prod-nightswatch-zk To setup new zookeeper box, follow How to setup Zookeeper cluster Verify Zookeeper is up Use netcat on the box to see the stats of zookeeper echo stat | nc localhost 2181 In case of any issue, contact id-dev@","title":"Zookeeper"},{"location":"runbooks/maintenance/zookeeperMaintenance/#maintenance-steps-for-nightswatch-zookeeper","text":"Instance Details Chennai App Id: prod-nightswatch-zookeeper Hyderabad App Id: prod-nightswatch-zookeeper Stop zookeeper instance sudo /etc/init.d/fk-zookeeper-server stop Verify if the quorum size is 3; else add Zookeeper node before stopping this Remove host from config bucket prod-nightswatch-zk To setup new zookeeper box, follow How to setup Zookeeper cluster Verify Zookeeper is up Use netcat on the box to see the stats of zookeeper echo stat | nc localhost 2181 In case of any issue, contact id-dev@","title":"Maintenance Steps for Nightswatch Zookeeper"}]}